{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "import calc_vector\n",
    "import multiprocessing as mp\n",
    "import random\n",
    "\n",
    "from scipy import sparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.sparse.linalg import spsolve\n",
    "\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.core.display import display, HTML\n",
    "# display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon Fashion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full CSJ fashion file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_pickle('Data/df_amazon_csj_with_styles')\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSJ 0.63m user above 5, r_u_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('Data/df_amazon_csj_with_styles_0.63m_u_above_5_rui')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MovieLens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.7m user above 5 r_u_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_pickle('Data/ml_0.7_u_above_5')\n",
    "# print('rating interval:', df.rating.unique().min(), ',', df.rating.unique().max())\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration\n",
    "\n",
    "First filtering active users and rated items with x or more ratings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ratings = df.groupby('user')['rating'].count()\n",
    "item_ratings = df.groupby('item')['rating'].count()\n",
    "norpu = user_ratings.mean()\n",
    "norpi = item_ratings.mean()\n",
    "total_users = df.user.unique().size\n",
    "total_items = df.item.unique().size\n",
    "sparseness = 1 - len(df) / (len(df['user'].unique()) * len(df['item'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.sample(frac=0.9, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('rows ', len(df), '\\n#ratings', len(df[df['rating'] != 0]), '\\n#ratings/user', round(norpu,2), '\\n#ratings/item', round(norpi,2), '\\naverage rating', \"{0:.2f}\".format(np.average(df['rating'])), '\\n#users ', df['user'].unique().size, '\\n#items ', df['item'].unique().size, '\\nsparse ', round(sparseness,5), '%')\n",
    "df.hist(column='rating', bins=5, grid=False)\n",
    "plt.title('Rating Distribution')\n",
    "plt.xlabel('Rating')\n",
    "plt.xticks(range(1,6))\n",
    "plt.savefig('Plots/Deliverables/rating_dist_ml')\n",
    "plt.show()\n",
    "\n",
    "plt.hist(item_ratings, bins = 1000)\n",
    "plt.xlim([0,100])\n",
    "plt.title('#ratings per item distribution (1000 bins)')\n",
    "plt.xlabel('Items')\n",
    "plt.ylabel('Count')\n",
    "plt.savefig('Plots/Deliverables/#ratings_per_item_dist_ml')\n",
    "plt.show()\n",
    "\n",
    "plt.hist(user_ratings, bins = 100)\n",
    "plt.xlim([0,30])\n",
    "plt.title('#ratings per user distribution (100 bins)')\n",
    "plt.xlabel('Users')\n",
    "plt.ylabel('Count')\n",
    "plt.savefig('Plots/Deliverables/#ratings_per_user_dist_ml')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification Analysis\n",
    "Only keep verified ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('verified:', df['verified'].sum() / len(df))\n",
    "df = df[df['verified']==True]\n",
    "print('verified:', df['verified'].sum() / len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['verified'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r = df['rating']\n",
    "# df['rating'] = df['verified']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_og = df\n",
    "df = df_og.sample(frac=0.1, random_state=1234)\n",
    "total_users = df.user.unique().size\n",
    "total_items = df.item.unique().size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep\n",
    "Create new ids for users and items that match the row and column indices of the user-item interaction matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(df):\n",
    "    items = df['item'].unique()\n",
    "    itemsDF = pd.DataFrame(data=items, columns=['original_item_id'])\n",
    "    itemsDF['new_item_id'] = itemsDF.index\n",
    "\n",
    "    users = df['user'].unique()\n",
    "    usersDF = pd.DataFrame(data=users, columns=['original_user_id'])\n",
    "    usersDF['new_user_id'] = usersDF.index\n",
    "\n",
    "    ratingDF = df.merge(itemsDF, left_on='item', right_on='original_item_id')\n",
    "    ratingDF = ratingDF.drop(columns=['original_item_id'])\n",
    "\n",
    "    ratingDF = ratingDF.merge(usersDF, left_on='user', right_on='original_user_id')\n",
    "    ratingDF = ratingDF.drop(columns=['original_user_id'])\n",
    "\n",
    "    df_new_ids = ratingDF\n",
    "    print('Full data #row: ', df_new_ids.shape[0])\n",
    "    \n",
    "    return df_new_ids\n",
    "\n",
    "df_new_ids = transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leave-one-out train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leave_x_out(full_data, leave_out):\n",
    "    # Input: data must be formatted by func: tranfsorm\n",
    "    # Output: full_data = without all entries in leave one out set\n",
    "    #         leave_one_out_set = data with one user and one item from full_data\n",
    "    \n",
    "    full_data['index'] = full_data.index\n",
    "    user_items_ind = full_data.groupby('new_user_id')['index'].apply(list)\n",
    "    index_to_drop = []\n",
    "    \n",
    "    for indices in user_items_ind:\n",
    "        if len(indices) > leave_out:\n",
    "            for to_leave_out in range(leave_out):\n",
    "                index = indices[- to_leave_out]\n",
    "                index_to_drop.append(index)\n",
    "    \n",
    "    leave_one_out_set = full_data.loc[index_to_drop]\n",
    "    full_data_leave_one_out = full_data.drop(index_to_drop)\n",
    "    \n",
    "    return full_data_leave_one_out.drop(columns=['index']), leave_one_out_set.drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_matrices(data, n_users, n_items):\n",
    "        r = data['new_user_id']\n",
    "        c = data['new_item_id']\n",
    "        d = data['rating']\n",
    "        m = sparse.csr_matrix((d, (r, c)), shape=(n_users, n_items))\n",
    "        m_ones = m.copy()\n",
    "        m_ones[m_ones > 0] = 1\n",
    "                               \n",
    "        return m, m_ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = leave_x_out(df_new_ids, 2)\n",
    "val_set, test_set = leave_x_out(test_set, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Personalized Ranking\n",
    "- Paper: https://arxiv.org/pdf/1205.2618.pdf\n",
    "- Code:  https://github.com/valerystrizh/bpr/blob/master/BPR.java"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPR():\n",
    "    def __init__(self, total_users, total_items, params):\n",
    "        self.total_users = total_users\n",
    "        self.total_items = total_items\n",
    "        self.nolf = params['nolf']\n",
    "        self.n_iterations = params['n_iterations']\n",
    "        self.alpha = params['alpha']\n",
    "        self.reg_user = params['reg_user']\n",
    "        self.reg_item = params['reg_item']\n",
    "        self.alpha_decay = self.alpha / self.n_iterations\n",
    "        self.incr_to_decay = False\n",
    "        self.model = {}\n",
    "        \n",
    "    def fit(self, train_set, val_set):\n",
    "        #Init\n",
    "        p = np.random.normal(0, .1, (self.total_users, self.nolf))  # users\n",
    "        q = np.random.normal(0, .1, (self.total_items, self.nolf))  # items\n",
    "        train_ratings, train_ones = self.create_matrices(train_set)\n",
    "#         ui_tuples = [tuple(x) for x in train_set[['new_user_id', 'new_item_id', 'verified']].to_numpy()] #positive\n",
    "        user_items = train_set.groupby('new_user_id')['new_item_id'].apply(list)\n",
    "        \n",
    "        #Loop\n",
    "        loss_list = []\n",
    "        alphas = []\n",
    "        for iteration in range(self.n_iterations):\n",
    "            it_loss = 0\n",
    "            for sample in range(train_ones.size):\n",
    "                u = int(np.random.uniform(0, self.total_users))\n",
    "                u_items = user_items[u]\n",
    "                i = random.choice(u_items)\n",
    "            \n",
    "                j = int(np.random.uniform(0, self.total_items)) # neg item\n",
    "                j_v = int(train_ones[u,j]) # Value, NEEDED?\n",
    "                \n",
    "                while j == i or j_v > 0: # j cannot be the same item or an item with a 1\n",
    "                    j = int(random.sample(range(self.total_items), 1)[0])\n",
    "                    j_v = int(train_ones[u,j])\n",
    "\n",
    "                pos_item_pred = self.predict(u,i,p,q)\n",
    "                neg_item_pred = self.predict(u,j,p,q)\n",
    "                diff = pos_item_pred - neg_item_pred\n",
    "\n",
    "                loss_value = - np.log(self.sigmoid(diff)) #NEGATIVE?\n",
    "                it_loss += loss_value\n",
    "\n",
    "                diff_deriv = self.sigmoid(- diff)\n",
    "\n",
    "                for f in range(self.nolf): # update each factor (see notes for derivatives)\n",
    "                    p[u,f] += self.alpha * (diff_deriv * (q[i,f] - q[j,f]) - self.reg_user * p[u,f])\n",
    "                    q[i,f] += self.alpha * (diff_deriv * p[u,f] - self.reg_item * q[i,f])\n",
    "                    q[j,f] += self.alpha * (diff_deriv * (-p[u,f]) - self.reg_item * q[j,f])\n",
    "\n",
    "                    it_loss += self.reg_user * p[u,f] * p[u,f] + self.reg_item * q[i,f] * q[i,f] + self.reg_item * q[j,f] * q[j,f]\n",
    "            \n",
    "            if iteration > 0:\n",
    "                self.update_alpha(loss_list[-1], it_loss)\n",
    "                \n",
    "            print('iteration:', iteration, 'loss:', round(it_loss,2), 'alpha:', self.alpha)\n",
    "            alphas.append(self.alpha)\n",
    "            loss_list.append(it_loss)\n",
    "        \n",
    "        self.model['p'] = p\n",
    "        self.model['q'] = q\n",
    "        self.model['train_loss'] = loss_list\n",
    "        self.model['learning_rate'] = alphas\n",
    "        \n",
    "        \n",
    "    def create_matrices(self, data):\n",
    "        r = data['new_user_id']\n",
    "        c = data['new_item_id']\n",
    "        d = data['rating']\n",
    "        m = sparse.csr_matrix((d, (r, c)), shape=(self.total_users, self.total_items))\n",
    "        m_ones = m.copy()\n",
    "        m_ones[m_ones > 0] = 1\n",
    "                               \n",
    "        return m, m_ones\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + math.exp(-x))\n",
    "    \n",
    "    def predict(self, u, i, p, q):\n",
    "        \"\"\"Using MF now but can be any prediction algorithm\"\"\"\n",
    "        return np.dot(p[u], q[i].T)\n",
    "    \n",
    "    def update_alpha(self, last_loss, it_loss):\n",
    "        if(last_loss < it_loss):\n",
    "            self.alpha = 0.5 * self.alpha\n",
    "            self.incr_to_decay = True\n",
    "            return\n",
    "        elif(self.incr_to_decay):\n",
    "            self.alpha = (1 - self.alpha_decay) * self.alpha\n",
    "            print(self.alpha)\n",
    "            return\n",
    "        \n",
    "        self.alpha = (1 + self.alpha_decay) * self.alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "\"nolf\":20, #Size of latent feature vectors\n",
    "\"n_iterations\":10,\n",
    "\"alpha\":0.004, # Impact of confidence\n",
    "          \n",
    "#Regularizers, still tweaking the values\n",
    "\"reg_user\":0.01,\n",
    "\"reg_item\":0.01,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BPR(total_users, total_items, params)\n",
    "model.fit(train_set, val_set)\n",
    "result = model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(res['p'][0], res['q'][0].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "375.742px",
    "left": "1048.75px",
    "top": "110.57px",
    "width": "239.774px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
