{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "import multiprocessing as mp\n",
    "import random\n",
    "\n",
    "from scipy import sparse\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.core.display import display, HTML\n",
    "# display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = 'C:/Users/robin.opdam/Google Drive/Thesis (Msc)/Thesis_shared_files/'\n",
    "path = '/Users/Robin/Google Drive/Thesis (Msc)/Thesis_shared_files/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon Fashion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/Amazon/'\n",
    "# file_name = 'Amazon_full' \n",
    "# file_name = 'Amazon_05_users' \n",
    "file_name = 'Amazon_01_users'\n",
    "# file_name = 'Amazon_005_users'\n",
    "# file_name = 'Amazon_001_users'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MovieLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = 'data/ML/'\n",
    "# file_name = 'ML_full' \n",
    "# file_name = 'ML_05_users'\n",
    "# file_name = 'ML_01_users'\n",
    "# file_name = 'ML_005_users'\n",
    "# file_name = 'ML_001_users'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "      <th>datetime</th>\n",
       "      <th>rating</th>\n",
       "      <th>item_id</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4983863</th>\n",
       "      <td>A39ZLL8ILVT2J8</td>\n",
       "      <td>B00FXSELCM</td>\n",
       "      <td>2014-03-24</td>\n",
       "      <td>3.0</td>\n",
       "      <td>155390</td>\n",
       "      <td>730619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7294092</th>\n",
       "      <td>A39ZLL8ILVT2J8</td>\n",
       "      <td>B00VDPQ884</td>\n",
       "      <td>2016-06-29</td>\n",
       "      <td>5.0</td>\n",
       "      <td>264632</td>\n",
       "      <td>730619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4809981</th>\n",
       "      <td>A39ZLL8ILVT2J8</td>\n",
       "      <td>B00EWC0W3W</td>\n",
       "      <td>2016-08-14</td>\n",
       "      <td>5.0</td>\n",
       "      <td>147315</td>\n",
       "      <td>730619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9337932</th>\n",
       "      <td>A39ZLL8ILVT2J8</td>\n",
       "      <td>B01EZKMD64</td>\n",
       "      <td>2016-10-03</td>\n",
       "      <td>5.0</td>\n",
       "      <td>362038</td>\n",
       "      <td>730619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8832820</th>\n",
       "      <td>A39ZLL8ILVT2J8</td>\n",
       "      <td>B01ABS4646</td>\n",
       "      <td>2016-12-22</td>\n",
       "      <td>5.0</td>\n",
       "      <td>335911</td>\n",
       "      <td>730619</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   user        item   datetime  rating item_id user_id\n",
       "4983863  A39ZLL8ILVT2J8  B00FXSELCM 2014-03-24     3.0  155390  730619\n",
       "7294092  A39ZLL8ILVT2J8  B00VDPQ884 2016-06-29     5.0  264632  730619\n",
       "4809981  A39ZLL8ILVT2J8  B00EWC0W3W 2016-08-14     5.0  147315  730619\n",
       "9337932  A39ZLL8ILVT2J8  B01EZKMD64 2016-10-03     5.0  362038  730619\n",
       "8832820  A39ZLL8ILVT2J8  B01ABS4646 2016-12-22     5.0  335911  730619"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle(path + data_path + file_name)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create new ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['item_id'] = df.item.astype('category').cat.codes\n",
    "df['user_id'] = df.user.astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split\n",
    "### Leave last item out of subset of users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leave_users_out(full_data, leave_out, seed=1234):\n",
    "    np.random.seed(seed)\n",
    "    full_data['index'] = full_data.index\n",
    "    user_index_df = full_data.groupby('user')['index'].apply(list)\n",
    "    users = np.random.choice(list(user_index_df.index), leave_out, replace=False)\n",
    "    users_indices = []\n",
    "    \n",
    "    for user in users:\n",
    "        users_indices.extend(user_index_df.loc[user])\n",
    "    \n",
    "    sub_set = full_data.loc[users_indices]\n",
    "    remaining = full_data.drop(users_indices)\n",
    "    \n",
    "    return remaining.drop(columns=['index']), sub_set.drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leave_last_x_out(full_data, n_users, leave_out=1, seed=1234):\n",
    "    # Input: data must contain user_id\n",
    "    # Output: full_data = without all last (time order) entries in leave one out set\n",
    "    #         leave_one_out_set = data with one user and one item from full_data\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    full_data['index'] = full_data.index\n",
    "    user_items_ind = full_data.groupby('user_id')['index'].apply(list)\n",
    "    users = full_data.user_id.unique()\n",
    "    leave_out_indices = []\n",
    "    users_picked = []\n",
    "    \n",
    "    for i in range(n_users):\n",
    "        random_user = np.random.choice(users)\n",
    "        item_indices = user_items_ind[random_user] # random user's items indices\n",
    "        while random_user not in users_picked and len(item_indices) <= leave_out: # needs to have more items than to leave out, or deleting users\n",
    "            random_user = np.random.choice(users)\n",
    "            item_indices = user_items_ind[random_user]\n",
    "            \n",
    "        users_picked.append(random_user)\n",
    "        leave_out_indices.extend(item_indices[-leave_out:])\n",
    "    \n",
    "    leave_out_set = full_data.loc[leave_out_indices] # the last items of n_users users with n_item > leave_out\n",
    "    full_data_leave_one_out = full_data.drop(leave_out_indices) # drops last items for n_users users\n",
    "    \n",
    "    return full_data_leave_one_out.drop(columns=['index']), leave_out_set.drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split(df, batch_size, val_perc, test_perc, n_items_val, n_items_test, stats=True):\n",
    "    # Input: df with user and item id, batch size for CFRNN data, val and test perc of users\n",
    "    #        number of last items to leave out for val and test set\n",
    "    # Output:full_data = total users and items of the original df, \n",
    "    #        Train, validation and test sets\n",
    "    \n",
    "    total_users = len(df.user_id.unique()) # Need all users for BPR\n",
    "    total_items = len(df.item_id.unique()) # Need all items for CFRNN\n",
    "    \n",
    "    users_to_remove = len(df.user_id.unique())%batch_size #Batch size compatible for CFRNN\n",
    "    df_new, deleted_users = leave_users_out(df, users_to_remove)\n",
    "\n",
    "    test_users = int(test_perc*total_users) # Number of users to be used for testing\n",
    "    test_last_items = n_items_test # Items to be removed from test users in train set and used in test set\n",
    "\n",
    "    val_users = int(val_perc*total_users) -1\n",
    "    val_last_items = n_items_val\n",
    "    \n",
    "    train_set, test_set = leave_last_x_out(df_new, test_users, test_last_items)\n",
    "    train_set, val_set = leave_last_x_out(train_set, val_users, val_last_items)\n",
    "    \n",
    "    if stats:\n",
    "        print('Total number of items:', total_items)\n",
    "        print('Total users:', total_users)\n",
    "        print('Number of train users:', len(train_set.user_id.unique()))\n",
    "        print('Number of test users:', test_users)\n",
    "        print('Number of validation users:', val_users, '\\n')\n",
    "        print('Users deleted:', len(deleted_users.user_id.unique()))\n",
    "    \n",
    "    return total_users, total_items, train_set, val_set, test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "val_perc = test_perc = 0.1\n",
    "n_last_items_val = n_last_items_test = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of items: 247465\n",
      "Total users: 121372\n",
      "Number of train users: 121344\n",
      "Number of test users: 12137\n",
      "Number of validation users: 12136 \n",
      "\n",
      "Users deleted: 28\n"
     ]
    }
   ],
   "source": [
    "total_users, total_items, train_set, val_set, test_set = train_val_test_split(df, BATCH_SIZE, val_perc, test_perc, n_last_items_val, n_last_items_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Personalized Ranking\n",
    "- Paper: https://arxiv.org/pdf/1205.2618.pdf\n",
    "- Code:  https://github.com/valerystrizh/bpr/blob/master/BPR.java"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPR():\n",
    "    def __init__(self, total_users, total_items, params):\n",
    "        self.total_users = total_users\n",
    "        self.total_items = total_items\n",
    "        self.nolf = params['nolf']\n",
    "        self.n_iterations = params['n_iterations']\n",
    "        self.sample_size = params['sample_size']\n",
    "        self.seed = params['seed']\n",
    "        self.alpha = params['alpha']\n",
    "        self.reg_user = params['reg_user']\n",
    "        self.reg_item = params['reg_item']\n",
    "        self.alpha_decay = self.alpha / self.n_iterations\n",
    "        \n",
    "        self.model = {}\n",
    "        self.model['val_auc'] = []\n",
    "        self.user_items = pd.DataFrame()\n",
    "        self.train_users = []\n",
    "        self.train_items = []\n",
    "        \n",
    "        self.val_user_items = pd.DataFrame()\n",
    "        self.val_users = []\n",
    "        \n",
    "    def fit(self, train_set, val_set=[]):\n",
    "        # Init\n",
    "        s = time.time()\n",
    "        np.random.seed(self.seed)\n",
    "        p = np.random.normal(0, .1, (self.total_users, self.nolf))  # users\n",
    "        q = np.random.normal(0, .1, (self.total_items, self.nolf))  # items\n",
    "        \n",
    "        ## Used for sampling\n",
    "        self.user_items = train_set.groupby('user_id')['item_id'].apply(list)\n",
    "        self.train_users  = train_set.user_id.unique()\n",
    "        self.train_items = train_set.item_id.unique()\n",
    "        \n",
    "        if len(val_set) > 0:\n",
    "            ## Used for validating\n",
    "            self.val_user_items = val_set.groupby('user_id')['item_id'].apply(list)\n",
    "            self.val_users = val_set.user_id.unique()\n",
    "        \n",
    "        ## Track losses and alphas used\n",
    "        loss_list = []\n",
    "        alphas = []\n",
    "        \n",
    "        ## Create samples for all iterations\n",
    "        all_uij_samples = self.sample()\n",
    "        \n",
    "        # Training Loop\n",
    "        for iteration in range(self.n_iterations):\n",
    "            it_loss = 0\n",
    "            uij_samples = all_uij_samples[iteration]\n",
    "            \n",
    "            for uij_sample in uij_samples:\n",
    "                u = uij_sample[0]\n",
    "                i = uij_sample[1]\n",
    "                j = uij_sample[2]\n",
    "                \n",
    "                ## Calculate the difference between positive and negative item\n",
    "                diff = np.dot(p[u], (q[i] - q[j]).T)\n",
    "                \n",
    "                ## Obtain loss \n",
    "                loss_value = - np.log(self.sigmoid(diff))\n",
    "                regulariser = self.reg_user * np.dot(p[u], p[u]) + self.reg_item * np.dot(q[i],q[i]) + self.reg_item/10 * np.dot(q[j], q[j])\n",
    "                it_loss += (loss_value + regulariser) / self.sample_size\n",
    "                \n",
    "                ## Derivative of the difference for update \n",
    "                diff_deriv = self.sigmoid(- diff)\n",
    "                \n",
    "                ## Update the factors of the latent features, using their respective derivatives\n",
    "                ## See http://ethen8181.github.io/machine-learning/recsys/4_bpr.html\n",
    "                p[u] += self.alpha * (diff_deriv * (q[i] - q[j]) - self.reg_user * p[u])\n",
    "                q[i] += self.alpha * (diff_deriv * p[u] - self.reg_item * q[i])\n",
    "                q[j] += self.alpha * (diff_deriv * (-p[u]) - self.reg_item * q[j])\n",
    "            \n",
    "            ## Store iteration variables\n",
    "            self.model['p'] = p\n",
    "            self.model['q'] = q\n",
    "            \n",
    "            if len(val_set) > 0: # TO DO: safe best & early stopping\n",
    "                val_auc = self.AUC()\n",
    "                self.model['val_auc'].append(val_auc)\n",
    "                print('iteration:', iteration, ' loss:', round(it_loss,6), ' val AUC:', val_auc)#, ' val prec@' + str(val_rank), ':', round(prec_at,5), ' val rec@' + str(val_rank), ':', round(rec_at,5), '  Hits:', hitcount)#'  alpha:', self.alpha)\n",
    "            else:\n",
    "                print('iteration:', iteration, ' loss:', round(it_loss,6))\n",
    "                \n",
    "            if iteration > 0:\n",
    "                self.update_alpha(loss_list[-1], it_loss)\n",
    "                \n",
    "            alphas.append(self.alpha)\n",
    "            loss_list.append(it_loss)\n",
    "        \n",
    "        # Store train values\n",
    "        train_time = time.time() - s\n",
    "        self.model['train_loss'] = loss_list\n",
    "        self.model['learning_rate'] = alphas\n",
    "        self.model['train_time'] = train_time\n",
    "        \n",
    "        \n",
    "    def sample(self):\n",
    "        n_samples = self.n_iterations\n",
    "        sample_size = int(self.sample_size)\n",
    "        print('Creating', str(n_samples), 'samples of length', str(sample_size))\n",
    "        all_uij_samples = []\n",
    "        \n",
    "        for n in range(n_samples):\n",
    "            uij_samples = []\n",
    "            for s in range(sample_size):\n",
    "                u = int(np.random.choice(self.train_users))\n",
    "                u_items = self.user_items[u]\n",
    "                i = random.choice(u_items)\n",
    "                j = int(np.random.choice(self.train_items)) \n",
    "                while j in u_items: #neg item j cannot be in the set of pos items of user u\n",
    "                    j = int(np.random.choice(self.train_items))\n",
    "                \n",
    "                uij_samples.append([u,i,j])\n",
    "                \n",
    "            all_uij_samples.append(uij_samples)\n",
    "            \n",
    "        return all_uij_samples\n",
    "        \n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + math.exp(-x))\n",
    "    \n",
    "    \n",
    "    def update_alpha(self, last_loss, it_loss):\n",
    "        if(last_loss < it_loss): #bold driver\n",
    "            self.alpha = 0.5 * self.alpha\n",
    "            return\n",
    "        \n",
    "        self.alpha = (1 - self.alpha_decay) * self.alpha\n",
    "    \n",
    "    def AUC(self):\n",
    "        auc = 0.0\n",
    "        n_users = len(self.val_users)\n",
    "\n",
    "        for u in self.val_users:\n",
    "            y_pred = np.dot(self.model['p'][u], self.model['q'].T)\n",
    "            y_true = np.zeros(self.total_items)\n",
    "            y_true[self.val_user_items[u]] = 1\n",
    "            auc += roc_auc_score(y_true, y_pred)\n",
    "        \n",
    "        auc /= n_users\n",
    "        return auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "\"nolf\":20, #Size of latent feature vectors\n",
    "\"n_iterations\":10, #around 20 is sufficient\n",
    "\"sample_size\":0.5*len(train_set),\n",
    "\"seed\":1234,\n",
    "\"alpha\":0.1, # Learning rate\n",
    "          \n",
    "#Regularizers, still tweaking the values\n",
    "\"reg_user\":0.1,\n",
    "\"reg_item\":0.1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Amazon_01_users'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 10 samples of length 517722\n",
      "iteration: 0  loss: 0.729974\n",
      "iteration: 1  loss: 0.722041\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-8f262a4ea4e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mBPR_amazon_01\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBPR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_users\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mBPR_amazon_01\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-51-fbcfa4b4d0e1>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_set, val_set)\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0;31m## Update the factors of the latent features, using their respective derivatives\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0;31m## See http://ethen8181.github.io/machine-learning/recsys/4_bpr.html\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdiff_deriv\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreg_user\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m                 \u001b[0mq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdiff_deriv\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreg_item\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0mq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdiff_deriv\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreg_item\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "BPR_amazon_01 = BPR(total_users, total_items, params)\n",
    "BPR_amazon_01.fit(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_results(results, log_path, res_name, file_name, params):\n",
    "        result_info = {'train_loss': results['train_loss'], 'val_auc':results['val_auc'], 'train_speed': results['train_time'], 'lr':results['learning_rate'], 'file':file_name}\n",
    "        other_info = {'p':results['p'], 'q':results['q']} #'train_size':train_size, 'test_size':test_size, 'val_size':val_size}\n",
    "        final_log = dict(result_info, **params, **other_info)\n",
    "\n",
    "        if not os.path.exists(log_path + res_name):\n",
    "            df_results = pd.DataFrame(columns=final_log.keys())\n",
    "            df_results.to_pickle(log_path + res_name)\n",
    "            print('new results created')\n",
    "\n",
    "        else:\n",
    "            df_results = pd.read_pickle(log_path + res_name)\n",
    "            print('results added')\n",
    "\n",
    "        df_results = df_results.append(final_log, ignore_index=True)\n",
    "        df_results.to_pickle(log_path + res_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = path + 'Results/BPR/'\n",
    "res_name = 'BPR_models'\n",
    "# store_results(BPR_amazon_001.model, log_path, res_name, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = pd.read_pickle(log_path + res_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last = len(df_res['train_loss']) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df_res['train_loss'][last])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df_res['lr'][last])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_predictions(model, test_set, rank_at, multiprocessing=True):\n",
    "    import eval_rank_bpr\n",
    "    \n",
    "    s = time.time()\n",
    "    users = test_set.user_id.unique()\n",
    "    items = test_set.item_id.unique()\n",
    "    test_user_items = test_set.groupby('user_id')['item_id'].apply(list)\n",
    "    ranked_df = pd.DataFrame(columns=['pred_items_ranked', 'true_id'], index=users)\n",
    "    \n",
    "    if multiprocessing:\n",
    "        mp_splits = 4\n",
    "        users_split = np.array_split(users, mp_splits)\n",
    "\n",
    "        if __name__ == '__main__':\n",
    "            pool = mp.Pool(processes = mp_splits)\n",
    "            ranked = pool.map(eval_rank_bpr.eval_rank_bpr, [\n",
    "                                                    [model, users_split[0], items, test_user_items, rank_at], \n",
    "                                                    [model, users_split[1], items, test_user_items, rank_at], \n",
    "                                                    [model, users_split[2], items, test_user_items, rank_at], \n",
    "                                                    [model, users_split[3], items, test_user_items, rank_at]\n",
    "                                                            ])\n",
    "            pool.close()\n",
    "\n",
    "            for i in range(mp_splits):\n",
    "                ranked_df = pd.concat([ranked_df, ranked[i]])\n",
    "    \n",
    "    else:\n",
    "        pred_items_ranked = []\n",
    "        true_items_list = []\n",
    "        \n",
    "        for u in users:\n",
    "            user_item_pred_score = []\n",
    "            true_items = []\n",
    "            for true_item in test_user_items.loc[u]:\n",
    "                true_items.append(true_item)\n",
    "\n",
    "            predictions = np.dot(model['p'][u], model['q'].T)\n",
    "            ids = np.argpartition(predictions, -rank_at)[-rank_at:]\n",
    "            best_ids = np.argsort(predictions[ids])[::-1]\n",
    "            best = ids[best_ids]\n",
    "            \n",
    "            pred_items_ranked.append(best)\n",
    "            true_items_list.append(true_items)\n",
    "\n",
    "        ranked_df['pred_items_ranked'] = pred_items_ranked\n",
    "        ranked_df['true_id'] = true_items_list\n",
    "\n",
    "    print('Ranking time:', round(time.time() - s,2))\n",
    "    \n",
    "    return ranked_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(ranked_df, steps, max_rank):\n",
    "    s = time.time()\n",
    "    ranks_at = [1] + [i for i in range(steps, max_rank + steps, steps)]\n",
    "    hitcounts = []\n",
    "    recs_at = []\n",
    "    precs_at = []\n",
    "    metrics = pd.DataFrame(columns=['rank_at', 'hitcounts', 'recall', 'precision'])\n",
    "    for rank in ranks_at:\n",
    "        hitcount = 0\n",
    "        for i, row in ranked_df.iterrows():\n",
    "            hitcount +=  len(set(row['true_id']) & set(row['pred_items_ranked'][:rank]))\n",
    "\n",
    "        prec_at = hitcount / rank / len(ranked_df)\n",
    "        rec_at = hitcount / len(ranked_df.iloc[0]['true_id']) / len(ranked_df)\n",
    "\n",
    "        hitcounts.append(hitcount)                     \n",
    "        recs_at.append(rec_at)\n",
    "        precs_at.append(prec_at)\n",
    "\n",
    "    metrics['rank_at'] = ranks_at\n",
    "    metrics['hitcounts'] = hitcounts\n",
    "    metrics['recall'] = recs_at\n",
    "    metrics['precision'] = precs_at\n",
    "    print('Obtaining metrics time:', round(time.time() - s,2))\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_set.user_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = {'p':df_res['p'][1], 'q':df_res['q'][1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_at = 20\n",
    "ranked_df = rank_predictions(model, test_set, rank_at, True)\n",
    "\n",
    "steps = 5\n",
    "metrics = get_metrics(ranked_df, steps, rank_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.to_pickle(path + 'Results/BPR/metrics_' + file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_pickle(path + 'Results/metrics_' + file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(metrics['rank_at'], metrics['precision'])\n",
    "plt.xlabel('Rank')\n",
    "plt.ylabel('precision@')\n",
    "plt.title('Precision for different rank values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(metrics['rank_at'], metrics['recall'])\n",
    "plt.xlabel('Rank')\n",
    "plt.ylabel('recall@')\n",
    "plt.title('Recall')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_run(file_names, file_paths, all_params, sample_percs):\n",
    "    runs = 0\n",
    "    for file_name, file_path, params, sample_perc in zip(file_names, file_paths, all_params, sample_percs):\n",
    "        # Init \n",
    "#         print('\\n\\n', '='*50)\n",
    "        df = pd.read_pickle(file_path + file_name)\n",
    "        df['item_id'] = df.item.astype('category').cat.codes\n",
    "        df['user_id'] = df.user.astype('category').cat.codes\n",
    "        \n",
    "        # Create Train and Test sets\n",
    "        BATCH_SIZE = 64\n",
    "        df_og = df\n",
    "        \n",
    "        users_to_remove = len(df_og.user_id.unique())%BATCH_SIZE #Batch size compatible for CFRNN\n",
    "        df, deleted_users = leave_users_out(df_og, users_to_remove)\n",
    "\n",
    "        total_users = len(df_og.user_id.unique()) # Need all users for BPR\n",
    "        total_items = len(df_og.item_id.unique()) # Need all items for CFRNN\n",
    "\n",
    "        test_users = int(0.1*total_users) # Number of users to be used for testing\n",
    "        test_last_items = 1 # Items to be removed from test users in train set and used in test set\n",
    "\n",
    "        val_users = int(0.1*total_users) -1\n",
    "        val_last_items = 1\n",
    "        \n",
    "        train_set, test_set = leave_last_x_out(df, test_users, test_last_items)\n",
    "        train_set, val_set = leave_last_x_out(train_set, val_users, val_last_items)\n",
    "        \n",
    "        train_users = len(train_set.user_id.unique())\n",
    "        \n",
    "#         print('Run:', runs + 1, '\\nFile:', file_name,\n",
    "#               '\\nTrain users:', train_users, '\\nTest users:', test_users,\n",
    "#               '\\nVal users:', val_users)\n",
    "        \n",
    "        # Model Run\n",
    "        params['sample_size'] = sample_perc*len(train_set)\n",
    "        model = BPR(total_users, total_items, params)\n",
    "        model.fit(train_set, val_set)\n",
    "        \n",
    "        log_path = path + 'Results/BPR/Grid_Search/'\n",
    "        res_name = 'BPR_models'\n",
    "        store_results(model.model, log_path, res_name, file_name, params)\n",
    "        \n",
    "        rank_at = 20\n",
    "        steps = 5\n",
    "        \n",
    "        ranked_df = rank_predictions(model.model, test_set, rank_at, False)\n",
    "        metrics = get_metrics(ranked_df, steps, rank_at)\n",
    "#         print('Results')\n",
    "#         print(metrics)\n",
    "        \n",
    "#         metrics.to_pickle(path + 'Results/BPR/Grid_Search/metrics_' + file_name)\n",
    "        \n",
    "        runs += 1\n",
    "        \n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_amazon_001 = {\n",
    "\"nolf\":20, # Size of latent feature vectors\n",
    "\"n_iterations\":70, # Depends on number of items and users\n",
    "\"sample_size\":1, # Adjusted in loop to 0.5*len(train_set)\n",
    "\"seed\":1234,\n",
    "\"alpha\":0.1, # Learning rate\n",
    "          \n",
    "#Regularizers\n",
    "\"reg_user\":0.08, # should be in proportion to the number of items \n",
    "\"reg_item\":0.08,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_amazon_005 = {\n",
    "\"nolf\":20, # Size of latent feature vectors\n",
    "\"n_iterations\":70, # Depends on number of items and users\n",
    "\"sample_size\":1, # Adjusted in loop to 0.5*len(train_set)\n",
    "\"seed\":1234,\n",
    "\"alpha\":0.1, # Learning rate\n",
    "          \n",
    "#Regularizers\n",
    "\"reg_user\":0.05, # should be in proportion to the number of items \n",
    "\"reg_item\":0.05,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_amazon_01 = {\n",
    "\"nolf\":20, # Size of latent feature vectors\n",
    "\"n_iterations\":70, # Depends on number of items and users\n",
    "\"sample_size\":1, # Adjusted in loop to 0.5*len(train_set)\n",
    "\"seed\":1234,\n",
    "\"alpha\":0.1, # Learning rate\n",
    "          \n",
    "#Regularizers\n",
    "\"reg_user\":0.03, # should be in proportion to the number of items \n",
    "\"reg_item\":0.03,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " ==================================================\n",
      "Run: 1 \n",
      "File: Amazon_01_users \n",
      "Train users: 121344 \n",
      "Test users: 12137 \n",
      "Val users: 12136\n",
      "Creating 70 samples of length 103544\n",
      "iteration: 0  loss: 0.705499\n",
      "iteration: 1  loss: 0.704073\n",
      "iteration: 2  loss: 0.702937\n",
      "iteration: 3  loss: 0.701574\n",
      "iteration: 4  loss: 0.700031\n",
      "iteration: 5  loss: 0.698568\n",
      "iteration: 6  loss: 0.697211\n",
      "iteration: 7  loss: 0.695605\n",
      "iteration: 8  loss: 0.693686\n",
      "iteration: 9  loss: 0.691666\n",
      "iteration: 10  loss: 0.689464\n",
      "iteration: 11  loss: 0.687243\n",
      "iteration: 12  loss: 0.684389\n",
      "iteration: 13  loss: 0.682038\n",
      "iteration: 14  loss: 0.679317\n",
      "iteration: 15  loss: 0.676951\n",
      "iteration: 16  loss: 0.673926\n",
      "iteration: 17  loss: 0.671167\n",
      "iteration: 18  loss: 0.668671\n",
      "iteration: 19  loss: 0.665844\n",
      "iteration: 20  loss: 0.662993\n",
      "iteration: 21  loss: 0.660027\n",
      "iteration: 22  loss: 0.657539\n",
      "iteration: 23  loss: 0.654365\n",
      "iteration: 24  loss: 0.651999\n",
      "iteration: 25  loss: 0.649196\n",
      "iteration: 26  loss: 0.645893\n",
      "iteration: 27  loss: 0.643724\n",
      "iteration: 28  loss: 0.640642\n",
      "iteration: 29  loss: 0.637553\n",
      "iteration: 30  loss: 0.634633\n",
      "iteration: 31  loss: 0.632675\n",
      "iteration: 32  loss: 0.62958\n",
      "iteration: 33  loss: 0.626819\n",
      "iteration: 34  loss: 0.623029\n",
      "iteration: 35  loss: 0.62045\n",
      "iteration: 36  loss: 0.618369\n",
      "iteration: 37  loss: 0.61601\n",
      "iteration: 38  loss: 0.612229\n",
      "iteration: 39  loss: 0.60898\n",
      "iteration: 40  loss: 0.606603\n",
      "iteration: 41  loss: 0.604102\n",
      "iteration: 42  loss: 0.601419\n",
      "iteration: 43  loss: 0.599295\n",
      "iteration: 44  loss: 0.594611\n",
      "iteration: 45  loss: 0.592945\n",
      "iteration: 46  loss: 0.590076\n",
      "iteration: 47  loss: 0.587355\n",
      "iteration: 48  loss: 0.583899\n",
      "iteration: 49  loss: 0.581907\n",
      "iteration: 50  loss: 0.578098\n",
      "iteration: 51  loss: 0.575759\n",
      "iteration: 52  loss: 0.573237\n",
      "iteration: 53  loss: 0.570748\n",
      "iteration: 54  loss: 0.567227\n",
      "iteration: 55  loss: 0.565313\n",
      "iteration: 56  loss: 0.562269\n",
      "iteration: 57  loss: 0.559117\n",
      "iteration: 58  loss: 0.556313\n",
      "iteration: 59  loss: 0.55337\n",
      "iteration: 60  loss: 0.551297\n",
      "iteration: 61  loss: 0.549085\n",
      "iteration: 62  loss: 0.546835\n",
      "iteration: 63  loss: 0.543752\n",
      "iteration: 64  loss: 0.540586\n",
      "iteration: 65  loss: 0.538263\n",
      "iteration: 66  loss: 0.536038\n",
      "iteration: 67  loss: 0.534071\n",
      "iteration: 68  loss: 0.531458\n",
      "iteration: 69  loss: 0.52844\n",
      "results added\n",
      "Ranking time: 89.2\n",
      "Obtaining metrics time: 7.29\n",
      "Results\n",
      "   rank_at  hitcounts    recall  precision\n",
      "0        1        122  0.010557   0.010557\n",
      "1        5        536  0.046383   0.009277\n",
      "2       10        665  0.057546   0.005755\n",
      "3       15        715  0.061873   0.004125\n",
      "4       20        748  0.064728   0.003236\n",
      "\n",
      "\n",
      " ==================================================\n",
      "Run: 2 \n",
      "File: Amazon_005_users \n",
      "Train users: 60672 \n",
      "Test users: 6068 \n",
      "Val users: 6067\n",
      "Creating 70 samples of length 103799\n",
      "iteration: 0  loss: 0.712946\n",
      "iteration: 1  loss: 0.710143\n",
      "iteration: 2  loss: 0.707811\n",
      "iteration: 3  loss: 0.704931\n",
      "iteration: 4  loss: 0.702308\n",
      "iteration: 5  loss: 0.699384\n",
      "iteration: 6  loss: 0.696285\n",
      "iteration: 7  loss: 0.693031\n",
      "iteration: 8  loss: 0.689932\n",
      "iteration: 9  loss: 0.686208\n",
      "iteration: 10  loss: 0.682517\n",
      "iteration: 11  loss: 0.678455\n",
      "iteration: 12  loss: 0.674397\n",
      "iteration: 13  loss: 0.670439\n",
      "iteration: 14  loss: 0.666339\n",
      "iteration: 15  loss: 0.662808\n",
      "iteration: 16  loss: 0.658905\n",
      "iteration: 17  loss: 0.654876\n",
      "iteration: 18  loss: 0.650919\n",
      "iteration: 19  loss: 0.647477\n",
      "iteration: 20  loss: 0.643182\n",
      "iteration: 21  loss: 0.639042\n",
      "iteration: 22  loss: 0.63522\n",
      "iteration: 23  loss: 0.631649\n",
      "iteration: 24  loss: 0.627695\n",
      "iteration: 25  loss: 0.623888\n",
      "iteration: 26  loss: 0.619641\n",
      "iteration: 27  loss: 0.616076\n",
      "iteration: 28  loss: 0.612472\n",
      "iteration: 29  loss: 0.608698\n",
      "iteration: 30  loss: 0.604838\n",
      "iteration: 31  loss: 0.600908\n",
      "iteration: 32  loss: 0.597396\n",
      "iteration: 33  loss: 0.593969\n",
      "iteration: 34  loss: 0.589757\n",
      "iteration: 35  loss: 0.586553\n",
      "iteration: 36  loss: 0.582621\n",
      "iteration: 37  loss: 0.579649\n",
      "iteration: 38  loss: 0.575809\n",
      "iteration: 39  loss: 0.571741\n",
      "iteration: 40  loss: 0.569202\n",
      "iteration: 41  loss: 0.566043\n",
      "iteration: 42  loss: 0.56196\n",
      "iteration: 43  loss: 0.558937\n",
      "iteration: 44  loss: 0.555478\n",
      "iteration: 45  loss: 0.552935\n",
      "iteration: 46  loss: 0.549317\n",
      "iteration: 47  loss: 0.546761\n",
      "iteration: 48  loss: 0.543417\n",
      "iteration: 49  loss: 0.540605\n",
      "iteration: 50  loss: 0.537892\n",
      "iteration: 51  loss: 0.535126\n",
      "iteration: 52  loss: 0.532885\n",
      "iteration: 53  loss: 0.530365\n",
      "iteration: 54  loss: 0.527939\n",
      "iteration: 55  loss: 0.525153\n",
      "iteration: 56  loss: 0.523699\n",
      "iteration: 57  loss: 0.521056\n",
      "iteration: 58  loss: 0.519232\n",
      "iteration: 59  loss: 0.516732\n",
      "iteration: 60  loss: 0.515156\n",
      "iteration: 61  loss: 0.512262\n",
      "iteration: 62  loss: 0.511457\n",
      "iteration: 63  loss: 0.509574\n",
      "iteration: 64  loss: 0.506655\n",
      "iteration: 65  loss: 0.505724\n",
      "iteration: 66  loss: 0.50416\n",
      "iteration: 67  loss: 0.502188\n",
      "iteration: 68  loss: 0.500926\n",
      "iteration: 69  loss: 0.498869\n",
      "results added\n",
      "Ranking time: 31.88\n",
      "Obtaining metrics time: 3.8\n",
      "Results\n",
      "   rank_at  hitcounts    recall  precision\n",
      "0        1         53  0.009179   0.009179\n",
      "1        5        262  0.045376   0.009075\n",
      "2       10        345  0.059751   0.005975\n",
      "3       15        366  0.063388   0.004226\n",
      "4       20        383  0.066332   0.003317\n",
      "\n",
      "\n",
      " ==================================================\n",
      "Run: 3 \n",
      "File: Amazon_001_users \n",
      "Train users: 12096 \n",
      "Test users: 1213 \n",
      "Val users: 1212\n",
      "Creating 70 samples of length 51483\n",
      "iteration: 0  loss: 0.723339\n",
      "iteration: 1  loss: 0.716133\n",
      "iteration: 2  loss: 0.709826\n",
      "iteration: 3  loss: 0.703884\n",
      "iteration: 4  loss: 0.69811\n",
      "iteration: 5  loss: 0.691891\n",
      "iteration: 6  loss: 0.685736\n",
      "iteration: 7  loss: 0.679777\n",
      "iteration: 8  loss: 0.673808\n",
      "iteration: 9  loss: 0.667451\n",
      "iteration: 10  loss: 0.661591\n",
      "iteration: 11  loss: 0.655003\n",
      "iteration: 12  loss: 0.64879\n",
      "iteration: 13  loss: 0.643934\n",
      "iteration: 14  loss: 0.637455\n",
      "iteration: 15  loss: 0.632017\n",
      "iteration: 16  loss: 0.626575\n",
      "iteration: 17  loss: 0.620361\n",
      "iteration: 18  loss: 0.615417\n",
      "iteration: 19  loss: 0.610142\n",
      "iteration: 20  loss: 0.606078\n",
      "iteration: 21  loss: 0.601153\n",
      "iteration: 22  loss: 0.596549\n",
      "iteration: 23  loss: 0.592711\n",
      "iteration: 24  loss: 0.587875\n",
      "iteration: 25  loss: 0.585278\n",
      "iteration: 26  loss: 0.582152\n",
      "iteration: 27  loss: 0.578407\n",
      "iteration: 28  loss: 0.575795\n",
      "iteration: 29  loss: 0.573407\n",
      "iteration: 30  loss: 0.571513\n",
      "iteration: 31  loss: 0.569414\n",
      "iteration: 32  loss: 0.566201\n",
      "iteration: 33  loss: 0.565571\n",
      "iteration: 34  loss: 0.564362\n",
      "iteration: 35  loss: 0.562722\n",
      "iteration: 36  loss: 0.56244\n",
      "iteration: 37  loss: 0.560461\n",
      "iteration: 38  loss: 0.558741\n",
      "iteration: 39  loss: 0.558441\n",
      "iteration: 40  loss: 0.558229\n",
      "iteration: 41  loss: 0.556458\n",
      "iteration: 42  loss: 0.555383\n",
      "iteration: 43  loss: 0.55504\n",
      "iteration: 44  loss: 0.553557\n",
      "iteration: 45  loss: 0.55419\n",
      "iteration: 46  loss: 0.553916\n",
      "iteration: 47  loss: 0.553063\n",
      "iteration: 48  loss: 0.55338\n",
      "iteration: 49  loss: 0.552925\n",
      "iteration: 50  loss: 0.552905\n",
      "iteration: 51  loss: 0.552436\n",
      "iteration: 52  loss: 0.552312\n",
      "iteration: 53  loss: 0.552148\n",
      "iteration: 54  loss: 0.551952\n",
      "iteration: 55  loss: 0.552611\n",
      "iteration: 56  loss: 0.551652\n",
      "iteration: 57  loss: 0.551098\n",
      "iteration: 58  loss: 0.551308\n",
      "iteration: 59  loss: 0.551358\n",
      "iteration: 60  loss: 0.551669\n",
      "iteration: 61  loss: 0.551347\n",
      "iteration: 62  loss: 0.551655\n",
      "iteration: 63  loss: 0.551286\n",
      "iteration: 64  loss: 0.551096\n",
      "iteration: 65  loss: 0.551449\n",
      "iteration: 66  loss: 0.551242\n",
      "iteration: 67  loss: 0.551973\n",
      "iteration: 68  loss: 0.551953\n",
      "iteration: 69  loss: 0.551292\n",
      "results added\n",
      "Ranking time: 1.5\n",
      "Obtaining metrics time: 0.72\n",
      "Results\n",
      "   rank_at  hitcounts    recall  precision\n",
      "0        1          9  0.007799   0.007799\n",
      "1        5         55  0.047660   0.009532\n",
      "2       10         72  0.062392   0.006239\n",
      "3       15         78  0.067591   0.004506\n",
      "4       20         78  0.067591   0.003380\n"
     ]
    }
   ],
   "source": [
    "file_names = ['Amazon_01_users', 'Amazon_005_users', 'Amazon_001_users']\n",
    "file_paths = 3*[path + 'Data/Amazon/']\n",
    "all_params = [params_amazon_01, params_amazon_005, params_amazon_001]\n",
    "sample_percs = [0.1, 0.2, 0.5]\n",
    "\n",
    "seq_run(file_names, file_paths, all_params, sample_percs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search\n",
    "- Amazon_01_users\n",
    "- ML_01_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_run(file_name, file_path, params, sample_perc):\n",
    "    # Init \n",
    "#         print('\\n\\n', '='*50)\n",
    "    df = pd.read_pickle(file_path + file_name)\n",
    "    df['item_id'] = df.item.astype('category').cat.codes\n",
    "    df['user_id'] = df.user.astype('category').cat.codes\n",
    "\n",
    "    # Create Train and Test sets\n",
    "    BATCH_SIZE = 64\n",
    "    df_og = df\n",
    "\n",
    "    users_to_remove = len(df_og.user_id.unique())%BATCH_SIZE #Batch size compatible for CFRNN\n",
    "    df, deleted_users = leave_users_out(df_og, users_to_remove)\n",
    "\n",
    "    total_users = len(df_og.user_id.unique()) # Need all users for BPR\n",
    "    total_items = len(df_og.item_id.unique()) # Need all items for CFRNN\n",
    "\n",
    "    test_users = int(0.1*total_users) # Number of users to be used for testing\n",
    "    test_last_items = 1 # Items to be removed from test users in train set and used in test set\n",
    "\n",
    "    val_users = int(0.1*total_users) -1\n",
    "    val_last_items = 1\n",
    "\n",
    "    train_set, test_set = leave_last_x_out(df, test_users, test_last_items)\n",
    "    train_set, val_set = leave_last_x_out(train_set, val_users, val_last_items)\n",
    "\n",
    "    train_users = len(train_set.user_id.unique())\n",
    "\n",
    "#         print('Run:', runs + 1, '\\nFile:', file_name,\n",
    "#               '\\nTrain users:', train_users, '\\nTest users:', test_users,\n",
    "#               '\\nVal users:', val_users)\n",
    "\n",
    "    # Model Run\n",
    "    params['sample_size'] = sample_perc*len(train_set)\n",
    "    model = BPR(total_users, total_items, params)\n",
    "    model.fit(train_set)\n",
    "\n",
    "    rank_at = 20\n",
    "    steps = 5\n",
    "\n",
    "    ranked_df = rank_predictions(model.model, test_set, rank_at, False)\n",
    "    metrics = get_metrics(ranked_df, steps, rank_at)\n",
    "#         print('Results')\n",
    "#         print(metrics)\n",
    "\n",
    "#         metrics.to_pickle(path + 'Results/BPR/Grid_Search/metrics_' + file_name)\n",
    "\n",
    "    return model, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "\"nolf\":20, # Size of latent feature vectors\n",
    "\"n_iterations\":70, # Depends on number of items and users\n",
    "\"sample_size\":1, # Adjusted in loop to sample_perc*len(train_set)\n",
    "\"seed\":1234,\n",
    "\"alpha\":0.1, # Learning rate\n",
    "          \n",
    "#Regularizers\n",
    "\"reg_user\":0.03, # should be in proportion to the number of items \n",
    "\"reg_item\":0.03,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ============================================================ \n",
      " Run: 1 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 0.01\n",
      "NOLF:\t\t\t 10\n",
      "Sample Percentage:\t 0.05\n",
      "Creating 20 samples of length 51772\n",
      "iteration: 0  loss: 0.695285\n",
      "iteration: 1  loss: 0.695178\n",
      "iteration: 2  loss: 0.694636\n",
      "iteration: 3  loss: 0.694325\n",
      "iteration: 4  loss: 0.693895\n",
      "iteration: 5  loss: 0.693509\n",
      "iteration: 6  loss: 0.693267\n",
      "iteration: 7  loss: 0.692764\n",
      "iteration: 8  loss: 0.692529\n",
      "iteration: 9  loss: 0.691768\n",
      "iteration: 10  loss: 0.69114\n",
      "iteration: 11  loss: 0.690608\n",
      "iteration: 12  loss: 0.689646\n",
      "iteration: 13  loss: 0.68884\n",
      "iteration: 14  loss: 0.68814\n",
      "iteration: 15  loss: 0.687322\n",
      "iteration: 16  loss: 0.686522\n",
      "iteration: 17  loss: 0.685456\n",
      "iteration: 18  loss: 0.684015\n",
      "iteration: 19  loss: 0.682704\n",
      "Ranking time: 91.01\n",
      "Obtaining metrics time: 8.92\n",
      "new results created\n",
      "\n",
      " ============================================================ \n",
      " Run: 2 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 0.01\n",
      "NOLF:\t\t\t 10\n",
      "Sample Percentage:\t 0.1\n",
      "Creating 20 samples of length 103544\n",
      "iteration: 0  loss: 0.695261\n",
      "iteration: 1  loss: 0.694499\n",
      "iteration: 2  loss: 0.69383\n",
      "iteration: 3  loss: 0.692976\n",
      "iteration: 4  loss: 0.692147\n",
      "iteration: 5  loss: 0.690981\n",
      "iteration: 6  loss: 0.689187\n",
      "iteration: 7  loss: 0.687601\n",
      "iteration: 8  loss: 0.685791\n",
      "iteration: 9  loss: 0.683046\n",
      "iteration: 10  loss: 0.681011\n",
      "iteration: 11  loss: 0.678616\n",
      "iteration: 12  loss: 0.675923\n",
      "iteration: 13  loss: 0.67247\n",
      "iteration: 14  loss: 0.669998\n",
      "iteration: 15  loss: 0.667065\n",
      "iteration: 16  loss: 0.664458\n",
      "iteration: 17  loss: 0.661483\n",
      "iteration: 18  loss: 0.658447\n",
      "iteration: 19  loss: 0.655458\n",
      "Ranking time: 91.43\n",
      "Obtaining metrics time: 8.74\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 3 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 0.01\n",
      "NOLF:\t\t\t 10\n",
      "Sample Percentage:\t 0.2\n",
      "Creating 20 samples of length 207089\n",
      "iteration: 0  loss: 0.694813\n",
      "iteration: 1  loss: 0.693397\n",
      "iteration: 2  loss: 0.691551\n",
      "iteration: 3  loss: 0.688409\n",
      "iteration: 4  loss: 0.684151\n",
      "iteration: 5  loss: 0.679234\n",
      "iteration: 6  loss: 0.673496\n",
      "iteration: 7  loss: 0.667526\n",
      "iteration: 8  loss: 0.661605\n",
      "iteration: 9  loss: 0.655241\n",
      "iteration: 10  loss: 0.649168\n",
      "iteration: 11  loss: 0.643438\n",
      "iteration: 12  loss: 0.637173\n",
      "iteration: 13  loss: 0.630755\n",
      "iteration: 14  loss: 0.624842\n",
      "iteration: 15  loss: 0.618425\n",
      "iteration: 16  loss: 0.612606\n",
      "iteration: 17  loss: 0.60628\n",
      "iteration: 18  loss: 0.599791\n",
      "iteration: 19  loss: 0.594391\n",
      "Ranking time: 101.73\n",
      "Obtaining metrics time: 9.51\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 4 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 0.01\n",
      "NOLF:\t\t\t 10\n",
      "Sample Percentage:\t 0.5\n",
      "Creating 20 samples of length 517722\n",
      "iteration: 0  loss: 0.693693\n",
      "iteration: 1  loss: 0.687173\n",
      "iteration: 2  loss: 0.674644\n",
      "iteration: 3  loss: 0.659227\n",
      "iteration: 4  loss: 0.643232\n",
      "iteration: 5  loss: 0.627307\n",
      "iteration: 6  loss: 0.61113\n",
      "iteration: 7  loss: 0.594746\n",
      "iteration: 8  loss: 0.577677\n",
      "iteration: 9  loss: 0.560806\n",
      "iteration: 10  loss: 0.543143\n",
      "iteration: 11  loss: 0.525502\n",
      "iteration: 12  loss: 0.508367\n",
      "iteration: 13  loss: 0.491104\n",
      "iteration: 14  loss: 0.474416\n",
      "iteration: 15  loss: 0.457817\n",
      "iteration: 16  loss: 0.442159\n",
      "iteration: 17  loss: 0.427419\n",
      "iteration: 18  loss: 0.412414\n",
      "iteration: 19  loss: 0.39898\n",
      "Ranking time: 92.33\n",
      "Obtaining metrics time: 8.72\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 5 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 0.01\n",
      "NOLF:\t\t\t 20\n",
      "Sample Percentage:\t 0.05\n",
      "Creating 20 samples of length 51772\n",
      "iteration: 0  loss: 0.697188\n",
      "iteration: 1  loss: 0.696819\n",
      "iteration: 2  loss: 0.695953\n",
      "iteration: 3  loss: 0.695237\n",
      "iteration: 4  loss: 0.694671\n",
      "iteration: 5  loss: 0.694028\n",
      "iteration: 6  loss: 0.693372\n",
      "iteration: 7  loss: 0.692315\n",
      "iteration: 8  loss: 0.691509\n",
      "iteration: 9  loss: 0.690238\n",
      "iteration: 10  loss: 0.688975\n",
      "iteration: 11  loss: 0.687991\n",
      "iteration: 12  loss: 0.687301\n",
      "iteration: 13  loss: 0.685515\n",
      "iteration: 14  loss: 0.684158\n",
      "iteration: 15  loss: 0.68257\n",
      "iteration: 16  loss: 0.68127\n",
      "iteration: 17  loss: 0.679761\n",
      "iteration: 18  loss: 0.678197\n",
      "iteration: 19  loss: 0.676916\n",
      "Ranking time: 104.03\n",
      "Obtaining metrics time: 8.6\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 6 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 0.01\n",
      "NOLF:\t\t\t 20\n",
      "Sample Percentage:\t 0.1\n",
      "Creating 20 samples of length 103544\n",
      "iteration: 0  loss: 0.696966\n",
      "iteration: 1  loss: 0.695734\n",
      "iteration: 2  loss: 0.694419\n",
      "iteration: 3  loss: 0.692709\n",
      "iteration: 4  loss: 0.690804\n",
      "iteration: 5  loss: 0.688252\n",
      "iteration: 6  loss: 0.685802\n",
      "iteration: 7  loss: 0.683098\n",
      "iteration: 8  loss: 0.679742\n",
      "iteration: 9  loss: 0.676835\n",
      "iteration: 10  loss: 0.673152\n",
      "iteration: 11  loss: 0.669757\n",
      "iteration: 12  loss: 0.665831\n",
      "iteration: 13  loss: 0.662369\n",
      "iteration: 14  loss: 0.657905\n",
      "iteration: 15  loss: 0.654469\n",
      "iteration: 16  loss: 0.650335\n",
      "iteration: 17  loss: 0.646798\n",
      "iteration: 18  loss: 0.64298\n",
      "iteration: 19  loss: 0.638789\n",
      "Ranking time: 103.62\n",
      "Obtaining metrics time: 8.64\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 7 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 0.01\n",
      "NOLF:\t\t\t 20\n",
      "Sample Percentage:\t 0.2\n",
      "Creating 20 samples of length 207089\n",
      "iteration: 0  loss: 0.696388\n",
      "iteration: 1  loss: 0.693498\n",
      "iteration: 2  loss: 0.689384\n",
      "iteration: 3  loss: 0.684425\n",
      "iteration: 4  loss: 0.677961\n",
      "iteration: 5  loss: 0.670844\n",
      "iteration: 6  loss: 0.663259\n",
      "iteration: 7  loss: 0.655335\n",
      "iteration: 8  loss: 0.646944\n",
      "iteration: 9  loss: 0.639157\n",
      "iteration: 10  loss: 0.631648\n",
      "iteration: 11  loss: 0.623956\n",
      "iteration: 12  loss: 0.616379\n",
      "iteration: 13  loss: 0.607868\n",
      "iteration: 14  loss: 0.600086\n",
      "iteration: 15  loss: 0.591967\n",
      "iteration: 16  loss: 0.584147\n",
      "iteration: 17  loss: 0.574891\n",
      "iteration: 18  loss: 0.568973\n",
      "iteration: 19  loss: 0.559548\n",
      "Ranking time: 103.41\n",
      "Obtaining metrics time: 8.65\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 8 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 0.01\n",
      "NOLF:\t\t\t 20\n",
      "Sample Percentage:\t 0.5\n",
      "Creating 20 samples of length 517722\n",
      "iteration: 0  loss: 0.6941\n",
      "iteration: 1  loss: 0.682463\n",
      "iteration: 2  loss: 0.664935\n",
      "iteration: 3  loss: 0.644244\n",
      "iteration: 4  loss: 0.624232\n",
      "iteration: 5  loss: 0.603451\n",
      "iteration: 6  loss: 0.58256\n",
      "iteration: 7  loss: 0.561626\n",
      "iteration: 8  loss: 0.540371\n",
      "iteration: 9  loss: 0.519336\n",
      "iteration: 10  loss: 0.498257\n",
      "iteration: 11  loss: 0.47854\n",
      "iteration: 12  loss: 0.458884\n",
      "iteration: 13  loss: 0.440426\n",
      "iteration: 14  loss: 0.422556\n",
      "iteration: 15  loss: 0.404626\n",
      "iteration: 16  loss: 0.38842\n",
      "iteration: 17  loss: 0.374163\n",
      "iteration: 18  loss: 0.359219\n",
      "iteration: 19  loss: 0.345965\n",
      "Ranking time: 104.71\n",
      "Obtaining metrics time: 8.81\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 9 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 0.01\n",
      "NOLF:\t\t\t 50\n",
      "Sample Percentage:\t 0.05\n",
      "Creating 20 samples of length 51772\n",
      "iteration: 0  loss: 0.704135\n",
      "iteration: 1  loss: 0.702956\n",
      "iteration: 2  loss: 0.700825\n",
      "iteration: 3  loss: 0.698567\n",
      "iteration: 4  loss: 0.697083\n",
      "iteration: 5  loss: 0.69532\n",
      "iteration: 6  loss: 0.693239\n",
      "iteration: 7  loss: 0.691581\n",
      "iteration: 8  loss: 0.689175\n",
      "iteration: 9  loss: 0.686864\n",
      "iteration: 10  loss: 0.685077\n",
      "iteration: 11  loss: 0.68268\n",
      "iteration: 12  loss: 0.680086\n",
      "iteration: 13  loss: 0.677317\n",
      "iteration: 14  loss: 0.67443\n",
      "iteration: 15  loss: 0.672439\n",
      "iteration: 16  loss: 0.669869\n",
      "iteration: 17  loss: 0.666917\n",
      "iteration: 18  loss: 0.664507\n",
      "iteration: 19  loss: 0.661234\n",
      "Ranking time: 146.35\n",
      "Obtaining metrics time: 8.79\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 10 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 0.01\n",
      "NOLF:\t\t\t 50\n",
      "Sample Percentage:\t 0.1\n",
      "Creating 20 samples of length 103544\n",
      "iteration: 0  loss: 0.703513\n",
      "iteration: 1  loss: 0.699665\n",
      "iteration: 2  loss: 0.696227\n",
      "iteration: 3  loss: 0.692114\n",
      "iteration: 4  loss: 0.687592\n",
      "iteration: 5  loss: 0.683557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 6  loss: 0.67813\n",
      "iteration: 7  loss: 0.672494\n",
      "iteration: 8  loss: 0.667555\n",
      "iteration: 9  loss: 0.661634\n",
      "iteration: 10  loss: 0.656403\n",
      "iteration: 11  loss: 0.650559\n",
      "iteration: 12  loss: 0.644182\n",
      "iteration: 13  loss: 0.638669\n",
      "iteration: 14  loss: 0.632578\n",
      "iteration: 15  loss: 0.627145\n",
      "iteration: 16  loss: 0.620751\n",
      "iteration: 17  loss: 0.615524\n",
      "iteration: 18  loss: 0.610095\n",
      "iteration: 19  loss: 0.604267\n",
      "Ranking time: 146.52\n",
      "Obtaining metrics time: 8.68\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 11 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 0.01\n",
      "NOLF:\t\t\t 50\n",
      "Sample Percentage:\t 0.2\n",
      "Creating 20 samples of length 207089\n",
      "iteration: 0  loss: 0.701775\n",
      "iteration: 1  loss: 0.694163\n",
      "iteration: 2  loss: 0.685464\n",
      "iteration: 3  loss: 0.674928\n",
      "iteration: 4  loss: 0.663847\n",
      "iteration: 5  loss: 0.6525\n",
      "iteration: 6  loss: 0.639964\n",
      "iteration: 7  loss: 0.628344\n",
      "iteration: 8  loss: 0.615846\n",
      "iteration: 9  loss: 0.604692\n",
      "iteration: 10  loss: 0.59324\n",
      "iteration: 11  loss: 0.581838\n",
      "iteration: 12  loss: 0.570465\n",
      "iteration: 13  loss: 0.560478\n",
      "iteration: 14  loss: 0.54988\n",
      "iteration: 15  loss: 0.539486\n",
      "iteration: 16  loss: 0.527651\n",
      "iteration: 17  loss: 0.518211\n",
      "iteration: 18  loss: 0.50861\n",
      "iteration: 19  loss: 0.498429\n",
      "Ranking time: 146.75\n",
      "Obtaining metrics time: 8.68\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 12 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 0.01\n",
      "NOLF:\t\t\t 50\n",
      "Sample Percentage:\t 0.5\n",
      "Creating 20 samples of length 517722\n",
      "iteration: 0  loss: 0.695793\n",
      "iteration: 1  loss: 0.672058\n",
      "iteration: 2  loss: 0.642485\n",
      "iteration: 3  loss: 0.612132\n",
      "iteration: 4  loss: 0.582122\n",
      "iteration: 5  loss: 0.554247\n",
      "iteration: 6  loss: 0.526623\n",
      "iteration: 7  loss: 0.500389\n",
      "iteration: 8  loss: 0.475087\n",
      "iteration: 9  loss: 0.451148\n",
      "iteration: 10  loss: 0.428288\n",
      "iteration: 11  loss: 0.407287\n",
      "iteration: 12  loss: 0.387736\n",
      "iteration: 13  loss: 0.369154\n",
      "iteration: 14  loss: 0.352426\n",
      "iteration: 15  loss: 0.337144\n",
      "iteration: 16  loss: 0.32305\n",
      "iteration: 17  loss: 0.310658\n",
      "iteration: 18  loss: 0.298785\n",
      "iteration: 19  loss: 0.288403\n",
      "Ranking time: 146.67\n",
      "Obtaining metrics time: 8.8\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 13 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 0.01\n",
      "NOLF:\t\t\t 100\n",
      "Sample Percentage:\t 0.05\n",
      "Creating 20 samples of length 51772\n",
      "iteration: 0  loss: 0.715155\n",
      "iteration: 1  loss: 0.711118\n",
      "iteration: 2  loss: 0.70813\n",
      "iteration: 3  loss: 0.704872\n",
      "iteration: 4  loss: 0.700831\n",
      "iteration: 5  loss: 0.69773\n",
      "iteration: 6  loss: 0.694977\n",
      "iteration: 7  loss: 0.691081\n",
      "iteration: 8  loss: 0.685502\n",
      "iteration: 9  loss: 0.683025\n",
      "iteration: 10  loss: 0.678854\n",
      "iteration: 11  loss: 0.675954\n",
      "iteration: 12  loss: 0.670804\n",
      "iteration: 13  loss: 0.666492\n",
      "iteration: 14  loss: 0.663575\n",
      "iteration: 15  loss: 0.658464\n",
      "iteration: 16  loss: 0.653855\n",
      "iteration: 17  loss: 0.651617\n",
      "iteration: 18  loss: 0.646144\n",
      "iteration: 19  loss: 0.641907\n",
      "Ranking time: 224.33\n",
      "Obtaining metrics time: 8.64\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 14 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 0.01\n",
      "NOLF:\t\t\t 100\n",
      "Sample Percentage:\t 0.1\n",
      "Creating 20 samples of length 103544\n",
      "iteration: 0  loss: 0.713092\n",
      "iteration: 1  loss: 0.706303\n",
      "iteration: 2  loss: 0.699331\n",
      "iteration: 3  loss: 0.692686\n",
      "iteration: 4  loss: 0.684168\n",
      "iteration: 5  loss: 0.676401\n",
      "iteration: 6  loss: 0.667058\n",
      "iteration: 7  loss: 0.659406\n",
      "iteration: 8  loss: 0.651098\n",
      "iteration: 9  loss: 0.642626\n",
      "iteration: 10  loss: 0.634349\n",
      "iteration: 11  loss: 0.626325\n",
      "iteration: 12  loss: 0.617486\n",
      "iteration: 13  loss: 0.609148\n",
      "iteration: 14  loss: 0.602023\n",
      "iteration: 15  loss: 0.593419\n",
      "iteration: 16  loss: 0.586792\n",
      "iteration: 17  loss: 0.578757\n",
      "iteration: 18  loss: 0.571168\n",
      "iteration: 19  loss: 0.56457\n",
      "Ranking time: 224.74\n",
      "Obtaining metrics time: 8.74\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 15 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 0.01\n",
      "NOLF:\t\t\t 100\n",
      "Sample Percentage:\t 0.2\n",
      "Creating 20 samples of length 207089\n",
      "iteration: 0  loss: 0.709961\n",
      "iteration: 1  loss: 0.695964\n",
      "iteration: 2  loss: 0.680111\n",
      "iteration: 3  loss: 0.663198\n",
      "iteration: 4  loss: 0.645872\n",
      "iteration: 5  loss: 0.629052\n",
      "iteration: 6  loss: 0.611932\n",
      "iteration: 7  loss: 0.595833\n",
      "iteration: 8  loss: 0.580327\n",
      "iteration: 9  loss: 0.564972\n",
      "iteration: 10  loss: 0.550503\n",
      "iteration: 11  loss: 0.537217\n",
      "iteration: 12  loss: 0.522864\n",
      "iteration: 13  loss: 0.509933\n",
      "iteration: 14  loss: 0.497164\n",
      "iteration: 15  loss: 0.485039\n",
      "iteration: 16  loss: 0.472579\n",
      "iteration: 17  loss: 0.461231\n",
      "iteration: 18  loss: 0.45097\n",
      "iteration: 19  loss: 0.440397\n",
      "Ranking time: 223.83\n",
      "Obtaining metrics time: 8.65\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 16 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 0.01\n",
      "NOLF:\t\t\t 100\n",
      "Sample Percentage:\t 0.5\n",
      "Creating 20 samples of length 517722\n",
      "iteration: 0  loss: 0.69912\n",
      "iteration: 1  loss: 0.658694\n",
      "iteration: 2  loss: 0.615497\n",
      "iteration: 3  loss: 0.574544\n",
      "iteration: 4  loss: 0.537503\n",
      "iteration: 5  loss: 0.503457\n",
      "iteration: 6  loss: 0.471456\n",
      "iteration: 7  loss: 0.442465\n",
      "iteration: 8  loss: 0.41683\n",
      "iteration: 9  loss: 0.393132\n",
      "iteration: 10  loss: 0.371616\n",
      "iteration: 11  loss: 0.352775\n",
      "iteration: 12  loss: 0.335624\n",
      "iteration: 13  loss: 0.320242\n",
      "iteration: 14  loss: 0.306489\n",
      "iteration: 15  loss: 0.294337\n",
      "iteration: 16  loss: 0.28336\n",
      "iteration: 17  loss: 0.273282\n",
      "iteration: 18  loss: 0.264517\n",
      "iteration: 19  loss: 0.25669\n",
      "Ranking time: 224.55\n",
      "Obtaining metrics time: 10.82\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 17 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 0.1\n",
      "NOLF:\t\t\t 10\n",
      "Sample Percentage:\t 0.05\n",
      "Creating 20 samples of length 51772\n",
      "iteration: 0  loss: 0.714032\n",
      "iteration: 1  loss: 0.713471\n",
      "iteration: 2  loss: 0.712823\n",
      "iteration: 3  loss: 0.712026\n",
      "iteration: 4  loss: 0.711752\n",
      "iteration: 5  loss: 0.711206\n",
      "iteration: 6  loss: 0.710685\n",
      "iteration: 7  loss: 0.71033\n",
      "iteration: 8  loss: 0.710037\n",
      "iteration: 9  loss: 0.709414\n",
      "iteration: 10  loss: 0.709118\n",
      "iteration: 11  loss: 0.708765\n",
      "iteration: 12  loss: 0.708377\n",
      "iteration: 13  loss: 0.708088\n",
      "iteration: 14  loss: 0.707688\n",
      "iteration: 15  loss: 0.707628\n",
      "iteration: 16  loss: 0.707489\n",
      "iteration: 17  loss: 0.707069\n",
      "iteration: 18  loss: 0.706646\n",
      "iteration: 19  loss: 0.706383\n",
      "Ranking time: 91.65\n",
      "Obtaining metrics time: 8.7\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 18 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 0.1\n",
      "NOLF:\t\t\t 10\n",
      "Sample Percentage:\t 0.1\n",
      "Creating 20 samples of length 103544\n",
      "iteration: 0  loss: 0.713794\n",
      "iteration: 1  loss: 0.712386\n",
      "iteration: 2  loss: 0.711484\n",
      "iteration: 3  loss: 0.71048\n",
      "iteration: 4  loss: 0.709778\n",
      "iteration: 5  loss: 0.708996\n",
      "iteration: 6  loss: 0.70821\n",
      "iteration: 7  loss: 0.707572\n",
      "iteration: 8  loss: 0.707202\n",
      "iteration: 9  loss: 0.706464\n",
      "iteration: 10  loss: 0.705818\n",
      "iteration: 11  loss: 0.705365\n",
      "iteration: 12  loss: 0.704807\n",
      "iteration: 13  loss: 0.704386\n",
      "iteration: 14  loss: 0.703794\n",
      "iteration: 15  loss: 0.703402\n",
      "iteration: 16  loss: 0.703133\n",
      "iteration: 17  loss: 0.702602\n",
      "iteration: 18  loss: 0.702147\n",
      "iteration: 19  loss: 0.701834\n",
      "Ranking time: 92.23\n",
      "Obtaining metrics time: 8.94\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 19 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 0.1\n",
      "NOLF:\t\t\t 10\n",
      "Sample Percentage:\t 0.2\n",
      "Creating 20 samples of length 207089\n",
      "iteration: 0  loss: 0.713114\n",
      "iteration: 1  loss: 0.711011\n",
      "iteration: 2  loss: 0.709328\n",
      "iteration: 3  loss: 0.707904\n",
      "iteration: 4  loss: 0.706782\n",
      "iteration: 5  loss: 0.705556\n",
      "iteration: 6  loss: 0.704581\n",
      "iteration: 7  loss: 0.703556\n",
      "iteration: 8  loss: 0.702774\n",
      "iteration: 9  loss: 0.701807\n",
      "iteration: 10  loss: 0.700976\n",
      "iteration: 11  loss: 0.700162\n",
      "iteration: 12  loss: 0.699179\n",
      "iteration: 13  loss: 0.698071\n",
      "iteration: 14  loss: 0.696973\n",
      "iteration: 15  loss: 0.695987\n",
      "iteration: 16  loss: 0.694837\n",
      "iteration: 17  loss: 0.693743\n",
      "iteration: 18  loss: 0.692436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 19  loss: 0.691695\n",
      "Ranking time: 93.46\n",
      "Obtaining metrics time: 8.84\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 20 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 0.1\n",
      "NOLF:\t\t\t 10\n",
      "Sample Percentage:\t 0.5\n",
      "Creating 20 samples of length 517722\n",
      "iteration: 0  loss: 0.711609\n",
      "iteration: 1  loss: 0.707662\n",
      "iteration: 2  loss: 0.704755\n",
      "iteration: 3  loss: 0.702476\n",
      "iteration: 4  loss: 0.700243\n",
      "iteration: 5  loss: 0.697556\n",
      "iteration: 6  loss: 0.694778\n",
      "iteration: 7  loss: 0.691986\n",
      "iteration: 8  loss: 0.68919\n",
      "iteration: 9  loss: 0.68674\n",
      "iteration: 10  loss: 0.684324\n",
      "iteration: 11  loss: 0.682119\n",
      "iteration: 12  loss: 0.680069\n",
      "iteration: 13  loss: 0.677965\n",
      "iteration: 14  loss: 0.67611\n",
      "iteration: 15  loss: 0.674177\n",
      "iteration: 16  loss: 0.672439\n",
      "iteration: 17  loss: 0.6708\n",
      "iteration: 18  loss: 0.669107\n",
      "iteration: 19  loss: 0.667604\n",
      "Ranking time: 93.12\n",
      "Obtaining metrics time: 8.98\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 21 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 0.1\n",
      "NOLF:\t\t\t 20\n",
      "Sample Percentage:\t 0.05\n",
      "Creating 20 samples of length 51772\n",
      "iteration: 0  loss: 0.734759\n",
      "iteration: 1  loss: 0.733517\n",
      "iteration: 2  loss: 0.732133\n",
      "iteration: 3  loss: 0.731285\n",
      "iteration: 4  loss: 0.730391\n",
      "iteration: 5  loss: 0.729425\n",
      "iteration: 6  loss: 0.728458\n",
      "iteration: 7  loss: 0.727451\n",
      "iteration: 8  loss: 0.726625\n",
      "iteration: 9  loss: 0.725975\n",
      "iteration: 10  loss: 0.724778\n",
      "iteration: 11  loss: 0.724374\n",
      "iteration: 12  loss: 0.723771\n",
      "iteration: 13  loss: 0.723114\n",
      "iteration: 14  loss: 0.722599\n",
      "iteration: 15  loss: 0.721792\n",
      "iteration: 16  loss: 0.721311\n",
      "iteration: 17  loss: 0.720694\n",
      "iteration: 18  loss: 0.720239\n",
      "iteration: 19  loss: 0.719469\n",
      "Ranking time: 104.08\n",
      "Obtaining metrics time: 8.89\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 22 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 0.1\n",
      "NOLF:\t\t\t 20\n",
      "Sample Percentage:\t 0.1\n",
      "Creating 20 samples of length 103544\n",
      "iteration: 0  loss: 0.734101\n",
      "iteration: 1  loss: 0.731669\n",
      "iteration: 2  loss: 0.72979\n",
      "iteration: 3  loss: 0.727975\n",
      "iteration: 4  loss: 0.726314\n",
      "iteration: 5  loss: 0.724506\n",
      "iteration: 6  loss: 0.723343\n",
      "iteration: 7  loss: 0.722063\n",
      "iteration: 8  loss: 0.720938\n",
      "iteration: 9  loss: 0.719735\n",
      "iteration: 10  loss: 0.71883\n",
      "iteration: 11  loss: 0.717651\n",
      "iteration: 12  loss: 0.716298\n",
      "iteration: 13  loss: 0.715534\n",
      "iteration: 14  loss: 0.714629\n",
      "iteration: 15  loss: 0.713867\n",
      "iteration: 16  loss: 0.713021\n",
      "iteration: 17  loss: 0.712196\n",
      "iteration: 18  loss: 0.711351\n",
      "iteration: 19  loss: 0.710661\n",
      "Ranking time: 104.7\n",
      "Obtaining metrics time: 8.53\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 23 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 0.1\n",
      "NOLF:\t\t\t 20\n",
      "Sample Percentage:\t 0.2\n",
      "Creating 20 samples of length 207089\n",
      "iteration: 0  loss: 0.732877\n",
      "iteration: 1  loss: 0.728816\n",
      "iteration: 2  loss: 0.725389\n",
      "iteration: 3  loss: 0.722655\n",
      "iteration: 4  loss: 0.720254\n",
      "iteration: 5  loss: 0.717993\n",
      "iteration: 6  loss: 0.715932\n",
      "iteration: 7  loss: 0.714079\n",
      "iteration: 8  loss: 0.712442\n",
      "iteration: 9  loss: 0.710827\n",
      "iteration: 10  loss: 0.709096\n",
      "iteration: 11  loss: 0.707348\n",
      "iteration: 12  loss: 0.705714\n",
      "iteration: 13  loss: 0.703869\n",
      "iteration: 14  loss: 0.702304\n",
      "iteration: 15  loss: 0.700645\n",
      "iteration: 16  loss: 0.69912\n",
      "iteration: 17  loss: 0.697253\n",
      "iteration: 18  loss: 0.69619\n",
      "iteration: 19  loss: 0.694353\n",
      "Ranking time: 104.39\n",
      "Obtaining metrics time: 8.86\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 24 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 0.1\n",
      "NOLF:\t\t\t 20\n",
      "Sample Percentage:\t 0.5\n",
      "Creating 20 samples of length 517722\n",
      "iteration: 0  loss: 0.729993\n",
      "iteration: 1  loss: 0.722029\n",
      "iteration: 2  loss: 0.716479\n",
      "iteration: 3  loss: 0.711899\n",
      "iteration: 4  loss: 0.707548\n",
      "iteration: 5  loss: 0.703147\n",
      "iteration: 6  loss: 0.698905\n",
      "iteration: 7  loss: 0.694957\n",
      "iteration: 8  loss: 0.691235\n",
      "iteration: 9  loss: 0.687951\n",
      "iteration: 10  loss: 0.684914\n",
      "iteration: 11  loss: 0.682111\n",
      "iteration: 12  loss: 0.679696\n",
      "iteration: 13  loss: 0.677547\n",
      "iteration: 14  loss: 0.675382\n",
      "iteration: 15  loss: 0.673277\n",
      "iteration: 16  loss: 0.671338\n",
      "iteration: 17  loss: 0.669746\n",
      "iteration: 18  loss: 0.668168\n",
      "iteration: 19  loss: 0.666677\n",
      "Ranking time: 106.12\n",
      "Obtaining metrics time: 8.7\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 25 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 0.1\n",
      "NOLF:\t\t\t 50\n",
      "Sample Percentage:\t 0.05\n",
      "Creating 20 samples of length 51772\n",
      "iteration: 0  loss: 0.797695\n",
      "iteration: 1  loss: 0.794745\n",
      "iteration: 2  loss: 0.791095\n",
      "iteration: 3  loss: 0.788014\n",
      "iteration: 4  loss: 0.785542\n",
      "iteration: 5  loss: 0.783711\n",
      "iteration: 6  loss: 0.781076\n",
      "iteration: 7  loss: 0.779053\n",
      "iteration: 8  loss: 0.776856\n",
      "iteration: 9  loss: 0.774805\n",
      "iteration: 10  loss: 0.773749\n",
      "iteration: 11  loss: 0.77162\n",
      "iteration: 12  loss: 0.770115\n",
      "iteration: 13  loss: 0.768632\n",
      "iteration: 14  loss: 0.766947\n",
      "iteration: 15  loss: 0.765458\n",
      "iteration: 16  loss: 0.764263\n",
      "iteration: 17  loss: 0.762451\n",
      "iteration: 18  loss: 0.761233\n",
      "iteration: 19  loss: 0.759781\n",
      "Ranking time: 147.31\n",
      "Obtaining metrics time: 8.84\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 26 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 0.1\n",
      "NOLF:\t\t\t 50\n",
      "Sample Percentage:\t 0.1\n",
      "Creating 20 samples of length 103544\n",
      "iteration: 0  loss: 0.796141\n",
      "iteration: 1  loss: 0.789696\n",
      "iteration: 2  loss: 0.78477\n",
      "iteration: 3  loss: 0.779838\n",
      "iteration: 4  loss: 0.775931\n",
      "iteration: 5  loss: 0.772691\n",
      "iteration: 6  loss: 0.769105\n",
      "iteration: 7  loss: 0.765868\n",
      "iteration: 8  loss: 0.763018\n",
      "iteration: 9  loss: 0.760021\n",
      "iteration: 10  loss: 0.757475\n",
      "iteration: 11  loss: 0.754712\n",
      "iteration: 12  loss: 0.752257\n",
      "iteration: 13  loss: 0.750174\n",
      "iteration: 14  loss: 0.747845\n",
      "iteration: 15  loss: 0.74573\n",
      "iteration: 16  loss: 0.743737\n",
      "iteration: 17  loss: 0.741747\n",
      "iteration: 18  loss: 0.739896\n",
      "iteration: 19  loss: 0.738019\n",
      "Ranking time: 149.43\n",
      "Obtaining metrics time: 8.72\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 27 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 0.1\n",
      "NOLF:\t\t\t 50\n",
      "Sample Percentage:\t 0.2\n",
      "Creating 20 samples of length 207089\n",
      "iteration: 0  loss: 0.792788\n",
      "iteration: 1  loss: 0.782318\n",
      "iteration: 2  loss: 0.774118\n",
      "iteration: 3  loss: 0.767242\n",
      "iteration: 4  loss: 0.761347\n",
      "iteration: 5  loss: 0.755807\n",
      "iteration: 6  loss: 0.75113\n",
      "iteration: 7  loss: 0.746535\n",
      "iteration: 8  loss: 0.742355\n",
      "iteration: 9  loss: 0.738244\n",
      "iteration: 10  loss: 0.734576\n",
      "iteration: 11  loss: 0.730843\n",
      "iteration: 12  loss: 0.727559\n",
      "iteration: 13  loss: 0.724288\n",
      "iteration: 14  loss: 0.721298\n",
      "iteration: 15  loss: 0.718491\n",
      "iteration: 16  loss: 0.715527\n",
      "iteration: 17  loss: 0.713075\n",
      "iteration: 18  loss: 0.710731\n",
      "iteration: 19  loss: 0.708529\n",
      "Ranking time: 147.6\n",
      "Obtaining metrics time: 8.7\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 28 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 0.1\n",
      "NOLF:\t\t\t 50\n",
      "Sample Percentage:\t 0.5\n",
      "Creating 20 samples of length 517722\n",
      "iteration: 0  loss: 0.785204\n",
      "iteration: 1  loss: 0.76581\n",
      "iteration: 2  loss: 0.752195\n",
      "iteration: 3  loss: 0.740974\n",
      "iteration: 4  loss: 0.731204\n",
      "iteration: 5  loss: 0.722933\n",
      "iteration: 6  loss: 0.715358\n",
      "iteration: 7  loss: 0.709283\n",
      "iteration: 8  loss: 0.703512\n",
      "iteration: 9  loss: 0.698911\n",
      "iteration: 10  loss: 0.694574\n",
      "iteration: 11  loss: 0.690835\n",
      "iteration: 12  loss: 0.687611\n",
      "iteration: 13  loss: 0.684939\n",
      "iteration: 14  loss: 0.682526\n",
      "iteration: 15  loss: 0.68036\n",
      "iteration: 16  loss: 0.678576\n",
      "iteration: 17  loss: 0.676966\n",
      "iteration: 18  loss: 0.675598\n",
      "iteration: 19  loss: 0.674502\n",
      "Ranking time: 148.19\n",
      "Obtaining metrics time: 9.06\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 29 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 0.1\n",
      "NOLF:\t\t\t 100\n",
      "Sample Percentage:\t 0.05\n",
      "Creating 20 samples of length 51772\n",
      "iteration: 0  loss: 0.901893\n",
      "iteration: 1  loss: 0.894895\n",
      "iteration: 2  loss: 0.889103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 3  loss: 0.883579\n",
      "iteration: 4  loss: 0.878171\n",
      "iteration: 5  loss: 0.874177\n",
      "iteration: 6  loss: 0.870224\n",
      "iteration: 7  loss: 0.865948\n",
      "iteration: 8  loss: 0.86096\n",
      "iteration: 9  loss: 0.857691\n",
      "iteration: 10  loss: 0.853628\n",
      "iteration: 11  loss: 0.85103\n",
      "iteration: 12  loss: 0.847012\n",
      "iteration: 13  loss: 0.843491\n",
      "iteration: 14  loss: 0.841651\n",
      "iteration: 15  loss: 0.838143\n",
      "iteration: 16  loss: 0.83507\n",
      "iteration: 17  loss: 0.83374\n",
      "iteration: 18  loss: 0.830006\n",
      "iteration: 19  loss: 0.827901\n",
      "Ranking time: 226.18\n",
      "Obtaining metrics time: 8.66\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 30 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 0.1\n",
      "NOLF:\t\t\t 100\n",
      "Sample Percentage:\t 0.1\n",
      "Creating 20 samples of length 103544\n",
      "iteration: 0  loss: 0.898381\n",
      "iteration: 1  loss: 0.886658\n",
      "iteration: 2  loss: 0.876237\n",
      "iteration: 3  loss: 0.867651\n",
      "iteration: 4  loss: 0.859335\n",
      "iteration: 5  loss: 0.852133\n",
      "iteration: 6  loss: 0.84502\n",
      "iteration: 7  loss: 0.839265\n",
      "iteration: 8  loss: 0.833603\n",
      "iteration: 9  loss: 0.827816\n",
      "iteration: 10  loss: 0.82318\n",
      "iteration: 11  loss: 0.818099\n",
      "iteration: 12  loss: 0.813383\n",
      "iteration: 13  loss: 0.808922\n",
      "iteration: 14  loss: 0.804943\n",
      "iteration: 15  loss: 0.801027\n",
      "iteration: 16  loss: 0.797675\n",
      "iteration: 17  loss: 0.793588\n",
      "iteration: 18  loss: 0.790082\n",
      "iteration: 19  loss: 0.786596\n",
      "Ranking time: 226.3\n",
      "Obtaining metrics time: 8.9\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 31 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 0.1\n",
      "NOLF:\t\t\t 100\n",
      "Sample Percentage:\t 0.2\n",
      "Creating 20 samples of length 207089\n",
      "iteration: 0  loss: 0.892461\n",
      "iteration: 1  loss: 0.87202\n",
      "iteration: 2  loss: 0.855662\n",
      "iteration: 3  loss: 0.841861\n",
      "iteration: 4  loss: 0.830324\n",
      "iteration: 5  loss: 0.820337\n",
      "iteration: 6  loss: 0.810696\n",
      "iteration: 7  loss: 0.802145\n",
      "iteration: 8  loss: 0.794669\n",
      "iteration: 9  loss: 0.787098\n",
      "iteration: 10  loss: 0.780612\n",
      "iteration: 11  loss: 0.774877\n",
      "iteration: 12  loss: 0.768699\n",
      "iteration: 13  loss: 0.763622\n",
      "iteration: 14  loss: 0.758411\n",
      "iteration: 15  loss: 0.753662\n",
      "iteration: 16  loss: 0.749557\n",
      "iteration: 17  loss: 0.745299\n",
      "iteration: 18  loss: 0.741996\n",
      "iteration: 19  loss: 0.738458\n",
      "Ranking time: 226.15\n",
      "Obtaining metrics time: 8.94\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 32 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 0.1\n",
      "NOLF:\t\t\t 100\n",
      "Sample Percentage:\t 0.5\n",
      "Creating 20 samples of length 517722\n",
      "iteration: 0  loss: 0.877635\n",
      "iteration: 1  loss: 0.839215\n",
      "iteration: 2  loss: 0.812995\n",
      "iteration: 3  loss: 0.792226\n",
      "iteration: 4  loss: 0.775514\n",
      "iteration: 5  loss: 0.761178\n",
      "iteration: 6  loss: 0.749404\n",
      "iteration: 7  loss: 0.739686\n",
      "iteration: 8  loss: 0.731825\n",
      "iteration: 9  loss: 0.724524\n",
      "iteration: 10  loss: 0.719049\n",
      "iteration: 11  loss: 0.714421\n",
      "iteration: 12  loss: 0.709831\n",
      "iteration: 13  loss: 0.706103\n",
      "iteration: 14  loss: 0.703256\n",
      "iteration: 15  loss: 0.700796\n",
      "iteration: 16  loss: 0.698408\n",
      "iteration: 17  loss: 0.69643\n",
      "iteration: 18  loss: 0.694761\n",
      "iteration: 19  loss: 0.693182\n",
      "Ranking time: 226.34\n",
      "Obtaining metrics time: 9.02\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 33 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 0.5\n",
      "NOLF:\t\t\t 10\n",
      "Sample Percentage:\t 0.05\n",
      "Creating 20 samples of length 51772\n",
      "iteration: 0  loss: 0.793085\n",
      "iteration: 1  loss: 0.78526\n",
      "iteration: 2  loss: 0.779222\n",
      "iteration: 3  loss: 0.774441\n",
      "iteration: 4  loss: 0.770063\n",
      "iteration: 5  loss: 0.766301\n",
      "iteration: 6  loss: 0.762791\n",
      "iteration: 7  loss: 0.759643\n",
      "iteration: 8  loss: 0.756591\n",
      "iteration: 9  loss: 0.753733\n",
      "iteration: 10  loss: 0.751221\n",
      "iteration: 11  loss: 0.74898\n",
      "iteration: 12  loss: 0.746535\n",
      "iteration: 13  loss: 0.744608\n",
      "iteration: 14  loss: 0.742548\n",
      "iteration: 15  loss: 0.740826\n",
      "iteration: 16  loss: 0.73906\n",
      "iteration: 17  loss: 0.73718\n",
      "iteration: 18  loss: 0.735596\n",
      "iteration: 19  loss: 0.734052\n",
      "Ranking time: 92.61\n",
      "Obtaining metrics time: 8.84\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 34 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 0.5\n",
      "NOLF:\t\t\t 10\n",
      "Sample Percentage:\t 0.1\n",
      "Creating 20 samples of length 103544\n",
      "iteration: 0  loss: 0.789066\n",
      "iteration: 1  loss: 0.776665\n",
      "iteration: 2  loss: 0.768194\n",
      "iteration: 3  loss: 0.761088\n",
      "iteration: 4  loss: 0.754999\n",
      "iteration: 5  loss: 0.749832\n",
      "iteration: 6  loss: 0.745233\n",
      "iteration: 7  loss: 0.741157\n",
      "iteration: 8  loss: 0.737667\n",
      "iteration: 9  loss: 0.734201\n",
      "iteration: 10  loss: 0.731132\n",
      "iteration: 11  loss: 0.728569\n",
      "iteration: 12  loss: 0.726056\n",
      "iteration: 13  loss: 0.723848\n",
      "iteration: 14  loss: 0.721747\n",
      "iteration: 15  loss: 0.719814\n",
      "iteration: 16  loss: 0.718117\n",
      "iteration: 17  loss: 0.716541\n",
      "iteration: 18  loss: 0.715017\n",
      "iteration: 19  loss: 0.713677\n",
      "Ranking time: 92.91\n",
      "Obtaining metrics time: 8.64\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 35 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 0.5\n",
      "NOLF:\t\t\t 10\n",
      "Sample Percentage:\t 0.2\n",
      "Creating 20 samples of length 207089\n",
      "iteration: 0  loss: 0.78294\n",
      "iteration: 1  loss: 0.764562\n",
      "iteration: 2  loss: 0.752285\n",
      "iteration: 3  loss: 0.743003\n",
      "iteration: 4  loss: 0.735667\n",
      "iteration: 5  loss: 0.729474\n",
      "iteration: 6  loss: 0.72453\n",
      "iteration: 7  loss: 0.720263\n",
      "iteration: 8  loss: 0.716856\n",
      "iteration: 9  loss: 0.713835\n",
      "iteration: 10  loss: 0.711324\n",
      "iteration: 11  loss: 0.709128\n",
      "iteration: 12  loss: 0.707248\n",
      "iteration: 13  loss: 0.705623\n",
      "iteration: 14  loss: 0.704232\n",
      "iteration: 15  loss: 0.703008\n",
      "iteration: 16  loss: 0.701946\n",
      "iteration: 17  loss: 0.70102\n",
      "iteration: 18  loss: 0.700197\n",
      "iteration: 19  loss: 0.699475\n",
      "Ranking time: 93.31\n",
      "Obtaining metrics time: 8.83\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 36 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 0.5\n",
      "NOLF:\t\t\t 10\n",
      "Sample Percentage:\t 0.5\n",
      "Creating 20 samples of length 517722\n",
      "iteration: 0  loss: 0.769981\n",
      "iteration: 1  loss: 0.741309\n",
      "iteration: 2  loss: 0.725632\n",
      "iteration: 3  loss: 0.715846\n",
      "iteration: 4  loss: 0.709361\n",
      "iteration: 5  loss: 0.704937\n",
      "iteration: 6  loss: 0.701857\n",
      "iteration: 7  loss: 0.699668\n",
      "iteration: 8  loss: 0.698084\n",
      "iteration: 9  loss: 0.696938\n",
      "iteration: 10  loss: 0.696076\n",
      "iteration: 11  loss: 0.695436\n",
      "iteration: 12  loss: 0.69494\n",
      "iteration: 13  loss: 0.694565\n",
      "iteration: 14  loss: 0.694275\n",
      "iteration: 15  loss: 0.694052\n",
      "iteration: 16  loss: 0.693876\n",
      "iteration: 17  loss: 0.693736\n",
      "iteration: 18  loss: 0.693626\n",
      "iteration: 19  loss: 0.693537\n",
      "Ranking time: 93.6\n",
      "Obtaining metrics time: 8.9\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 37 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 0.5\n",
      "NOLF:\t\t\t 20\n",
      "Sample Percentage:\t 0.05\n",
      "Creating 20 samples of length 51772\n",
      "iteration: 0  loss: 0.892401\n",
      "iteration: 1  loss: 0.876889\n",
      "iteration: 2  loss: 0.865732\n",
      "iteration: 3  loss: 0.855611\n",
      "iteration: 4  loss: 0.847058\n",
      "iteration: 5  loss: 0.839538\n",
      "iteration: 6  loss: 0.83278\n",
      "iteration: 7  loss: 0.825903\n",
      "iteration: 8  loss: 0.820103\n",
      "iteration: 9  loss: 0.814858\n",
      "iteration: 10  loss: 0.809106\n",
      "iteration: 11  loss: 0.804743\n",
      "iteration: 12  loss: 0.800163\n",
      "iteration: 13  loss: 0.79591\n",
      "iteration: 14  loss: 0.79182\n",
      "iteration: 15  loss: 0.787782\n",
      "iteration: 16  loss: 0.784448\n",
      "iteration: 17  loss: 0.781425\n",
      "iteration: 18  loss: 0.778004\n",
      "iteration: 19  loss: 0.775151\n",
      "Ranking time: 105.45\n",
      "Obtaining metrics time: 9.0\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 38 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 0.5\n",
      "NOLF:\t\t\t 20\n",
      "Sample Percentage:\t 0.1\n",
      "Creating 20 samples of length 103544\n",
      "iteration: 0  loss: 0.8847\n",
      "iteration: 1  loss: 0.860548\n",
      "iteration: 2  loss: 0.842941\n",
      "iteration: 3  loss: 0.829124\n",
      "iteration: 4  loss: 0.816968\n",
      "iteration: 5  loss: 0.806306\n",
      "iteration: 6  loss: 0.797289\n",
      "iteration: 7  loss: 0.789027\n",
      "iteration: 8  loss: 0.781808\n",
      "iteration: 9  loss: 0.775326\n",
      "iteration: 10  loss: 0.76947\n",
      "iteration: 11  loss: 0.76383\n",
      "iteration: 12  loss: 0.758821\n",
      "iteration: 13  loss: 0.754445\n",
      "iteration: 14  loss: 0.750356\n",
      "iteration: 15  loss: 0.746516\n",
      "iteration: 16  loss: 0.743226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 17  loss: 0.740026\n",
      "iteration: 18  loss: 0.73698\n",
      "iteration: 19  loss: 0.734294\n",
      "Ranking time: 106.83\n",
      "Obtaining metrics time: 8.87\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 39 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 0.5\n",
      "NOLF:\t\t\t 20\n",
      "Sample Percentage:\t 0.2\n",
      "Creating 20 samples of length 207089\n",
      "iteration: 0  loss: 0.872518\n",
      "iteration: 1  loss: 0.835921\n",
      "iteration: 2  loss: 0.811418\n",
      "iteration: 3  loss: 0.792649\n",
      "iteration: 4  loss: 0.777851\n",
      "iteration: 5  loss: 0.765857\n",
      "iteration: 6  loss: 0.755722\n",
      "iteration: 7  loss: 0.747493\n",
      "iteration: 8  loss: 0.740539\n",
      "iteration: 9  loss: 0.734492\n",
      "iteration: 10  loss: 0.729431\n",
      "iteration: 11  loss: 0.725077\n",
      "iteration: 12  loss: 0.721368\n",
      "iteration: 13  loss: 0.718077\n",
      "iteration: 14  loss: 0.715255\n",
      "iteration: 15  loss: 0.712834\n",
      "iteration: 16  loss: 0.710726\n",
      "iteration: 17  loss: 0.708815\n",
      "iteration: 18  loss: 0.707213\n",
      "iteration: 19  loss: 0.705786\n",
      "Ranking time: 107.65\n",
      "Obtaining metrics time: 8.9\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 40 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 0.5\n",
      "NOLF:\t\t\t 20\n",
      "Sample Percentage:\t 0.5\n",
      "Creating 20 samples of length 517722\n",
      "iteration: 0  loss: 0.846701\n",
      "iteration: 1  loss: 0.789335\n",
      "iteration: 2  loss: 0.758168\n",
      "iteration: 3  loss: 0.738487\n",
      "iteration: 4  loss: 0.725539\n",
      "iteration: 5  loss: 0.716675\n",
      "iteration: 6  loss: 0.710548\n",
      "iteration: 7  loss: 0.706168\n",
      "iteration: 8  loss: 0.703004\n",
      "iteration: 9  loss: 0.700696\n",
      "iteration: 10  loss: 0.69898\n",
      "iteration: 11  loss: 0.697703\n",
      "iteration: 12  loss: 0.696721\n",
      "iteration: 13  loss: 0.695974\n",
      "iteration: 14  loss: 0.695402\n",
      "iteration: 15  loss: 0.69495\n",
      "iteration: 16  loss: 0.694595\n",
      "iteration: 17  loss: 0.694319\n",
      "iteration: 18  loss: 0.6941\n",
      "iteration: 19  loss: 0.693923\n",
      "Ranking time: 105.14\n",
      "Obtaining metrics time: 8.83\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 41 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 0.5\n",
      "NOLF:\t\t\t 50\n",
      "Sample Percentage:\t 0.05\n",
      "Creating 20 samples of length 51772\n",
      "iteration: 0  loss: 1.191675\n",
      "iteration: 1  loss: 1.153359\n",
      "iteration: 2  loss: 1.124154\n",
      "iteration: 3  loss: 1.098595\n",
      "iteration: 4  loss: 1.077519\n",
      "iteration: 5  loss: 1.058606\n",
      "iteration: 6  loss: 1.040786\n",
      "iteration: 7  loss: 1.024711\n",
      "iteration: 8  loss: 1.009427\n",
      "iteration: 9  loss: 0.99665\n",
      "iteration: 10  loss: 0.984762\n",
      "iteration: 11  loss: 0.971832\n",
      "iteration: 12  loss: 0.960704\n",
      "iteration: 13  loss: 0.949705\n",
      "iteration: 14  loss: 0.939478\n",
      "iteration: 15  loss: 0.931109\n",
      "iteration: 16  loss: 0.922192\n",
      "iteration: 17  loss: 0.913405\n",
      "iteration: 18  loss: 0.905758\n",
      "iteration: 19  loss: 0.897051\n",
      "Ranking time: 149.0\n",
      "Obtaining metrics time: 8.98\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 42 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 0.5\n",
      "NOLF:\t\t\t 50\n",
      "Sample Percentage:\t 0.1\n",
      "Creating 20 samples of length 103544\n",
      "iteration: 0  loss: 1.172614\n",
      "iteration: 1  loss: 1.11137\n",
      "iteration: 2  loss: 1.067433\n",
      "iteration: 3  loss: 1.031886\n",
      "iteration: 4  loss: 1.002042\n",
      "iteration: 5  loss: 0.97655\n",
      "iteration: 6  loss: 0.953464\n",
      "iteration: 7  loss: 0.932907\n",
      "iteration: 8  loss: 0.914958\n",
      "iteration: 9  loss: 0.8981\n",
      "iteration: 10  loss: 0.883224\n",
      "iteration: 11  loss: 0.869483\n",
      "iteration: 12  loss: 0.857508\n",
      "iteration: 13  loss: 0.845907\n",
      "iteration: 14  loss: 0.835613\n",
      "iteration: 15  loss: 0.826281\n",
      "iteration: 16  loss: 0.817652\n",
      "iteration: 17  loss: 0.809846\n",
      "iteration: 18  loss: 0.802494\n",
      "iteration: 19  loss: 0.795591\n",
      "Ranking time: 149.0\n",
      "Obtaining metrics time: 9.09\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 43 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 0.5\n",
      "NOLF:\t\t\t 50\n",
      "Sample Percentage:\t 0.2\n",
      "Creating 20 samples of length 207089\n",
      "iteration: 0  loss: 1.142137\n",
      "iteration: 1  loss: 1.049576\n",
      "iteration: 2  loss: 0.988616\n",
      "iteration: 3  loss: 0.94197\n",
      "iteration: 4  loss: 0.90536\n",
      "iteration: 5  loss: 0.874638\n",
      "iteration: 6  loss: 0.849637\n",
      "iteration: 7  loss: 0.828475\n",
      "iteration: 8  loss: 0.810957\n",
      "iteration: 9  loss: 0.796149\n",
      "iteration: 10  loss: 0.78339\n",
      "iteration: 11  loss: 0.772763\n",
      "iteration: 12  loss: 0.763253\n",
      "iteration: 13  loss: 0.755238\n",
      "iteration: 14  loss: 0.748255\n",
      "iteration: 15  loss: 0.74217\n",
      "iteration: 16  loss: 0.736833\n",
      "iteration: 17  loss: 0.732142\n",
      "iteration: 18  loss: 0.728107\n",
      "iteration: 19  loss: 0.724491\n",
      "Ranking time: 149.62\n",
      "Obtaining metrics time: 8.78\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 44 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 0.5\n",
      "NOLF:\t\t\t 50\n",
      "Sample Percentage:\t 0.5\n",
      "Creating 20 samples of length 517722\n",
      "iteration: 0  loss: 1.076871\n",
      "iteration: 1  loss: 0.933673\n",
      "iteration: 2  loss: 0.855262\n",
      "iteration: 3  loss: 0.806116\n",
      "iteration: 4  loss: 0.773774\n",
      "iteration: 5  loss: 0.751803\n",
      "iteration: 6  loss: 0.736385\n",
      "iteration: 7  loss: 0.725491\n",
      "iteration: 8  loss: 0.717633\n",
      "iteration: 9  loss: 0.711896\n",
      "iteration: 10  loss: 0.707631\n",
      "iteration: 11  loss: 0.70443\n",
      "iteration: 12  loss: 0.701984\n",
      "iteration: 13  loss: 0.700135\n",
      "iteration: 14  loss: 0.698701\n",
      "iteration: 15  loss: 0.697591\n",
      "iteration: 16  loss: 0.696722\n",
      "iteration: 17  loss: 0.696027\n",
      "iteration: 18  loss: 0.695491\n",
      "iteration: 19  loss: 0.695054\n",
      "Ranking time: 149.92\n",
      "Obtaining metrics time: 8.88\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 45 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 0.5\n",
      "NOLF:\t\t\t 100\n",
      "Sample Percentage:\t 0.05\n",
      "Creating 20 samples of length 51772\n",
      "iteration: 0  loss: 1.689642\n",
      "iteration: 1  loss: 1.612397\n",
      "iteration: 2  loss: 1.5538\n",
      "iteration: 3  loss: 1.504834\n",
      "iteration: 4  loss: 1.46308\n",
      "iteration: 5  loss: 1.425001\n",
      "iteration: 6  loss: 1.390775\n",
      "iteration: 7  loss: 1.35727\n",
      "iteration: 8  loss: 1.326872\n",
      "iteration: 9  loss: 1.300492\n",
      "iteration: 10  loss: 1.274019\n",
      "iteration: 11  loss: 1.249683\n",
      "iteration: 12  loss: 1.226941\n",
      "iteration: 13  loss: 1.205141\n",
      "iteration: 14  loss: 1.187216\n",
      "iteration: 15  loss: 1.166689\n",
      "iteration: 16  loss: 1.149916\n",
      "iteration: 17  loss: 1.132544\n",
      "iteration: 18  loss: 1.115661\n",
      "iteration: 19  loss: 1.101546\n",
      "Ranking time: 228.74\n",
      "Obtaining metrics time: 8.72\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 46 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 0.5\n",
      "NOLF:\t\t\t 100\n",
      "Sample Percentage:\t 0.1\n",
      "Creating 20 samples of length 103544\n",
      "iteration: 0  loss: 1.651337\n",
      "iteration: 1  loss: 1.529793\n",
      "iteration: 2  loss: 1.442377\n",
      "iteration: 3  loss: 1.372327\n",
      "iteration: 4  loss: 1.311542\n",
      "iteration: 5  loss: 1.258624\n",
      "iteration: 6  loss: 1.211834\n",
      "iteration: 7  loss: 1.172221\n",
      "iteration: 8  loss: 1.135687\n",
      "iteration: 9  loss: 1.102693\n",
      "iteration: 10  loss: 1.072835\n",
      "iteration: 11  loss: 1.046328\n",
      "iteration: 12  loss: 1.021127\n",
      "iteration: 13  loss: 0.998013\n",
      "iteration: 14  loss: 0.97818\n",
      "iteration: 15  loss: 0.958769\n",
      "iteration: 16  loss: 0.941692\n",
      "iteration: 17  loss: 0.926243\n",
      "iteration: 18  loss: 0.910862\n",
      "iteration: 19  loss: 0.897304\n",
      "Ranking time: 228.25\n",
      "Obtaining metrics time: 9.87\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 47 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 0.5\n",
      "NOLF:\t\t\t 100\n",
      "Sample Percentage:\t 0.2\n",
      "Creating 20 samples of length 207089\n",
      "iteration: 0  loss: 1.59042\n",
      "iteration: 1  loss: 1.407304\n",
      "iteration: 2  loss: 1.283228\n",
      "iteration: 3  loss: 1.189587\n",
      "iteration: 4  loss: 1.116461\n",
      "iteration: 5  loss: 1.055422\n",
      "iteration: 6  loss: 1.005217\n",
      "iteration: 7  loss: 0.963715\n",
      "iteration: 8  loss: 0.92901\n",
      "iteration: 9  loss: 0.898385\n",
      "iteration: 10  loss: 0.873212\n",
      "iteration: 11  loss: 0.851396\n",
      "iteration: 12  loss: 0.832946\n",
      "iteration: 13  loss: 0.816674\n",
      "iteration: 14  loss: 0.802634\n",
      "iteration: 15  loss: 0.790299\n",
      "iteration: 16  loss: 0.779808\n",
      "iteration: 17  loss: 0.770564\n",
      "iteration: 18  loss: 0.762449\n",
      "iteration: 19  loss: 0.755342\n",
      "Ranking time: 227.55\n",
      "Obtaining metrics time: 8.79\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 48 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 0.5\n",
      "NOLF:\t\t\t 100\n",
      "Sample Percentage:\t 0.5\n",
      "Creating 20 samples of length 517722\n",
      "iteration: 0  loss: 1.46102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1  loss: 1.173308\n",
      "iteration: 2  loss: 1.016755\n",
      "iteration: 3  loss: 0.918721\n",
      "iteration: 4  loss: 0.853909\n",
      "iteration: 5  loss: 0.809735\n",
      "iteration: 6  loss: 0.778995\n",
      "iteration: 7  loss: 0.757183\n",
      "iteration: 8  loss: 0.741534\n",
      "iteration: 9  loss: 0.730126\n",
      "iteration: 10  loss: 0.721665\n",
      "iteration: 11  loss: 0.715296\n",
      "iteration: 12  loss: 0.710526\n",
      "iteration: 13  loss: 0.706869\n",
      "iteration: 14  loss: 0.70403\n",
      "iteration: 15  loss: 0.701838\n",
      "iteration: 16  loss: 0.700122\n",
      "iteration: 17  loss: 0.698779\n",
      "iteration: 18  loss: 0.697706\n",
      "iteration: 19  loss: 0.696868\n",
      "Ranking time: 228.59\n",
      "Obtaining metrics time: 8.82\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 49 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 1\n",
      "NOLF:\t\t\t 10\n",
      "Sample Percentage:\t 0.05\n",
      "Creating 20 samples of length 51772\n",
      "iteration: 0  loss: 0.885005\n",
      "iteration: 1  loss: 0.861187\n",
      "iteration: 2  loss: 0.843604\n",
      "iteration: 3  loss: 0.829573\n",
      "iteration: 4  loss: 0.817777\n",
      "iteration: 5  loss: 0.80774\n",
      "iteration: 6  loss: 0.797936\n",
      "iteration: 7  loss: 0.789941\n",
      "iteration: 8  loss: 0.782367\n",
      "iteration: 9  loss: 0.775809\n",
      "iteration: 10  loss: 0.769252\n",
      "iteration: 11  loss: 0.763823\n",
      "iteration: 12  loss: 0.758788\n",
      "iteration: 13  loss: 0.754263\n",
      "iteration: 14  loss: 0.75018\n",
      "iteration: 15  loss: 0.746272\n",
      "iteration: 16  loss: 0.742715\n",
      "iteration: 17  loss: 0.739353\n",
      "iteration: 18  loss: 0.736091\n",
      "iteration: 19  loss: 0.733215\n",
      "Ranking time: 94.15\n",
      "Obtaining metrics time: 8.78\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 50 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 1\n",
      "NOLF:\t\t\t 10\n",
      "Sample Percentage:\t 0.1\n",
      "Creating 20 samples of length 103544\n",
      "iteration: 0  loss: 0.873102\n",
      "iteration: 1  loss: 0.83673\n",
      "iteration: 2  loss: 0.812212\n",
      "iteration: 3  loss: 0.793618\n",
      "iteration: 4  loss: 0.778157\n",
      "iteration: 5  loss: 0.765901\n",
      "iteration: 6  loss: 0.755547\n",
      "iteration: 7  loss: 0.747164\n",
      "iteration: 8  loss: 0.739844\n",
      "iteration: 9  loss: 0.73356\n",
      "iteration: 10  loss: 0.728166\n",
      "iteration: 11  loss: 0.723587\n",
      "iteration: 12  loss: 0.719743\n",
      "iteration: 13  loss: 0.716439\n",
      "iteration: 14  loss: 0.713502\n",
      "iteration: 15  loss: 0.711079\n",
      "iteration: 16  loss: 0.708888\n",
      "iteration: 17  loss: 0.707016\n",
      "iteration: 18  loss: 0.70534\n",
      "iteration: 19  loss: 0.703888\n",
      "Ranking time: 94.83\n",
      "Obtaining metrics time: 9.33\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 51 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 1\n",
      "NOLF:\t\t\t 10\n",
      "Sample Percentage:\t 0.2\n",
      "Creating 20 samples of length 207089\n",
      "iteration: 0  loss: 0.854889\n",
      "iteration: 1  loss: 0.80292\n",
      "iteration: 2  loss: 0.771829\n",
      "iteration: 3  loss: 0.750941\n",
      "iteration: 4  loss: 0.73609\n",
      "iteration: 5  loss: 0.725308\n",
      "iteration: 6  loss: 0.717433\n",
      "iteration: 7  loss: 0.711612\n",
      "iteration: 8  loss: 0.707341\n",
      "iteration: 9  loss: 0.704055\n",
      "iteration: 10  loss: 0.701603\n",
      "iteration: 11  loss: 0.699739\n",
      "iteration: 12  loss: 0.698294\n",
      "iteration: 13  loss: 0.697198\n",
      "iteration: 14  loss: 0.696345\n",
      "iteration: 15  loss: 0.695686\n",
      "iteration: 16  loss: 0.695167\n",
      "iteration: 17  loss: 0.694763\n",
      "iteration: 18  loss: 0.694446\n",
      "iteration: 19  loss: 0.694192\n",
      "Ranking time: 93.1\n",
      "Obtaining metrics time: 8.65\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 52 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 1\n",
      "NOLF:\t\t\t 10\n",
      "Sample Percentage:\t 0.5\n",
      "Creating 20 samples of length 517722\n",
      "iteration: 0  loss: 0.818831\n",
      "iteration: 1  loss: 0.747737\n",
      "iteration: 2  loss: 0.719329\n",
      "iteration: 3  loss: 0.706282\n",
      "iteration: 4  loss: 0.699946\n",
      "iteration: 5  loss: 0.696782\n",
      "iteration: 6  loss: 0.695138\n",
      "iteration: 7  loss: 0.694262\n",
      "iteration: 8  loss: 0.693784\n",
      "iteration: 9  loss: 0.69352\n",
      "iteration: 10  loss: 0.693368\n",
      "iteration: 11  loss: 0.693279\n",
      "iteration: 12  loss: 0.693227\n",
      "iteration: 13  loss: 0.693196\n",
      "iteration: 14  loss: 0.693177\n",
      "iteration: 15  loss: 0.693165\n",
      "iteration: 16  loss: 0.693158\n",
      "iteration: 17  loss: 0.693154\n",
      "iteration: 18  loss: 0.693151\n",
      "iteration: 19  loss: 0.69315\n",
      "Ranking time: 93.13\n",
      "Obtaining metrics time: 8.71\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 53 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 1\n",
      "NOLF:\t\t\t 20\n",
      "Sample Percentage:\t 0.05\n",
      "Creating 20 samples of length 51772\n",
      "iteration: 0  loss: 1.076736\n",
      "iteration: 1  loss: 1.02892\n",
      "iteration: 2  loss: 0.994821\n",
      "iteration: 3  loss: 0.966617\n",
      "iteration: 4  loss: 0.941985\n",
      "iteration: 5  loss: 0.921299\n",
      "iteration: 6  loss: 0.902873\n",
      "iteration: 7  loss: 0.886341\n",
      "iteration: 8  loss: 0.871176\n",
      "iteration: 9  loss: 0.858761\n",
      "iteration: 10  loss: 0.845211\n",
      "iteration: 11  loss: 0.834829\n",
      "iteration: 12  loss: 0.82443\n",
      "iteration: 13  loss: 0.815008\n",
      "iteration: 14  loss: 0.806735\n",
      "iteration: 15  loss: 0.798797\n",
      "iteration: 16  loss: 0.791607\n",
      "iteration: 17  loss: 0.785402\n",
      "iteration: 18  loss: 0.779025\n",
      "iteration: 19  loss: 0.773624\n",
      "Ranking time: 106.84\n",
      "Obtaining metrics time: 9.02\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 54 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 1\n",
      "NOLF:\t\t\t 20\n",
      "Sample Percentage:\t 0.1\n",
      "Creating 20 samples of length 103544\n",
      "iteration: 0  loss: 1.052209\n",
      "iteration: 1  loss: 0.980539\n",
      "iteration: 2  loss: 0.93127\n",
      "iteration: 3  loss: 0.894186\n",
      "iteration: 4  loss: 0.863837\n",
      "iteration: 5  loss: 0.838489\n",
      "iteration: 6  loss: 0.818159\n",
      "iteration: 7  loss: 0.800688\n",
      "iteration: 8  loss: 0.786243\n",
      "iteration: 9  loss: 0.774028\n",
      "iteration: 10  loss: 0.763068\n",
      "iteration: 11  loss: 0.754122\n",
      "iteration: 12  loss: 0.746255\n",
      "iteration: 13  loss: 0.73954\n",
      "iteration: 14  loss: 0.73389\n",
      "iteration: 15  loss: 0.728912\n",
      "iteration: 16  loss: 0.724647\n",
      "iteration: 17  loss: 0.721019\n",
      "iteration: 18  loss: 0.71756\n",
      "iteration: 19  loss: 0.714773\n",
      "Ranking time: 107.22\n",
      "Obtaining metrics time: 8.87\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 55 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 1\n",
      "NOLF:\t\t\t 20\n",
      "Sample Percentage:\t 0.2\n",
      "Creating 20 samples of length 207089\n",
      "iteration: 0  loss: 1.016169\n",
      "iteration: 1  loss: 0.912325\n",
      "iteration: 2  loss: 0.850756\n",
      "iteration: 3  loss: 0.808402\n",
      "iteration: 4  loss: 0.778929\n",
      "iteration: 5  loss: 0.757522\n",
      "iteration: 6  loss: 0.741723\n",
      "iteration: 7  loss: 0.730075\n",
      "iteration: 8  loss: 0.721578\n",
      "iteration: 9  loss: 0.715023\n",
      "iteration: 10  loss: 0.710039\n",
      "iteration: 11  loss: 0.706328\n",
      "iteration: 12  loss: 0.703434\n",
      "iteration: 13  loss: 0.701258\n",
      "iteration: 14  loss: 0.699533\n",
      "iteration: 15  loss: 0.698197\n",
      "iteration: 16  loss: 0.697192\n",
      "iteration: 17  loss: 0.696363\n",
      "iteration: 18  loss: 0.695741\n",
      "iteration: 19  loss: 0.695241\n",
      "Ranking time: 105.04\n",
      "Obtaining metrics time: 9.3\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 56 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 1\n",
      "NOLF:\t\t\t 20\n",
      "Sample Percentage:\t 0.5\n",
      "Creating 20 samples of length 517722\n",
      "iteration: 0  loss: 0.94416\n",
      "iteration: 1  loss: 0.802175\n",
      "iteration: 2  loss: 0.745512\n",
      "iteration: 3  loss: 0.719467\n",
      "iteration: 4  loss: 0.70679\n",
      "iteration: 5  loss: 0.700402\n",
      "iteration: 6  loss: 0.697122\n",
      "iteration: 7  loss: 0.695379\n",
      "iteration: 8  loss: 0.69442\n",
      "iteration: 9  loss: 0.69389\n",
      "iteration: 10  loss: 0.693586\n",
      "iteration: 11  loss: 0.693411\n",
      "iteration: 12  loss: 0.693308\n",
      "iteration: 13  loss: 0.693246\n",
      "iteration: 14  loss: 0.693208\n",
      "iteration: 15  loss: 0.693185\n",
      "iteration: 16  loss: 0.69317\n",
      "iteration: 17  loss: 0.693162\n",
      "iteration: 18  loss: 0.693156\n",
      "iteration: 19  loss: 0.693153\n",
      "Ranking time: 106.04\n",
      "Obtaining metrics time: 8.84\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 57 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 1\n",
      "NOLF:\t\t\t 50\n",
      "Sample Percentage:\t 0.05\n",
      "Creating 20 samples of length 51772\n",
      "iteration: 0  loss: 1.651916\n",
      "iteration: 1  loss: 1.532599\n",
      "iteration: 2  loss: 1.448089\n",
      "iteration: 3  loss: 1.37387\n",
      "iteration: 4  loss: 1.316025\n",
      "iteration: 5  loss: 1.26324\n",
      "iteration: 6  loss: 1.215728\n",
      "iteration: 7  loss: 1.176156\n",
      "iteration: 8  loss: 1.138008\n",
      "iteration: 9  loss: 1.1049\n",
      "iteration: 10  loss: 1.075807\n",
      "iteration: 11  loss: 1.046368\n",
      "iteration: 12  loss: 1.021511\n",
      "iteration: 13  loss: 0.997914\n",
      "iteration: 14  loss: 0.976868\n",
      "iteration: 15  loss: 0.957921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 16  loss: 0.940119\n",
      "iteration: 17  loss: 0.922859\n",
      "iteration: 18  loss: 0.90789\n",
      "iteration: 19  loss: 0.892778\n",
      "Ranking time: 151.35\n",
      "Obtaining metrics time: 8.76\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 58 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 1\n",
      "NOLF:\t\t\t 50\n",
      "Sample Percentage:\t 0.1\n",
      "Creating 20 samples of length 103544\n",
      "iteration: 0  loss: 1.592799\n",
      "iteration: 1  loss: 1.410054\n",
      "iteration: 2  loss: 1.288869\n",
      "iteration: 3  loss: 1.194223\n",
      "iteration: 4  loss: 1.118602\n",
      "iteration: 5  loss: 1.056903\n",
      "iteration: 6  loss: 1.005701\n",
      "iteration: 7  loss: 0.961969\n",
      "iteration: 8  loss: 0.926467\n",
      "iteration: 9  loss: 0.894216\n",
      "iteration: 10  loss: 0.86863\n",
      "iteration: 11  loss: 0.845606\n",
      "iteration: 12  loss: 0.826449\n",
      "iteration: 13  loss: 0.808984\n",
      "iteration: 14  loss: 0.794853\n",
      "iteration: 15  loss: 0.782044\n",
      "iteration: 16  loss: 0.771323\n",
      "iteration: 17  loss: 0.762428\n",
      "iteration: 18  loss: 0.754072\n",
      "iteration: 19  loss: 0.747165\n",
      "Ranking time: 149.6\n",
      "Obtaining metrics time: 8.79\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 59 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 1\n",
      "NOLF:\t\t\t 50\n",
      "Sample Percentage:\t 0.2\n",
      "Creating 20 samples of length 207089\n",
      "iteration: 0  loss: 1.501439\n",
      "iteration: 1  loss: 1.240402\n",
      "iteration: 2  loss: 1.086401\n",
      "iteration: 3  loss: 0.981566\n",
      "iteration: 4  loss: 0.907433\n",
      "iteration: 5  loss: 0.853826\n",
      "iteration: 6  loss: 0.814312\n",
      "iteration: 7  loss: 0.785304\n",
      "iteration: 8  loss: 0.763875\n",
      "iteration: 9  loss: 0.747659\n",
      "iteration: 10  loss: 0.735231\n",
      "iteration: 11  loss: 0.726164\n",
      "iteration: 12  loss: 0.718934\n",
      "iteration: 13  loss: 0.713319\n",
      "iteration: 14  loss: 0.709103\n",
      "iteration: 15  loss: 0.705803\n",
      "iteration: 16  loss: 0.703198\n",
      "iteration: 17  loss: 0.701204\n",
      "iteration: 18  loss: 0.699634\n",
      "iteration: 19  loss: 0.698366\n",
      "Ranking time: 150.72\n",
      "Obtaining metrics time: 8.91\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 60 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 1\n",
      "NOLF:\t\t\t 50\n",
      "Sample Percentage:\t 0.5\n",
      "Creating 20 samples of length 517722\n",
      "iteration: 0  loss: 1.320184\n",
      "iteration: 1  loss: 0.965755\n",
      "iteration: 2  loss: 0.824034\n",
      "iteration: 3  loss: 0.758649\n",
      "iteration: 4  loss: 0.72717\n",
      "iteration: 5  loss: 0.71131\n",
      "iteration: 6  loss: 0.703072\n",
      "iteration: 7  loss: 0.698728\n",
      "iteration: 8  loss: 0.696335\n",
      "iteration: 9  loss: 0.695005\n",
      "iteration: 10  loss: 0.694246\n",
      "iteration: 11  loss: 0.693805\n",
      "iteration: 12  loss: 0.693547\n",
      "iteration: 13  loss: 0.693393\n",
      "iteration: 14  loss: 0.693299\n",
      "iteration: 15  loss: 0.693242\n",
      "iteration: 16  loss: 0.693207\n",
      "iteration: 17  loss: 0.693185\n",
      "iteration: 18  loss: 0.693171\n",
      "iteration: 19  loss: 0.693162\n",
      "Ranking time: 149.82\n",
      "Obtaining metrics time: 8.74\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 61 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 1\n",
      "NOLF:\t\t\t 100\n",
      "Sample Percentage:\t 0.05\n",
      "Creating 20 samples of length 51772\n",
      "iteration: 0  loss: 2.606952\n",
      "iteration: 1  loss: 2.371512\n",
      "iteration: 2  loss: 2.198894\n",
      "iteration: 3  loss: 2.057782\n",
      "iteration: 4  loss: 1.938978\n",
      "iteration: 5  loss: 1.837502\n",
      "iteration: 6  loss: 1.742401\n",
      "iteration: 7  loss: 1.660316\n",
      "iteration: 8  loss: 1.583064\n",
      "iteration: 9  loss: 1.517048\n",
      "iteration: 10  loss: 1.45526\n",
      "iteration: 11  loss: 1.400141\n",
      "iteration: 12  loss: 1.347062\n",
      "iteration: 13  loss: 1.300838\n",
      "iteration: 14  loss: 1.261133\n",
      "iteration: 15  loss: 1.221577\n",
      "iteration: 16  loss: 1.185939\n",
      "iteration: 17  loss: 1.153472\n",
      "iteration: 18  loss: 1.120691\n",
      "iteration: 19  loss: 1.093169\n",
      "Ranking time: 231.95\n",
      "Obtaining metrics time: 8.75\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 62 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 1\n",
      "NOLF:\t\t\t 100\n",
      "Sample Percentage:\t 0.1\n",
      "Creating 20 samples of length 103544\n",
      "iteration: 0  loss: 2.490007\n",
      "iteration: 1  loss: 2.129577\n",
      "iteration: 2  loss: 1.884105\n",
      "iteration: 3  loss: 1.69689\n",
      "iteration: 4  loss: 1.543978\n",
      "iteration: 5  loss: 1.419319\n",
      "iteration: 6  loss: 1.315458\n",
      "iteration: 7  loss: 1.230054\n",
      "iteration: 8  loss: 1.158141\n",
      "iteration: 9  loss: 1.094519\n",
      "iteration: 10  loss: 1.042564\n",
      "iteration: 11  loss: 0.997378\n",
      "iteration: 12  loss: 0.958696\n",
      "iteration: 13  loss: 0.924474\n",
      "iteration: 14  loss: 0.896864\n",
      "iteration: 15  loss: 0.871272\n",
      "iteration: 16  loss: 0.850162\n",
      "iteration: 17  loss: 0.831349\n",
      "iteration: 18  loss: 0.814773\n",
      "iteration: 19  loss: 0.800451\n",
      "Ranking time: 228.29\n",
      "Obtaining metrics time: 8.99\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 63 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 1\n",
      "NOLF:\t\t\t 100\n",
      "Sample Percentage:\t 0.2\n",
      "Creating 20 samples of length 207089\n",
      "iteration: 0  loss: 2.30903\n",
      "iteration: 1  loss: 1.791328\n",
      "iteration: 2  loss: 1.478324\n",
      "iteration: 3  loss: 1.268517\n",
      "iteration: 4  loss: 1.121525\n",
      "iteration: 5  loss: 1.014044\n",
      "iteration: 6  loss: 0.935102\n",
      "iteration: 7  loss: 0.877573\n",
      "iteration: 8  loss: 0.834694\n",
      "iteration: 9  loss: 0.801516\n",
      "iteration: 10  loss: 0.777257\n",
      "iteration: 11  loss: 0.758757\n",
      "iteration: 12  loss: 0.744422\n",
      "iteration: 13  loss: 0.733555\n",
      "iteration: 14  loss: 0.724983\n",
      "iteration: 15  loss: 0.718353\n",
      "iteration: 16  loss: 0.713261\n",
      "iteration: 17  loss: 0.709227\n",
      "iteration: 18  loss: 0.706102\n",
      "iteration: 19  loss: 0.703567\n",
      "Ranking time: 229.71\n",
      "Obtaining metrics time: 8.88\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 64 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 1\n",
      "NOLF:\t\t\t 100\n",
      "Sample Percentage:\t 0.5\n",
      "Creating 20 samples of length 517722\n",
      "iteration: 0  loss: 1.947735\n",
      "iteration: 1  loss: 1.23765\n",
      "iteration: 2  loss: 0.954335\n",
      "iteration: 3  loss: 0.824168\n",
      "iteration: 4  loss: 0.760995\n",
      "iteration: 5  loss: 0.729369\n",
      "iteration: 6  loss: 0.712957\n",
      "iteration: 7  loss: 0.704276\n",
      "iteration: 8  loss: 0.699501\n",
      "iteration: 9  loss: 0.696849\n",
      "iteration: 10  loss: 0.695344\n",
      "iteration: 11  loss: 0.694457\n",
      "iteration: 12  loss: 0.693943\n",
      "iteration: 13  loss: 0.693638\n",
      "iteration: 14  loss: 0.69345\n",
      "iteration: 15  loss: 0.693338\n",
      "iteration: 16  loss: 0.693266\n",
      "iteration: 17  loss: 0.693222\n",
      "iteration: 18  loss: 0.693195\n",
      "iteration: 19  loss: 0.693178\n",
      "Ranking time: 228.07\n",
      "Obtaining metrics time: 8.92\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 65 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 1.5\n",
      "NOLF:\t\t\t 10\n",
      "Sample Percentage:\t 0.05\n",
      "Creating 20 samples of length 51772\n",
      "iteration: 0  loss: 0.972014\n",
      "iteration: 1  loss: 0.926179\n",
      "iteration: 2  loss: 0.894947\n",
      "iteration: 3  loss: 0.869401\n",
      "iteration: 4  loss: 0.848715\n",
      "iteration: 5  loss: 0.831631\n",
      "iteration: 6  loss: 0.815849\n",
      "iteration: 7  loss: 0.802245\n",
      "iteration: 8  loss: 0.790561\n",
      "iteration: 9  loss: 0.780648\n",
      "iteration: 10  loss: 0.771356\n",
      "iteration: 11  loss: 0.763362\n",
      "iteration: 12  loss: 0.756504\n",
      "iteration: 13  loss: 0.750105\n",
      "iteration: 14  loss: 0.744605\n",
      "iteration: 15  loss: 0.739886\n",
      "iteration: 16  loss: 0.735432\n",
      "iteration: 17  loss: 0.731472\n",
      "iteration: 18  loss: 0.72765\n",
      "iteration: 19  loss: 0.724442\n",
      "Ranking time: 93.78\n",
      "Obtaining metrics time: 8.67\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 66 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 1.5\n",
      "NOLF:\t\t\t 10\n",
      "Sample Percentage:\t 0.1\n",
      "Creating 20 samples of length 103544\n",
      "iteration: 0  loss: 0.948985\n",
      "iteration: 1  loss: 0.881746\n",
      "iteration: 2  loss: 0.839567\n",
      "iteration: 3  loss: 0.808492\n",
      "iteration: 4  loss: 0.78469\n",
      "iteration: 5  loss: 0.766616\n",
      "iteration: 6  loss: 0.75217\n",
      "iteration: 7  loss: 0.741141\n",
      "iteration: 8  loss: 0.73205\n",
      "iteration: 9  loss: 0.72475\n",
      "iteration: 10  loss: 0.718952\n",
      "iteration: 11  loss: 0.714388\n",
      "iteration: 12  loss: 0.710522\n",
      "iteration: 13  loss: 0.707523\n",
      "iteration: 14  loss: 0.705081\n",
      "iteration: 15  loss: 0.703055\n",
      "iteration: 16  loss: 0.701353\n",
      "iteration: 17  loss: 0.700097\n",
      "iteration: 18  loss: 0.698875\n",
      "iteration: 19  loss: 0.697979\n",
      "Ranking time: 96.64\n",
      "Obtaining metrics time: 11.32\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 67 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 1.5\n",
      "NOLF:\t\t\t 10\n",
      "Sample Percentage:\t 0.2\n",
      "Creating 20 samples of length 207089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0  loss: 0.915504\n",
      "iteration: 1  loss: 0.823793\n",
      "iteration: 2  loss: 0.775032\n",
      "iteration: 3  loss: 0.746001\n",
      "iteration: 4  loss: 0.727877\n",
      "iteration: 5  loss: 0.716105\n",
      "iteration: 6  loss: 0.708496\n",
      "iteration: 7  loss: 0.703569\n",
      "iteration: 8  loss: 0.700307\n",
      "iteration: 9  loss: 0.698069\n",
      "iteration: 10  loss: 0.696602\n",
      "iteration: 11  loss: 0.695568\n",
      "iteration: 12  loss: 0.694862\n",
      "iteration: 13  loss: 0.694385\n",
      "iteration: 14  loss: 0.694033\n",
      "iteration: 15  loss: 0.693789\n",
      "iteration: 16  loss: 0.693615\n",
      "iteration: 17  loss: 0.69349\n",
      "iteration: 18  loss: 0.693401\n",
      "iteration: 19  loss: 0.693338\n",
      "Ranking time: 94.06\n",
      "Obtaining metrics time: 8.72\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 68 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 1.5\n",
      "NOLF:\t\t\t 10\n",
      "Sample Percentage:\t 0.5\n",
      "Creating 20 samples of length 517722\n",
      "iteration: 0  loss: 0.852581\n",
      "iteration: 1  loss: 0.742512\n",
      "iteration: 2  loss: 0.710458\n",
      "iteration: 3  loss: 0.699633\n",
      "iteration: 4  loss: 0.695719\n",
      "iteration: 5  loss: 0.694221\n",
      "iteration: 6  loss: 0.693613\n",
      "iteration: 7  loss: 0.693356\n",
      "iteration: 8  loss: 0.693244\n",
      "iteration: 9  loss: 0.693193\n",
      "iteration: 10  loss: 0.693169\n",
      "iteration: 11  loss: 0.693158\n",
      "iteration: 12  loss: 0.693152\n",
      "iteration: 13  loss: 0.693149\n",
      "iteration: 14  loss: 0.693148\n",
      "iteration: 15  loss: 0.693147\n",
      "iteration: 16  loss: 0.693147\n",
      "iteration: 17  loss: 0.693147\n",
      "iteration: 18  loss: 0.693147\n",
      "iteration: 19  loss: 0.693147\n",
      "Ranking time: 92.04\n",
      "Obtaining metrics time: 8.87\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 69 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 1.5\n",
      "NOLF:\t\t\t 20\n",
      "Sample Percentage:\t 0.05\n",
      "Creating 20 samples of length 51772\n",
      "iteration: 0  loss: 1.25047\n",
      "iteration: 1  loss: 1.158769\n",
      "iteration: 2  loss: 1.096484\n",
      "iteration: 3  loss: 1.046285\n",
      "iteration: 4  loss: 1.003595\n",
      "iteration: 5  loss: 0.968276\n",
      "iteration: 6  loss: 0.937887\n",
      "iteration: 7  loss: 0.912676\n",
      "iteration: 8  loss: 0.886743\n",
      "iteration: 9  loss: 0.868404\n",
      "iteration: 10  loss: 0.848282\n",
      "iteration: 11  loss: 0.834043\n",
      "iteration: 12  loss: 0.819721\n",
      "iteration: 13  loss: 0.807165\n",
      "iteration: 14  loss: 0.796072\n",
      "iteration: 15  loss: 0.786027\n",
      "iteration: 16  loss: 0.777034\n",
      "iteration: 17  loss: 0.769579\n",
      "iteration: 18  loss: 0.762002\n",
      "iteration: 19  loss: 0.756211\n",
      "Ranking time: 108.79\n",
      "Obtaining metrics time: 8.89\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 70 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 1.5\n",
      "NOLF:\t\t\t 20\n",
      "Sample Percentage:\t 0.1\n",
      "Creating 20 samples of length 103544\n",
      "iteration: 0  loss: 1.204088\n",
      "iteration: 1  loss: 1.071122\n",
      "iteration: 2  loss: 0.98511\n",
      "iteration: 3  loss: 0.923875\n",
      "iteration: 4  loss: 0.876045\n",
      "iteration: 5  loss: 0.839499\n",
      "iteration: 6  loss: 0.811321\n",
      "iteration: 7  loss: 0.788503\n",
      "iteration: 8  loss: 0.77067\n",
      "iteration: 9  loss: 0.756455\n",
      "iteration: 10  loss: 0.744975\n",
      "iteration: 11  loss: 0.735683\n",
      "iteration: 12  loss: 0.727963\n",
      "iteration: 13  loss: 0.722057\n",
      "iteration: 14  loss: 0.717088\n",
      "iteration: 15  loss: 0.712894\n",
      "iteration: 16  loss: 0.709705\n",
      "iteration: 17  loss: 0.706961\n",
      "iteration: 18  loss: 0.704748\n",
      "iteration: 19  loss: 0.702812\n",
      "Ranking time: 108.0\n",
      "Obtaining metrics time: 8.94\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 71 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 1.5\n",
      "NOLF:\t\t\t 20\n",
      "Sample Percentage:\t 0.2\n",
      "Creating 20 samples of length 207089\n",
      "iteration: 0  loss: 1.137632\n",
      "iteration: 1  loss: 0.954344\n",
      "iteration: 2  loss: 0.857167\n",
      "iteration: 3  loss: 0.798763\n",
      "iteration: 4  loss: 0.762513\n",
      "iteration: 5  loss: 0.739131\n",
      "iteration: 6  loss: 0.723844\n",
      "iteration: 7  loss: 0.714021\n",
      "iteration: 8  loss: 0.707446\n",
      "iteration: 9  loss: 0.703064\n",
      "iteration: 10  loss: 0.699993\n",
      "iteration: 11  loss: 0.697984\n",
      "iteration: 12  loss: 0.69657\n",
      "iteration: 13  loss: 0.695589\n",
      "iteration: 14  loss: 0.69491\n",
      "iteration: 15  loss: 0.694431\n",
      "iteration: 16  loss: 0.6941\n",
      "iteration: 17  loss: 0.693831\n",
      "iteration: 18  loss: 0.693656\n",
      "iteration: 19  loss: 0.693529\n",
      "Ranking time: 106.31\n",
      "Obtaining metrics time: 8.78\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 72 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 1.5\n",
      "NOLF:\t\t\t 20\n",
      "Sample Percentage:\t 0.5\n",
      "Creating 20 samples of length 517722\n",
      "iteration: 0  loss: 1.011913\n",
      "iteration: 1  loss: 0.791679\n",
      "iteration: 2  loss: 0.727832\n",
      "iteration: 3  loss: 0.706179\n",
      "iteration: 4  loss: 0.698263\n",
      "iteration: 5  loss: 0.695282\n",
      "iteration: 6  loss: 0.694075\n",
      "iteration: 7  loss: 0.693565\n",
      "iteration: 8  loss: 0.693342\n",
      "iteration: 9  loss: 0.693241\n",
      "iteration: 10  loss: 0.693192\n",
      "iteration: 11  loss: 0.693169\n",
      "iteration: 12  loss: 0.693158\n",
      "iteration: 13  loss: 0.693152\n",
      "iteration: 14  loss: 0.69315\n",
      "iteration: 15  loss: 0.693148\n",
      "iteration: 16  loss: 0.693147\n",
      "iteration: 17  loss: 0.693147\n",
      "iteration: 18  loss: 0.693147\n",
      "iteration: 19  loss: 0.693147\n",
      "Ranking time: 104.58\n",
      "Obtaining metrics time: 8.69\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 73 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 1.5\n",
      "NOLF:\t\t\t 50\n",
      "Sample Percentage:\t 0.05\n",
      "Creating 20 samples of length 51772\n",
      "iteration: 0  loss: 2.08411\n",
      "iteration: 1  loss: 1.859001\n",
      "iteration: 2  loss: 1.703017\n",
      "iteration: 3  loss: 1.570756\n",
      "iteration: 4  loss: 1.471049\n",
      "iteration: 5  loss: 1.379662\n",
      "iteration: 6  loss: 1.304763\n",
      "iteration: 7  loss: 1.237964\n",
      "iteration: 8  loss: 1.178171\n",
      "iteration: 9  loss: 1.129032\n",
      "iteration: 10  loss: 1.086447\n",
      "iteration: 11  loss: 1.04483\n",
      "iteration: 12  loss: 1.010989\n",
      "iteration: 13  loss: 0.9784\n",
      "iteration: 14  loss: 0.950321\n",
      "iteration: 15  loss: 0.925527\n",
      "iteration: 16  loss: 0.905295\n",
      "iteration: 17  loss: 0.882885\n",
      "iteration: 18  loss: 0.865887\n",
      "iteration: 19  loss: 0.849347\n",
      "Ranking time: 149.45\n",
      "Obtaining metrics time: 9.14\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 74 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 1.5\n",
      "NOLF:\t\t\t 50\n",
      "Sample Percentage:\t 0.1\n",
      "Creating 20 samples of length 103544\n",
      "iteration: 0  loss: 1.971215\n",
      "iteration: 1  loss: 1.636512\n",
      "iteration: 2  loss: 1.422893\n",
      "iteration: 3  loss: 1.2687\n",
      "iteration: 4  loss: 1.149353\n",
      "iteration: 5  loss: 1.059449\n",
      "iteration: 6  loss: 0.988381\n",
      "iteration: 7  loss: 0.931562\n",
      "iteration: 8  loss: 0.88778\n",
      "iteration: 9  loss: 0.851982\n",
      "iteration: 10  loss: 0.822538\n",
      "iteration: 11  loss: 0.799258\n",
      "iteration: 12  loss: 0.780384\n",
      "iteration: 13  loss: 0.764919\n",
      "iteration: 14  loss: 0.752833\n",
      "iteration: 15  loss: 0.742461\n",
      "iteration: 16  loss: 0.734176\n",
      "iteration: 17  loss: 0.7277\n",
      "iteration: 18  loss: 0.721868\n",
      "iteration: 19  loss: 0.717443\n",
      "Ranking time: 150.89\n",
      "Obtaining metrics time: 8.59\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 75 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 1.5\n",
      "NOLF:\t\t\t 50\n",
      "Sample Percentage:\t 0.2\n",
      "Creating 20 samples of length 207089\n",
      "iteration: 0  loss: 1.805036\n",
      "iteration: 1  loss: 1.344822\n",
      "iteration: 2  loss: 1.102199\n",
      "iteration: 3  loss: 0.957675\n",
      "iteration: 4  loss: 0.866367\n",
      "iteration: 5  loss: 0.807996\n",
      "iteration: 6  loss: 0.770121\n",
      "iteration: 7  loss: 0.744982\n",
      "iteration: 8  loss: 0.72862\n",
      "iteration: 9  loss: 0.717791\n",
      "iteration: 10  loss: 0.710236\n",
      "iteration: 11  loss: 0.705263\n",
      "iteration: 12  loss: 0.701748\n",
      "iteration: 13  loss: 0.699259\n",
      "iteration: 14  loss: 0.697572\n",
      "iteration: 15  loss: 0.69633\n",
      "iteration: 16  loss: 0.695469\n",
      "iteration: 17  loss: 0.694861\n",
      "iteration: 18  loss: 0.694421\n",
      "iteration: 19  loss: 0.6941\n",
      "Ranking time: 150.06\n",
      "Obtaining metrics time: 8.89\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 76 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 1.5\n",
      "NOLF:\t\t\t 50\n",
      "Sample Percentage:\t 0.5\n",
      "Creating 20 samples of length 517722\n",
      "iteration: 0  loss: 1.489526\n",
      "iteration: 1  loss: 0.939685\n",
      "iteration: 2  loss: 0.779664\n",
      "iteration: 3  loss: 0.725527\n",
      "iteration: 4  loss: 0.70596\n",
      "iteration: 5  loss: 0.698493\n",
      "iteration: 6  loss: 0.695467\n",
      "iteration: 7  loss: 0.694183\n",
      "iteration: 8  loss: 0.693636\n",
      "iteration: 9  loss: 0.693381\n",
      "iteration: 10  loss: 0.693261\n",
      "iteration: 11  loss: 0.693204\n",
      "iteration: 12  loss: 0.693175\n",
      "iteration: 13  loss: 0.693161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 14  loss: 0.693154\n",
      "iteration: 15  loss: 0.69315\n",
      "iteration: 16  loss: 0.693149\n",
      "iteration: 17  loss: 0.693148\n",
      "iteration: 18  loss: 0.693147\n",
      "iteration: 19  loss: 0.693147\n",
      "Ranking time: 150.39\n",
      "Obtaining metrics time: 9.45\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 77 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 1.5\n",
      "NOLF:\t\t\t 100\n",
      "Sample Percentage:\t 0.05\n",
      "Creating 20 samples of length 51772\n",
      "iteration: 0  loss: 3.47424\n",
      "iteration: 1  loss: 3.026045\n",
      "iteration: 2  loss: 2.711486\n",
      "iteration: 3  loss: 2.45824\n",
      "iteration: 4  loss: 2.24808\n",
      "iteration: 5  loss: 2.074545\n",
      "iteration: 6  loss: 1.918373\n",
      "iteration: 7  loss: 1.787068\n",
      "iteration: 8  loss: 1.664154\n",
      "iteration: 9  loss: 1.561922\n",
      "iteration: 10  loss: 1.47492\n",
      "iteration: 11  loss: 1.393332\n",
      "iteration: 12  loss: 1.323759\n",
      "iteration: 13  loss: 1.259408\n",
      "iteration: 14  loss: 1.208294\n",
      "iteration: 15  loss: 1.155238\n",
      "iteration: 16  loss: 1.114186\n",
      "iteration: 17  loss: 1.075201\n",
      "iteration: 18  loss: 1.036847\n",
      "iteration: 19  loss: 1.006963\n",
      "Ranking time: 232.67\n",
      "Obtaining metrics time: 8.79\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 78 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 1.5\n",
      "NOLF:\t\t\t 100\n",
      "Sample Percentage:\t 0.1\n",
      "Creating 20 samples of length 103544\n",
      "iteration: 0  loss: 3.253082\n",
      "iteration: 1  loss: 2.582414\n",
      "iteration: 2  loss: 2.157196\n",
      "iteration: 3  loss: 1.846381\n",
      "iteration: 4  loss: 1.604219\n",
      "iteration: 5  loss: 1.424371\n",
      "iteration: 6  loss: 1.279754\n",
      "iteration: 7  loss: 1.170703\n",
      "iteration: 8  loss: 1.08146\n",
      "iteration: 9  loss: 1.007884\n",
      "iteration: 10  loss: 0.951789\n",
      "iteration: 11  loss: 0.904603\n",
      "iteration: 12  loss: 0.8682\n",
      "iteration: 13  loss: 0.836902\n",
      "iteration: 14  loss: 0.812706\n",
      "iteration: 15  loss: 0.792008\n",
      "iteration: 16  loss: 0.77567\n",
      "iteration: 17  loss: 0.762393\n",
      "iteration: 18  loss: 0.750734\n",
      "iteration: 19  loss: 0.741493\n",
      "Ranking time: 230.1\n",
      "Obtaining metrics time: 8.89\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 79 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 1.5\n",
      "NOLF:\t\t\t 100\n",
      "Sample Percentage:\t 0.2\n",
      "Creating 20 samples of length 207089\n",
      "iteration: 0  loss: 2.916152\n",
      "iteration: 1  loss: 1.998089\n",
      "iteration: 2  loss: 1.509922\n",
      "iteration: 3  loss: 1.21872\n",
      "iteration: 4  loss: 1.03878\n",
      "iteration: 5  loss: 0.922227\n",
      "iteration: 6  loss: 0.847004\n",
      "iteration: 7  loss: 0.797189\n",
      "iteration: 8  loss: 0.764829\n",
      "iteration: 9  loss: 0.742422\n",
      "iteration: 10  loss: 0.727453\n",
      "iteration: 11  loss: 0.717434\n",
      "iteration: 12  loss: 0.710386\n",
      "iteration: 13  loss: 0.705425\n",
      "iteration: 14  loss: 0.702058\n",
      "iteration: 15  loss: 0.699589\n",
      "iteration: 16  loss: 0.697855\n",
      "iteration: 17  loss: 0.696626\n",
      "iteration: 18  loss: 0.695709\n",
      "iteration: 19  loss: 0.695051\n",
      "Ranking time: 233.35\n",
      "Obtaining metrics time: 8.73\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 80 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 1.5\n",
      "NOLF:\t\t\t 100\n",
      "Sample Percentage:\t 0.5\n",
      "Creating 20 samples of length 517722\n",
      "iteration: 0  loss: 2.285833\n",
      "iteration: 1  loss: 1.185332\n",
      "iteration: 2  loss: 0.865961\n",
      "iteration: 3  loss: 0.757924\n",
      "iteration: 4  loss: 0.718775\n",
      "iteration: 5  loss: 0.703823\n",
      "iteration: 6  loss: 0.697802\n",
      "iteration: 7  loss: 0.695244\n",
      "iteration: 8  loss: 0.694126\n",
      "iteration: 9  loss: 0.693617\n",
      "iteration: 10  loss: 0.693377\n",
      "iteration: 11  loss: 0.693259\n",
      "iteration: 12  loss: 0.693203\n",
      "iteration: 13  loss: 0.693176\n",
      "iteration: 14  loss: 0.693162\n",
      "iteration: 15  loss: 0.693155\n",
      "iteration: 16  loss: 0.693151\n",
      "iteration: 17  loss: 0.693149\n",
      "iteration: 18  loss: 0.693148\n",
      "iteration: 19  loss: 0.693147\n",
      "Ranking time: 227.78\n",
      "Obtaining metrics time: 8.63\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 81 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 10\n",
      "NOLF:\t\t\t 10\n",
      "Sample Percentage:\t 0.05\n",
      "Creating 20 samples of length 51772\n",
      "iteration: 0  loss: 2.212498\n",
      "iteration: 1  loss: 1.616992\n",
      "iteration: 2  loss: 1.297594\n",
      "iteration: 3  loss: 1.087272\n",
      "iteration: 4  loss: 0.957111\n",
      "iteration: 5  loss: 0.874536\n",
      "iteration: 6  loss: 0.818423\n",
      "iteration: 7  loss: 0.778672\n",
      "iteration: 8  loss: 0.751926\n",
      "iteration: 9  loss: 0.734979\n",
      "iteration: 10  loss: 0.723053\n",
      "iteration: 11  loss: 0.714139\n",
      "iteration: 12  loss: 0.707849\n",
      "iteration: 13  loss: 0.703197\n",
      "iteration: 14  loss: 0.699999\n",
      "iteration: 15  loss: 0.699021\n",
      "iteration: 16  loss: 0.697472\n",
      "iteration: 17  loss: 0.695764\n",
      "iteration: 18  loss: 0.695545\n",
      "iteration: 19  loss: 0.694624\n",
      "Ranking time: 103.77\n",
      "Obtaining metrics time: 8.91\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 82 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 10\n",
      "NOLF:\t\t\t 10\n",
      "Sample Percentage:\t 0.1\n",
      "Creating 20 samples of length 103544\n",
      "iteration: 0  loss: 1.914156\n",
      "iteration: 1  loss: 1.194519\n",
      "iteration: 2  loss: 0.916151\n",
      "iteration: 3  loss: 0.796838\n",
      "iteration: 4  loss: 0.743825\n",
      "iteration: 5  loss: 0.717987\n",
      "iteration: 6  loss: 0.705009\n",
      "iteration: 7  loss: 0.699837\n",
      "iteration: 8  loss: 0.696542\n",
      "iteration: 9  loss: 0.695195\n",
      "iteration: 10  loss: 0.694134\n",
      "iteration: 11  loss: 0.693664\n",
      "iteration: 12  loss: 0.69339\n",
      "iteration: 13  loss: 0.693356\n",
      "iteration: 14  loss: 0.693213\n",
      "iteration: 15  loss: 0.693219\n",
      "iteration: 16  loss: 0.693157\n",
      "iteration: 17  loss: 0.693164\n",
      "iteration: 18  loss: 0.693176\n",
      "iteration: 19  loss: 0.693149\n",
      "Ranking time: 97.66\n",
      "Obtaining metrics time: 8.72\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 83 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 10\n",
      "NOLF:\t\t\t 10\n",
      "Sample Percentage:\t 0.2\n",
      "Creating 20 samples of length 207089\n",
      "iteration: 0  loss: 1.554292\n",
      "iteration: 1  loss: 0.856835\n",
      "iteration: 2  loss: 0.730103\n",
      "iteration: 3  loss: 0.702466\n",
      "iteration: 4  loss: 0.69569\n",
      "iteration: 5  loss: 0.693804\n",
      "iteration: 6  loss: 0.693367\n",
      "iteration: 7  loss: 0.693229\n",
      "iteration: 8  loss: 0.693169\n",
      "iteration: 9  loss: 0.693153\n",
      "iteration: 10  loss: 0.693148\n",
      "iteration: 11  loss: 0.693148\n",
      "iteration: 12  loss: 0.693147\n",
      "iteration: 13  loss: 0.693147\n",
      "iteration: 14  loss: 0.693147\n",
      "iteration: 15  loss: 0.693147\n",
      "iteration: 16  loss: 0.693147\n",
      "iteration: 17  loss: 0.693147\n",
      "iteration: 18  loss: 0.693147\n",
      "iteration: 19  loss: 0.693147\n",
      "Ranking time: 97.09\n",
      "Obtaining metrics time: 8.8\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 84 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 10\n",
      "NOLF:\t\t\t 10\n",
      "Sample Percentage:\t 0.5\n",
      "Creating 20 samples of length 517722\n",
      "iteration: 0  loss: 1.112625\n",
      "iteration: 1  loss: 0.702772\n",
      "iteration: 2  loss: 0.693557\n",
      "iteration: 3  loss: 0.693171\n",
      "iteration: 4  loss: 0.693147\n",
      "iteration: 5  loss: 0.693147\n",
      "iteration: 6  loss: 0.693147\n",
      "iteration: 7  loss: 0.693147\n",
      "iteration: 8  loss: 0.693147\n",
      "iteration: 9  loss: 0.693147\n",
      "iteration: 10  loss: 0.693147\n",
      "iteration: 11  loss: 0.693147\n",
      "iteration: 12  loss: 0.693147\n",
      "iteration: 13  loss: 0.693147\n",
      "iteration: 14  loss: 0.693147\n",
      "iteration: 15  loss: 0.693147\n",
      "iteration: 16  loss: 0.693147\n",
      "iteration: 17  loss: 0.693147\n",
      "iteration: 18  loss: 0.693147\n",
      "iteration: 19  loss: 0.693147\n",
      "Ranking time: 94.65\n",
      "Obtaining metrics time: 9.06\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 85 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 10\n",
      "NOLF:\t\t\t 20\n",
      "Sample Percentage:\t 0.05\n",
      "Creating 20 samples of length 51772\n",
      "iteration: 0  loss: 3.721841\n",
      "iteration: 1  loss: 2.546476\n",
      "iteration: 2  loss: 1.901567\n",
      "iteration: 3  loss: 1.486209\n",
      "iteration: 4  loss: 1.2267\n",
      "iteration: 5  loss: 1.054816\n",
      "iteration: 6  loss: 0.938392\n",
      "iteration: 7  loss: 0.864364\n",
      "iteration: 8  loss: 0.808165\n",
      "iteration: 9  loss: 0.777956\n",
      "iteration: 10  loss: 0.748246\n",
      "iteration: 11  loss: 0.732838\n",
      "iteration: 12  loss: 0.722279\n",
      "iteration: 13  loss: 0.71269\n",
      "iteration: 14  loss: 0.707744\n",
      "iteration: 15  loss: 0.703451\n",
      "iteration: 16  loss: 0.700705\n",
      "iteration: 17  loss: 0.699008\n",
      "iteration: 18  loss: 0.697116\n",
      "iteration: 19  loss: 0.696308\n",
      "Ranking time: 104.84\n",
      "Obtaining metrics time: 8.65\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 86 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 10\n",
      "NOLF:\t\t\t 20\n",
      "Sample Percentage:\t 0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 20 samples of length 103544\n",
      "iteration: 0  loss: 3.128187\n",
      "iteration: 1  loss: 1.694676\n",
      "iteration: 2  loss: 1.14115\n",
      "iteration: 3  loss: 0.901098\n",
      "iteration: 4  loss: 0.792001\n",
      "iteration: 5  loss: 0.741102\n",
      "iteration: 6  loss: 0.71736\n",
      "iteration: 7  loss: 0.705469\n",
      "iteration: 8  loss: 0.700288\n",
      "iteration: 9  loss: 0.696914\n",
      "iteration: 10  loss: 0.69538\n",
      "iteration: 11  loss: 0.69428\n",
      "iteration: 12  loss: 0.693801\n",
      "iteration: 13  loss: 0.693475\n",
      "iteration: 14  loss: 0.693297\n",
      "iteration: 15  loss: 0.69327\n",
      "iteration: 16  loss: 0.693172\n",
      "iteration: 17  loss: 0.693205\n",
      "iteration: 18  loss: 0.693175\n",
      "iteration: 19  loss: 0.693149\n",
      "Ranking time: 112.18\n",
      "Obtaining metrics time: 8.81\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 87 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 10\n",
      "NOLF:\t\t\t 20\n",
      "Sample Percentage:\t 0.2\n",
      "Creating 20 samples of length 207089\n",
      "iteration: 0  loss: 2.41295\n",
      "iteration: 1  loss: 1.020363\n",
      "iteration: 2  loss: 0.766635\n",
      "iteration: 3  loss: 0.711393\n",
      "iteration: 4  loss: 0.698453\n",
      "iteration: 5  loss: 0.694662\n",
      "iteration: 6  loss: 0.693638\n",
      "iteration: 7  loss: 0.69328\n",
      "iteration: 8  loss: 0.693176\n",
      "iteration: 9  loss: 0.693161\n",
      "iteration: 10  loss: 0.69315\n",
      "iteration: 11  loss: 0.693147\n",
      "iteration: 12  loss: 0.693149\n",
      "iteration: 13  loss: 0.693147\n",
      "iteration: 14  loss: 0.693147\n",
      "iteration: 15  loss: 0.693147\n",
      "iteration: 16  loss: 0.693147\n",
      "iteration: 17  loss: 0.693147\n",
      "iteration: 18  loss: 0.693147\n",
      "iteration: 19  loss: 0.693147\n",
      "Ranking time: 95.61\n",
      "Obtaining metrics time: 8.92\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 88 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 10\n",
      "NOLF:\t\t\t 20\n",
      "Sample Percentage:\t 0.5\n",
      "Creating 20 samples of length 517722\n",
      "iteration: 0  loss: 1.53264\n",
      "iteration: 1  loss: 0.711981\n",
      "iteration: 2  loss: 0.694021\n",
      "iteration: 3  loss: 0.693174\n",
      "iteration: 4  loss: 0.693151\n",
      "iteration: 5  loss: 0.693147\n",
      "iteration: 6  loss: 0.693147\n",
      "iteration: 7  loss: 0.693147\n",
      "iteration: 8  loss: 0.693147\n",
      "iteration: 9  loss: 0.693147\n",
      "iteration: 10  loss: 0.693147\n",
      "iteration: 11  loss: 0.693147\n",
      "iteration: 12  loss: 0.693147\n",
      "iteration: 13  loss: 0.693147\n",
      "iteration: 14  loss: 0.693147\n",
      "iteration: 15  loss: 0.693147\n",
      "iteration: 16  loss: 0.693147\n",
      "iteration: 17  loss: 0.693147\n",
      "iteration: 18  loss: 0.693147\n",
      "iteration: 19  loss: 0.693147\n",
      "Ranking time: 102.11\n",
      "Obtaining metrics time: 8.97\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 89 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 10\n",
      "NOLF:\t\t\t 50\n",
      "Sample Percentage:\t 0.05\n",
      "Creating 20 samples of length 51772\n",
      "iteration: 0  loss: 8.267489\n",
      "iteration: 1  loss: 5.292604\n",
      "iteration: 2  loss: 3.718135\n",
      "iteration: 3  loss: 2.663077\n",
      "iteration: 4  loss: 2.036753\n",
      "iteration: 5  loss: 1.593811\n",
      "iteration: 6  loss: 1.30084\n",
      "iteration: 7  loss: 1.123542\n",
      "iteration: 8  loss: 0.979938\n",
      "iteration: 9  loss: 0.895082\n",
      "iteration: 10  loss: 0.83929\n",
      "iteration: 11  loss: 0.789926\n",
      "iteration: 12  loss: 0.764103\n",
      "iteration: 13  loss: 0.74533\n",
      "iteration: 14  loss: 0.732538\n",
      "iteration: 15  loss: 0.718653\n",
      "iteration: 16  loss: 0.712871\n",
      "iteration: 17  loss: 0.707806\n",
      "iteration: 18  loss: 0.703667\n",
      "iteration: 19  loss: 0.700625\n",
      "Ranking time: 149.74\n",
      "Obtaining metrics time: 8.81\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 90 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 10\n",
      "NOLF:\t\t\t 50\n",
      "Sample Percentage:\t 0.1\n",
      "Creating 20 samples of length 103544\n",
      "iteration: 0  loss: 6.778498\n",
      "iteration: 1  loss: 3.185224\n",
      "iteration: 2  loss: 1.817382\n",
      "iteration: 3  loss: 1.213991\n",
      "iteration: 4  loss: 0.932895\n",
      "iteration: 5  loss: 0.817276\n",
      "iteration: 6  loss: 0.755083\n",
      "iteration: 7  loss: 0.725078\n",
      "iteration: 8  loss: 0.709552\n",
      "iteration: 9  loss: 0.702227\n",
      "iteration: 10  loss: 0.698365\n",
      "iteration: 11  loss: 0.695769\n",
      "iteration: 12  loss: 0.695009\n",
      "iteration: 13  loss: 0.694077\n",
      "iteration: 14  loss: 0.693498\n",
      "iteration: 15  loss: 0.693468\n",
      "iteration: 16  loss: 0.693284\n",
      "iteration: 17  loss: 0.693187\n",
      "iteration: 18  loss: 0.693175\n",
      "iteration: 19  loss: 0.693224\n",
      "Ranking time: 147.11\n",
      "Obtaining metrics time: 8.85\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 91 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 10\n",
      "NOLF:\t\t\t 50\n",
      "Sample Percentage:\t 0.2\n",
      "Creating 20 samples of length 207089\n",
      "iteration: 0  loss: 4.988875\n",
      "iteration: 1  loss: 1.518606\n",
      "iteration: 2  loss: 0.876437\n",
      "iteration: 3  loss: 0.738484\n",
      "iteration: 4  loss: 0.706139\n",
      "iteration: 5  loss: 0.696897\n",
      "iteration: 6  loss: 0.694351\n",
      "iteration: 7  loss: 0.693503\n",
      "iteration: 8  loss: 0.693235\n",
      "iteration: 9  loss: 0.693166\n",
      "iteration: 10  loss: 0.693149\n",
      "iteration: 11  loss: 0.693147\n",
      "iteration: 12  loss: 0.693147\n",
      "iteration: 13  loss: 0.693147\n",
      "iteration: 14  loss: 0.693147\n",
      "iteration: 15  loss: 0.693147\n",
      "iteration: 16  loss: 0.693147\n",
      "iteration: 17  loss: 0.693147\n",
      "iteration: 18  loss: 0.693147\n",
      "iteration: 19  loss: 0.693147\n",
      "Ranking time: 151.17\n",
      "Obtaining metrics time: 8.8\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 92 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 10\n",
      "NOLF:\t\t\t 50\n",
      "Sample Percentage:\t 0.5\n",
      "Creating 20 samples of length 517722\n",
      "iteration: 0  loss: 2.789105\n",
      "iteration: 1  loss: 0.741413\n",
      "iteration: 2  loss: 0.695376\n",
      "iteration: 3  loss: 0.693258\n",
      "iteration: 4  loss: 0.693147\n",
      "iteration: 5  loss: 0.693147\n",
      "iteration: 6  loss: 0.693147\n",
      "iteration: 7  loss: 0.693147\n",
      "iteration: 8  loss: 0.693147\n",
      "iteration: 9  loss: 0.693147\n",
      "iteration: 10  loss: 0.693147\n",
      "iteration: 11  loss: 0.693147\n",
      "iteration: 12  loss: 0.693147\n",
      "iteration: 13  loss: 0.693147\n",
      "iteration: 14  loss: 0.693147\n",
      "iteration: 15  loss: 0.693147\n",
      "iteration: 16  loss: 0.693147\n",
      "iteration: 17  loss: 0.693147\n",
      "iteration: 18  loss: 0.693147\n",
      "iteration: 19  loss: 0.693147\n",
      "Ranking time: 148.53\n",
      "Obtaining metrics time: 9.0\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 93 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 10\n",
      "NOLF:\t\t\t 100\n",
      "Sample Percentage:\t 0.05\n",
      "Creating 20 samples of length 51772\n",
      "iteration: 0  loss: 15.814906\n",
      "iteration: 1  loss: 9.94822\n",
      "iteration: 2  loss: 6.71555\n",
      "iteration: 3  loss: 4.648161\n",
      "iteration: 4  loss: 3.388777\n",
      "iteration: 5  loss: 2.524022\n",
      "iteration: 6  loss: 1.940025\n",
      "iteration: 7  loss: 1.540107\n",
      "iteration: 8  loss: 1.274077\n",
      "iteration: 9  loss: 1.084375\n",
      "iteration: 10  loss: 0.975795\n",
      "iteration: 11  loss: 0.89617\n",
      "iteration: 12  loss: 0.842914\n",
      "iteration: 13  loss: 0.799389\n",
      "iteration: 14  loss: 0.768668\n",
      "iteration: 15  loss: 0.746219\n",
      "iteration: 16  loss: 0.729144\n",
      "iteration: 17  loss: 0.725136\n",
      "iteration: 18  loss: 0.718215\n",
      "iteration: 19  loss: 0.707574\n",
      "Ranking time: 234.38\n",
      "Obtaining metrics time: 8.57\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 94 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 10\n",
      "NOLF:\t\t\t 100\n",
      "Sample Percentage:\t 0.1\n",
      "Creating 20 samples of length 103544\n",
      "iteration: 0  loss: 12.868198\n",
      "iteration: 1  loss: 5.690151\n",
      "iteration: 2  loss: 2.965004\n",
      "iteration: 3  loss: 1.732236\n",
      "iteration: 4  loss: 1.183032\n",
      "iteration: 5  loss: 0.930799\n",
      "iteration: 6  loss: 0.827209\n",
      "iteration: 7  loss: 0.753674\n",
      "iteration: 8  loss: 0.725971\n",
      "iteration: 9  loss: 0.712532\n",
      "iteration: 10  loss: 0.701447\n",
      "iteration: 11  loss: 0.700304\n",
      "iteration: 12  loss: 0.696153\n",
      "iteration: 13  loss: 0.695319\n",
      "iteration: 14  loss: 0.694115\n",
      "iteration: 15  loss: 0.693555\n",
      "iteration: 16  loss: 0.693558\n",
      "iteration: 17  loss: 0.693537\n",
      "iteration: 18  loss: 0.693234\n",
      "iteration: 19  loss: 0.693281\n",
      "Ranking time: 229.97\n",
      "Obtaining metrics time: 8.97\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 95 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 10\n",
      "NOLF:\t\t\t 100\n",
      "Sample Percentage:\t 0.2\n",
      "Creating 20 samples of length 207089\n",
      "iteration: 0  loss: 9.293548\n",
      "iteration: 1  loss: 2.332891\n",
      "iteration: 2  loss: 1.061407\n",
      "iteration: 3  loss: 0.788577\n",
      "iteration: 4  loss: 0.719754\n",
      "iteration: 5  loss: 0.700304\n",
      "iteration: 6  loss: 0.696143\n",
      "iteration: 7  loss: 0.69403\n",
      "iteration: 8  loss: 0.693482\n",
      "iteration: 9  loss: 0.693189\n",
      "iteration: 10  loss: 0.693157\n",
      "iteration: 11  loss: 0.693158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 12  loss: 0.693151\n",
      "iteration: 13  loss: 0.693147\n",
      "iteration: 14  loss: 0.693202\n",
      "iteration: 15  loss: 0.693156\n",
      "iteration: 16  loss: 0.693149\n",
      "iteration: 17  loss: 0.69315\n",
      "iteration: 18  loss: 0.693148\n",
      "iteration: 19  loss: 0.693148\n",
      "Ranking time: 231.01\n",
      "Obtaining metrics time: 11.02\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 96 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 20\n",
      "Regularisation:\t\t 10\n",
      "NOLF:\t\t\t 100\n",
      "Sample Percentage:\t 0.5\n",
      "Creating 20 samples of length 517722\n",
      "iteration: 0  loss: 4.885221\n",
      "iteration: 1  loss: 0.791355\n",
      "iteration: 2  loss: 0.697858\n",
      "iteration: 3  loss: 0.693372\n",
      "iteration: 4  loss: 0.693159\n",
      "iteration: 5  loss: 0.693172\n",
      "iteration: 6  loss: 0.693149\n",
      "iteration: 7  loss: 0.693147\n",
      "iteration: 8  loss: 0.693147\n",
      "iteration: 9  loss: 0.693147\n",
      "iteration: 10  loss: 0.693147\n",
      "iteration: 11  loss: 0.693147\n",
      "iteration: 12  loss: 0.693147\n",
      "iteration: 13  loss: 0.693147\n",
      "iteration: 14  loss: 0.693147\n",
      "iteration: 15  loss: 0.693147\n",
      "iteration: 16  loss: 0.693147\n",
      "iteration: 17  loss: 0.693147\n",
      "iteration: 18  loss: 0.693147\n",
      "iteration: 19  loss: 0.693147\n",
      "Ranking time: 233.29\n",
      "Obtaining metrics time: 8.73\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 97 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 40\n",
      "Regularisation:\t\t 0.01\n",
      "NOLF:\t\t\t 10\n",
      "Sample Percentage:\t 0.05\n",
      "Creating 40 samples of length 51772\n",
      "iteration: 0  loss: 0.695357\n",
      "iteration: 1  loss: 0.695015\n",
      "iteration: 2  loss: 0.694653\n",
      "iteration: 3  loss: 0.694236\n",
      "iteration: 4  loss: 0.694048\n",
      "iteration: 5  loss: 0.693593\n",
      "iteration: 6  loss: 0.693376\n",
      "iteration: 7  loss: 0.692583\n",
      "iteration: 8  loss: 0.692466\n",
      "iteration: 9  loss: 0.691839\n",
      "iteration: 10  loss: 0.691427\n",
      "iteration: 11  loss: 0.690634\n",
      "iteration: 12  loss: 0.689835\n",
      "iteration: 13  loss: 0.688898\n",
      "iteration: 14  loss: 0.6882\n",
      "iteration: 15  loss: 0.687093\n",
      "iteration: 16  loss: 0.686224\n",
      "iteration: 17  loss: 0.685395\n",
      "iteration: 18  loss: 0.683855\n",
      "iteration: 19  loss: 0.682169\n",
      "iteration: 20  loss: 0.681651\n",
      "iteration: 21  loss: 0.680118\n",
      "iteration: 22  loss: 0.679092\n",
      "iteration: 23  loss: 0.677992\n",
      "iteration: 24  loss: 0.676545\n",
      "iteration: 25  loss: 0.67503\n",
      "iteration: 26  loss: 0.673579\n",
      "iteration: 27  loss: 0.672002\n",
      "iteration: 28  loss: 0.67077\n",
      "iteration: 29  loss: 0.66915\n",
      "iteration: 30  loss: 0.667803\n",
      "iteration: 31  loss: 0.666088\n",
      "iteration: 32  loss: 0.665439\n",
      "iteration: 33  loss: 0.664123\n",
      "iteration: 34  loss: 0.662405\n",
      "iteration: 35  loss: 0.660297\n",
      "iteration: 36  loss: 0.658503\n",
      "iteration: 37  loss: 0.658116\n",
      "iteration: 38  loss: 0.65639\n",
      "iteration: 39  loss: 0.654322\n",
      "Ranking time: 93.55\n",
      "Obtaining metrics time: 9.03\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 98 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 40\n",
      "Regularisation:\t\t 0.01\n",
      "NOLF:\t\t\t 10\n",
      "Sample Percentage:\t 0.1\n",
      "Creating 40 samples of length 103544\n",
      "iteration: 0  loss: 0.695199\n",
      "iteration: 1  loss: 0.694508\n",
      "iteration: 2  loss: 0.693792\n",
      "iteration: 3  loss: 0.692899\n",
      "iteration: 4  loss: 0.692195\n",
      "iteration: 5  loss: 0.690959\n",
      "iteration: 6  loss: 0.689274\n",
      "iteration: 7  loss: 0.68746\n",
      "iteration: 8  loss: 0.685434\n",
      "iteration: 9  loss: 0.682733\n",
      "iteration: 10  loss: 0.680564\n",
      "iteration: 11  loss: 0.67796\n",
      "iteration: 12  loss: 0.675484\n",
      "iteration: 13  loss: 0.671925\n",
      "iteration: 14  loss: 0.669352\n",
      "iteration: 15  loss: 0.666035\n",
      "iteration: 16  loss: 0.663563\n",
      "iteration: 17  loss: 0.660329\n",
      "iteration: 18  loss: 0.656536\n",
      "iteration: 19  loss: 0.654034\n",
      "iteration: 20  loss: 0.650614\n",
      "iteration: 21  loss: 0.647676\n",
      "iteration: 22  loss: 0.645488\n",
      "iteration: 23  loss: 0.641813\n",
      "iteration: 24  loss: 0.638438\n",
      "iteration: 25  loss: 0.635756\n",
      "iteration: 26  loss: 0.632385\n",
      "iteration: 27  loss: 0.629198\n",
      "iteration: 28  loss: 0.626473\n",
      "iteration: 29  loss: 0.623965\n",
      "iteration: 30  loss: 0.62006\n",
      "iteration: 31  loss: 0.617652\n",
      "iteration: 32  loss: 0.614046\n",
      "iteration: 33  loss: 0.611462\n",
      "iteration: 34  loss: 0.607594\n",
      "iteration: 35  loss: 0.605612\n",
      "iteration: 36  loss: 0.601374\n",
      "iteration: 37  loss: 0.598819\n",
      "iteration: 38  loss: 0.596062\n",
      "iteration: 39  loss: 0.59333\n",
      "Ranking time: 94.13\n",
      "Obtaining metrics time: 8.68\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 99 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 40\n",
      "Regularisation:\t\t 0.01\n",
      "NOLF:\t\t\t 10\n",
      "Sample Percentage:\t 0.2\n",
      "Creating 40 samples of length 207089\n",
      "iteration: 0  loss: 0.694872\n",
      "iteration: 1  loss: 0.69334\n",
      "iteration: 2  loss: 0.69159\n",
      "iteration: 3  loss: 0.68839\n",
      "iteration: 4  loss: 0.684205\n",
      "iteration: 5  loss: 0.679168\n",
      "iteration: 6  loss: 0.673499\n",
      "iteration: 7  loss: 0.667402\n",
      "iteration: 8  loss: 0.661244\n",
      "iteration: 9  loss: 0.654586\n",
      "iteration: 10  loss: 0.648201\n",
      "iteration: 11  loss: 0.642531\n",
      "iteration: 12  loss: 0.635912\n",
      "iteration: 13  loss: 0.629488\n",
      "iteration: 14  loss: 0.62355\n",
      "iteration: 15  loss: 0.616527\n",
      "iteration: 16  loss: 0.610406\n",
      "iteration: 17  loss: 0.604192\n",
      "iteration: 18  loss: 0.596776\n",
      "iteration: 19  loss: 0.591441\n",
      "iteration: 20  loss: 0.583846\n",
      "iteration: 21  loss: 0.577663\n",
      "iteration: 22  loss: 0.570702\n",
      "iteration: 23  loss: 0.564355\n",
      "iteration: 24  loss: 0.557014\n",
      "iteration: 25  loss: 0.550571\n",
      "iteration: 26  loss: 0.543899\n",
      "iteration: 27  loss: 0.535921\n",
      "iteration: 28  loss: 0.529932\n",
      "iteration: 29  loss: 0.522763\n",
      "iteration: 30  loss: 0.515987\n",
      "iteration: 31  loss: 0.509298\n",
      "iteration: 32  loss: 0.50305\n",
      "iteration: 33  loss: 0.496089\n",
      "iteration: 34  loss: 0.488869\n",
      "iteration: 35  loss: 0.481825\n",
      "iteration: 36  loss: 0.476088\n",
      "iteration: 37  loss: 0.469983\n",
      "iteration: 38  loss: 0.461556\n",
      "iteration: 39  loss: 0.456741\n",
      "Ranking time: 95.41\n",
      "Obtaining metrics time: 8.92\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 100 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 40\n",
      "Regularisation:\t\t 0.01\n",
      "NOLF:\t\t\t 10\n",
      "Sample Percentage:\t 0.5\n",
      "Creating 40 samples of length 517722\n",
      "iteration: 0  loss: 0.69372\n",
      "iteration: 1  loss: 0.687234\n",
      "iteration: 2  loss: 0.674582\n",
      "iteration: 3  loss: 0.659164\n",
      "iteration: 4  loss: 0.643073\n",
      "iteration: 5  loss: 0.626987\n",
      "iteration: 6  loss: 0.610393\n",
      "iteration: 7  loss: 0.594068\n",
      "iteration: 8  loss: 0.576433\n",
      "iteration: 9  loss: 0.559495\n",
      "iteration: 10  loss: 0.5415\n",
      "iteration: 11  loss: 0.523494\n",
      "iteration: 12  loss: 0.506039\n",
      "iteration: 13  loss: 0.488294\n",
      "iteration: 14  loss: 0.471108\n",
      "iteration: 15  loss: 0.453942\n",
      "iteration: 16  loss: 0.437819\n",
      "iteration: 17  loss: 0.421813\n",
      "iteration: 18  loss: 0.407194\n",
      "iteration: 19  loss: 0.393681\n",
      "iteration: 20  loss: 0.38006\n",
      "iteration: 21  loss: 0.368024\n",
      "iteration: 22  loss: 0.356611\n",
      "iteration: 23  loss: 0.345748\n",
      "iteration: 24  loss: 0.335714\n",
      "iteration: 25  loss: 0.326552\n",
      "iteration: 26  loss: 0.316955\n",
      "iteration: 27  loss: 0.309333\n",
      "iteration: 28  loss: 0.301731\n",
      "iteration: 29  loss: 0.29505\n",
      "iteration: 30  loss: 0.288554\n",
      "iteration: 31  loss: 0.28266\n",
      "iteration: 32  loss: 0.277054\n",
      "iteration: 33  loss: 0.271689\n",
      "iteration: 34  loss: 0.267038\n",
      "iteration: 35  loss: 0.262769\n",
      "iteration: 36  loss: 0.259055\n",
      "iteration: 37  loss: 0.255198\n",
      "iteration: 38  loss: 0.251968\n",
      "iteration: 39  loss: 0.248899\n",
      "Ranking time: 94.56\n",
      "Obtaining metrics time: 8.68\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 101 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 40\n",
      "Regularisation:\t\t 0.01\n",
      "NOLF:\t\t\t 20\n",
      "Sample Percentage:\t 0.05\n",
      "Creating 40 samples of length 51772\n",
      "iteration: 0  loss: 0.697147\n",
      "iteration: 1  loss: 0.696787\n",
      "iteration: 2  loss: 0.696173\n",
      "iteration: 3  loss: 0.69542\n",
      "iteration: 4  loss: 0.694948\n",
      "iteration: 5  loss: 0.693897\n",
      "iteration: 6  loss: 0.693223\n",
      "iteration: 7  loss: 0.6922\n",
      "iteration: 8  loss: 0.691275\n",
      "iteration: 9  loss: 0.690067\n",
      "iteration: 10  loss: 0.688605\n",
      "iteration: 11  loss: 0.687895\n",
      "iteration: 12  loss: 0.686864\n",
      "iteration: 13  loss: 0.685164\n",
      "iteration: 14  loss: 0.684038\n",
      "iteration: 15  loss: 0.682328\n",
      "iteration: 16  loss: 0.68074\n",
      "iteration: 17  loss: 0.679349\n",
      "iteration: 18  loss: 0.677624\n",
      "iteration: 19  loss: 0.675963\n",
      "iteration: 20  loss: 0.674132\n",
      "iteration: 21  loss: 0.67259\n",
      "iteration: 22  loss: 0.670704\n",
      "iteration: 23  loss: 0.669235\n",
      "iteration: 24  loss: 0.667411\n",
      "iteration: 25  loss: 0.665151\n",
      "iteration: 26  loss: 0.662812\n",
      "iteration: 27  loss: 0.661989\n",
      "iteration: 28  loss: 0.659298\n",
      "iteration: 29  loss: 0.657342\n",
      "iteration: 30  loss: 0.655802\n",
      "iteration: 31  loss: 0.654503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 32  loss: 0.651302\n",
      "iteration: 33  loss: 0.64976\n",
      "iteration: 34  loss: 0.647641\n",
      "iteration: 35  loss: 0.646673\n",
      "iteration: 36  loss: 0.643605\n",
      "iteration: 37  loss: 0.64225\n",
      "iteration: 38  loss: 0.640817\n",
      "iteration: 39  loss: 0.637995\n",
      "Ranking time: 106.08\n",
      "Obtaining metrics time: 8.88\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 102 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 40\n",
      "Regularisation:\t\t 0.01\n",
      "NOLF:\t\t\t 20\n",
      "Sample Percentage:\t 0.1\n",
      "Creating 40 samples of length 103544\n",
      "iteration: 0  loss: 0.697035\n",
      "iteration: 1  loss: 0.695631\n",
      "iteration: 2  loss: 0.694451\n",
      "iteration: 3  loss: 0.692643\n",
      "iteration: 4  loss: 0.690765\n",
      "iteration: 5  loss: 0.688173\n",
      "iteration: 6  loss: 0.685794\n",
      "iteration: 7  loss: 0.682879\n",
      "iteration: 8  loss: 0.679557\n",
      "iteration: 9  loss: 0.676558\n",
      "iteration: 10  loss: 0.672992\n",
      "iteration: 11  loss: 0.669473\n",
      "iteration: 12  loss: 0.665425\n",
      "iteration: 13  loss: 0.661216\n",
      "iteration: 14  loss: 0.657211\n",
      "iteration: 15  loss: 0.653765\n",
      "iteration: 16  loss: 0.648963\n",
      "iteration: 17  loss: 0.645455\n",
      "iteration: 18  loss: 0.641806\n",
      "iteration: 19  loss: 0.637508\n",
      "iteration: 20  loss: 0.633984\n",
      "iteration: 21  loss: 0.629807\n",
      "iteration: 22  loss: 0.62588\n",
      "iteration: 23  loss: 0.622294\n",
      "iteration: 24  loss: 0.61862\n",
      "iteration: 25  loss: 0.614284\n",
      "iteration: 26  loss: 0.610041\n",
      "iteration: 27  loss: 0.606568\n",
      "iteration: 28  loss: 0.602824\n",
      "iteration: 29  loss: 0.597698\n",
      "iteration: 30  loss: 0.594421\n",
      "iteration: 31  loss: 0.590523\n",
      "iteration: 32  loss: 0.58626\n",
      "iteration: 33  loss: 0.582905\n",
      "iteration: 34  loss: 0.577683\n",
      "iteration: 35  loss: 0.573211\n",
      "iteration: 36  loss: 0.570595\n",
      "iteration: 37  loss: 0.567563\n",
      "iteration: 38  loss: 0.562354\n",
      "iteration: 39  loss: 0.557548\n",
      "Ranking time: 110.87\n",
      "Obtaining metrics time: 8.91\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 103 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 40\n",
      "Regularisation:\t\t 0.01\n",
      "NOLF:\t\t\t 20\n",
      "Sample Percentage:\t 0.2\n",
      "Creating 40 samples of length 207089\n",
      "iteration: 0  loss: 0.69638\n",
      "iteration: 1  loss: 0.693618\n",
      "iteration: 2  loss: 0.689581\n",
      "iteration: 3  loss: 0.684563\n",
      "iteration: 4  loss: 0.677949\n",
      "iteration: 5  loss: 0.670998\n",
      "iteration: 6  loss: 0.663278\n",
      "iteration: 7  loss: 0.654931\n",
      "iteration: 8  loss: 0.646566\n",
      "iteration: 9  loss: 0.638673\n",
      "iteration: 10  loss: 0.630626\n",
      "iteration: 11  loss: 0.622825\n",
      "iteration: 12  loss: 0.614857\n",
      "iteration: 13  loss: 0.606173\n",
      "iteration: 14  loss: 0.598025\n",
      "iteration: 15  loss: 0.589709\n",
      "iteration: 16  loss: 0.581611\n",
      "iteration: 17  loss: 0.572144\n",
      "iteration: 18  loss: 0.565863\n",
      "iteration: 19  loss: 0.555859\n",
      "iteration: 20  loss: 0.548212\n",
      "iteration: 21  loss: 0.540508\n",
      "iteration: 22  loss: 0.531031\n",
      "iteration: 23  loss: 0.523186\n",
      "iteration: 24  loss: 0.515592\n",
      "iteration: 25  loss: 0.506592\n",
      "iteration: 26  loss: 0.499073\n",
      "iteration: 27  loss: 0.490774\n",
      "iteration: 28  loss: 0.483051\n",
      "iteration: 29  loss: 0.474559\n",
      "iteration: 30  loss: 0.467334\n",
      "iteration: 31  loss: 0.459963\n",
      "iteration: 32  loss: 0.451434\n",
      "iteration: 33  loss: 0.44552\n",
      "iteration: 34  loss: 0.437972\n",
      "iteration: 35  loss: 0.430482\n",
      "iteration: 36  loss: 0.424242\n",
      "iteration: 37  loss: 0.417042\n",
      "iteration: 38  loss: 0.410067\n",
      "iteration: 39  loss: 0.403376\n",
      "Ranking time: 111.14\n",
      "Obtaining metrics time: 9.13\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 104 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 40\n",
      "Regularisation:\t\t 0.01\n",
      "NOLF:\t\t\t 20\n",
      "Sample Percentage:\t 0.5\n",
      "Creating 40 samples of length 517722\n",
      "iteration: 0  loss: 0.694154\n",
      "iteration: 1  loss: 0.682342\n",
      "iteration: 2  loss: 0.664669\n",
      "iteration: 3  loss: 0.644177\n",
      "iteration: 4  loss: 0.623913\n",
      "iteration: 5  loss: 0.603146\n",
      "iteration: 6  loss: 0.581819\n",
      "iteration: 7  loss: 0.5604\n",
      "iteration: 8  loss: 0.538933\n",
      "iteration: 9  loss: 0.51781\n",
      "iteration: 10  loss: 0.496034\n",
      "iteration: 11  loss: 0.475886\n",
      "iteration: 12  loss: 0.455954\n",
      "iteration: 13  loss: 0.437021\n",
      "iteration: 14  loss: 0.418326\n",
      "iteration: 15  loss: 0.400811\n",
      "iteration: 16  loss: 0.38445\n",
      "iteration: 17  loss: 0.369027\n",
      "iteration: 18  loss: 0.354584\n",
      "iteration: 19  loss: 0.340427\n",
      "iteration: 20  loss: 0.328907\n",
      "iteration: 21  loss: 0.317231\n",
      "iteration: 22  loss: 0.306418\n",
      "iteration: 23  loss: 0.296681\n",
      "iteration: 24  loss: 0.287394\n",
      "iteration: 25  loss: 0.279515\n",
      "iteration: 26  loss: 0.27163\n",
      "iteration: 27  loss: 0.264931\n",
      "iteration: 28  loss: 0.258294\n",
      "iteration: 29  loss: 0.252773\n",
      "iteration: 30  loss: 0.247206\n",
      "iteration: 31  loss: 0.242223\n",
      "iteration: 32  loss: 0.23791\n",
      "iteration: 33  loss: 0.233809\n",
      "iteration: 34  loss: 0.230218\n",
      "iteration: 35  loss: 0.226496\n",
      "iteration: 36  loss: 0.222691\n",
      "iteration: 37  loss: 0.21981\n",
      "iteration: 38  loss: 0.217135\n",
      "iteration: 39  loss: 0.214808\n",
      "Ranking time: 109.02\n",
      "Obtaining metrics time: 8.82\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 105 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 40\n",
      "Regularisation:\t\t 0.01\n",
      "NOLF:\t\t\t 50\n",
      "Sample Percentage:\t 0.05\n",
      "Creating 40 samples of length 51772\n",
      "iteration: 0  loss: 0.704172\n",
      "iteration: 1  loss: 0.70301\n",
      "iteration: 2  loss: 0.70084\n",
      "iteration: 3  loss: 0.69864\n",
      "iteration: 4  loss: 0.697093\n",
      "iteration: 5  loss: 0.695745\n",
      "iteration: 6  loss: 0.693252\n",
      "iteration: 7  loss: 0.691235\n",
      "iteration: 8  loss: 0.688781\n",
      "iteration: 9  loss: 0.686487\n",
      "iteration: 10  loss: 0.684745\n",
      "iteration: 11  loss: 0.682327\n",
      "iteration: 12  loss: 0.679834\n",
      "iteration: 13  loss: 0.677426\n",
      "iteration: 14  loss: 0.673884\n",
      "iteration: 15  loss: 0.671283\n",
      "iteration: 16  loss: 0.669529\n",
      "iteration: 17  loss: 0.665701\n",
      "iteration: 18  loss: 0.663635\n",
      "iteration: 19  loss: 0.660108\n",
      "iteration: 20  loss: 0.658175\n",
      "iteration: 21  loss: 0.654773\n",
      "iteration: 22  loss: 0.652503\n",
      "iteration: 23  loss: 0.648996\n",
      "iteration: 24  loss: 0.645838\n",
      "iteration: 25  loss: 0.642315\n",
      "iteration: 26  loss: 0.640234\n",
      "iteration: 27  loss: 0.637529\n",
      "iteration: 28  loss: 0.634393\n",
      "iteration: 29  loss: 0.631394\n",
      "iteration: 30  loss: 0.628657\n",
      "iteration: 31  loss: 0.626005\n",
      "iteration: 32  loss: 0.62265\n",
      "iteration: 33  loss: 0.619791\n",
      "iteration: 34  loss: 0.617339\n",
      "iteration: 35  loss: 0.614557\n",
      "iteration: 36  loss: 0.612012\n",
      "iteration: 37  loss: 0.609586\n",
      "iteration: 38  loss: 0.605553\n",
      "iteration: 39  loss: 0.6037\n",
      "Ranking time: 151.4\n",
      "Obtaining metrics time: 8.95\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 106 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 40\n",
      "Regularisation:\t\t 0.01\n",
      "NOLF:\t\t\t 50\n",
      "Sample Percentage:\t 0.1\n",
      "Creating 40 samples of length 103544\n",
      "iteration: 0  loss: 0.703538\n",
      "iteration: 1  loss: 0.699722\n",
      "iteration: 2  loss: 0.696157\n",
      "iteration: 3  loss: 0.69216\n",
      "iteration: 4  loss: 0.687727\n",
      "iteration: 5  loss: 0.683525\n",
      "iteration: 6  loss: 0.677872\n",
      "iteration: 7  loss: 0.672131\n",
      "iteration: 8  loss: 0.667157\n",
      "iteration: 9  loss: 0.661052\n",
      "iteration: 10  loss: 0.65516\n",
      "iteration: 11  loss: 0.649659\n",
      "iteration: 12  loss: 0.642996\n",
      "iteration: 13  loss: 0.637295\n",
      "iteration: 14  loss: 0.631407\n",
      "iteration: 15  loss: 0.625611\n",
      "iteration: 16  loss: 0.619272\n",
      "iteration: 17  loss: 0.613115\n",
      "iteration: 18  loss: 0.607823\n",
      "iteration: 19  loss: 0.602302\n",
      "iteration: 20  loss: 0.596507\n",
      "iteration: 21  loss: 0.590681\n",
      "iteration: 22  loss: 0.584701\n",
      "iteration: 23  loss: 0.579135\n",
      "iteration: 24  loss: 0.574245\n",
      "iteration: 25  loss: 0.568881\n",
      "iteration: 26  loss: 0.563237\n",
      "iteration: 27  loss: 0.559133\n",
      "iteration: 28  loss: 0.552745\n",
      "iteration: 29  loss: 0.547216\n",
      "iteration: 30  loss: 0.541782\n",
      "iteration: 31  loss: 0.537835\n",
      "iteration: 32  loss: 0.53156\n",
      "iteration: 33  loss: 0.525941\n",
      "iteration: 34  loss: 0.521281\n",
      "iteration: 35  loss: 0.516362\n",
      "iteration: 36  loss: 0.511968\n",
      "iteration: 37  loss: 0.506512\n",
      "iteration: 38  loss: 0.502071\n",
      "iteration: 39  loss: 0.497278\n",
      "Ranking time: 152.75\n",
      "Obtaining metrics time: 8.82\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 107 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 40\n",
      "Regularisation:\t\t 0.01\n",
      "NOLF:\t\t\t 50\n",
      "Sample Percentage:\t 0.2\n",
      "Creating 40 samples of length 207089\n",
      "iteration: 0  loss: 0.701568\n",
      "iteration: 1  loss: 0.694256\n",
      "iteration: 2  loss: 0.685306\n",
      "iteration: 3  loss: 0.675107\n",
      "iteration: 4  loss: 0.663921\n",
      "iteration: 5  loss: 0.651968\n",
      "iteration: 6  loss: 0.639569\n",
      "iteration: 7  loss: 0.627536\n",
      "iteration: 8  loss: 0.615227\n",
      "iteration: 9  loss: 0.603448\n",
      "iteration: 10  loss: 0.591577\n",
      "iteration: 11  loss: 0.580139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 12  loss: 0.568797\n",
      "iteration: 13  loss: 0.558195\n",
      "iteration: 14  loss: 0.547281\n",
      "iteration: 15  loss: 0.536264\n",
      "iteration: 16  loss: 0.524631\n",
      "iteration: 17  loss: 0.514253\n",
      "iteration: 18  loss: 0.504765\n",
      "iteration: 19  loss: 0.495129\n",
      "iteration: 20  loss: 0.484167\n",
      "iteration: 21  loss: 0.473783\n",
      "iteration: 22  loss: 0.465144\n",
      "iteration: 23  loss: 0.455303\n",
      "iteration: 24  loss: 0.446623\n",
      "iteration: 25  loss: 0.437238\n",
      "iteration: 26  loss: 0.429068\n",
      "iteration: 27  loss: 0.419824\n",
      "iteration: 28  loss: 0.411395\n",
      "iteration: 29  loss: 0.404001\n",
      "iteration: 30  loss: 0.3959\n",
      "iteration: 31  loss: 0.388888\n",
      "iteration: 32  loss: 0.380973\n",
      "iteration: 33  loss: 0.373543\n",
      "iteration: 34  loss: 0.36751\n",
      "iteration: 35  loss: 0.360919\n",
      "iteration: 36  loss: 0.353837\n",
      "iteration: 37  loss: 0.347459\n",
      "iteration: 38  loss: 0.342407\n",
      "iteration: 39  loss: 0.335588\n",
      "Ranking time: 152.17\n",
      "Obtaining metrics time: 8.69\n",
      "results added\n",
      "\n",
      " ============================================================ \n",
      " Run: 108 / 288\n",
      "File:\t\t\t Amazon_01_users\n",
      "Iterations:\t\t 40\n",
      "Regularisation:\t\t 0.01\n",
      "NOLF:\t\t\t 50\n",
      "Sample Percentage:\t 0.5\n",
      "Creating 40 samples of length 517722\n",
      "iteration: 0  loss: 0.695917\n",
      "iteration: 1  loss: 0.672214\n",
      "iteration: 2  loss: 0.642506\n",
      "iteration: 3  loss: 0.611966\n",
      "iteration: 4  loss: 0.58204\n",
      "iteration: 5  loss: 0.55386\n",
      "iteration: 6  loss: 0.525885\n",
      "iteration: 7  loss: 0.499345\n",
      "iteration: 8  loss: 0.473468\n",
      "iteration: 9  loss: 0.449055\n",
      "iteration: 10  loss: 0.426036\n",
      "iteration: 11  loss: 0.404309\n",
      "iteration: 12  loss: 0.384816\n",
      "iteration: 13  loss: 0.366057\n",
      "iteration: 14  loss: 0.349167\n",
      "iteration: 15  loss: 0.333624\n",
      "iteration: 16  loss: 0.319404\n",
      "iteration: 17  loss: 0.306511\n",
      "iteration: 18  loss: 0.29481\n",
      "iteration: 19  loss: 0.28423\n",
      "iteration: 20  loss: 0.274816\n",
      "iteration: 21  loss: 0.26628\n",
      "iteration: 22  loss: 0.258018\n",
      "iteration: 23  loss: 0.251327\n",
      "iteration: 24  loss: 0.244638\n",
      "iteration: 25  loss: 0.238119\n",
      "iteration: 26  loss: 0.233197\n",
      "iteration: 27  loss: 0.227885\n",
      "iteration: 28  loss: 0.223895\n",
      "iteration: 29  loss: 0.219498\n",
      "iteration: 30  loss: 0.215852\n",
      "iteration: 31  loss: 0.212258\n",
      "iteration: 32  loss: 0.209315\n",
      "iteration: 33  loss: 0.206252\n",
      "iteration: 34  loss: 0.203862\n",
      "iteration: 35  loss: 0.201274\n",
      "iteration: 36  loss: 0.199228\n",
      "iteration: 37  loss: 0.197111\n",
      "iteration: 38  loss: 0.194746\n",
      "iteration: 39  loss: 0.193036\n",
      "Ranking time: 152.15\n",
      "Obtaining metrics time: 8.82\n",
      "results added\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-113-6d6273dbaf31>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m                     \u001b[0mres_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'BPR_models'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m                     \u001b[0mstore_results\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m                     \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'Results/BPR/Grid_Search/metrics_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfinal_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-92-cb75110d71b8>\u001b[0m in \u001b[0;36mstore_results\u001b[1;34m(results, log_path, res_name, file_name, params)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mdf_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_results\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinal_log\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mdf_results\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_path\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mres_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\re_research\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_pickle\u001b[1;34m(self, path, compression, protocol)\u001b[0m\n\u001b[0;32m   2723\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpickle\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mto_pickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2724\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2725\u001b[1;33m         \u001b[0mto_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2726\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2727\u001b[0m     def to_clipboard(\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\re_research\\lib\\site-packages\\pandas\\io\\pickle.py\u001b[0m in \u001b[0;36mto_pickle\u001b[1;34m(obj, filepath_or_buffer, compression, protocol)\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHIGHEST_PROTOCOL\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m         \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "file_names = ['Amazon_01_users', 'ML_01_users']\n",
    "file_paths = [path + 'Data/Amazon/', path + 'Data/ML/']\n",
    "ext = 'Am'\n",
    "\n",
    "n_iterations = [40, 80]\n",
    "regs = [0.01, 0.1, 0.5, 1, 1.5, 10]\n",
    "nolfs = [10, 20, 50, 100]\n",
    "sample_percs = [0.05, 0.1, 0.2, 0.5]\n",
    "\n",
    "runs = 0\n",
    "all_runs = len(n_iterations) * len(regs) * len(nolfs) * len(sample_percs)\n",
    "\n",
    "for file_name, file_path in zip(file_names, file_paths):\n",
    "    if file_name == 'ML_01_users':\n",
    "        ext = 'ML'\n",
    "    for n_iters in n_iterations:\n",
    "        params['n_iterations'] = n_iters\n",
    "        for reg in regs:\n",
    "            params['reg_user'] = reg\n",
    "            params['reg_item'] = reg\n",
    "            for nolf in nolfs:\n",
    "                params['nolf'] = nolf\n",
    "                for sample_perc in sample_percs:\n",
    "                    runs += 1\n",
    "                    print('\\n', '='*60, '\\n', 'Run:', runs, '/', all_runs)\n",
    "                    print('File:\\t\\t\\t', file_name)\n",
    "                    print('Iterations:\\t\\t', n_iters)\n",
    "                    print('Regularisation:\\t\\t', reg)\n",
    "                    print('NOLF:\\t\\t\\t', nolf)\n",
    "                    print('Sample Percentage:\\t', sample_perc)\n",
    "                    # matrics_n_iter_reg_nolf_sample_prec\n",
    "                    final_name = ext + '_' + str(n_iters) + '_' + str(nolf) + '_' + str(reg) + '_' + str(sample_perc)\n",
    "                    \n",
    "                    model, metrics = full_run(file_name, file_path, params, sample_perc)\n",
    "                    \n",
    "                    log_path = path + 'Results/BPR/Grid_Search/'\n",
    "                    res_name = 'BPR_models'\n",
    "                    \n",
    "                    store_results(model.model, log_path, res_name, final_name, params)\n",
    "                    metrics.to_pickle(path + 'Results/BPR/Grid_Search/metrics_' + final_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 005 sample_prec = 0.2\n",
    "# Results\n",
    "#    rank_at  hitcounts    recall  precision\n",
    "# 0        1         45  0.007794   0.007794\n",
    "# 1        5        265  0.045895   0.009179\n",
    "# 2       10        339  0.058711   0.005871\n",
    "# 3       15        364  0.063041   0.004203\n",
    "# 4       20        392  0.067891   0.003395\n",
    "\n",
    "\n",
    "# 001 sample_prec = 0.5\n",
    "# Results\n",
    "#    rank_at  hitcounts    recall  precision\n",
    "# 0        1         11  0.009532   0.009532\n",
    "# 1        5         53  0.045927   0.009185\n",
    "# 2       10         70  0.060659   0.006066\n",
    "# 3       15         75  0.064991   0.004333\n",
    "# 4       20         76  0.065858   0.003293"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_pickle('Data/amazon_clothing_shoes_jewelry_data')\n",
    "# users = df.user.unique()\n",
    "# to_keep = users[:300000]\n",
    "\n",
    "# user_indices = df.groupby('user')['index'].apply(list)\n",
    "# to_keep_indices = []\n",
    "# for u in user_indices[to_keep]:\n",
    "#     to_keep_indices.extend(u)\n",
    "\n",
    "# new_df = df_og.loc[to_keep_indices]\n",
    "# len(to_keep_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## standard SVD model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVD:\n",
    "    \"\"\"\" All functions used to run, test, plot and store the\n",
    "    Singular Value Decomposition Model\"\"\"\n",
    "\n",
    "    def __init__(self, params, total_users, total_items):\n",
    "        self.nolf = params['nolf']\n",
    "        self.n_epochs = params['n_epochs']\n",
    "        self.alpha = params['alpha']\n",
    "        self.alpha_b = params['alpha_b']\n",
    "        self.alpha_cb = params['alpha_cb']\n",
    "        self.use_bias = params['use_bias']\n",
    "        self.use_impl_fb = params['use_impl_fb']\n",
    "        self.use_color = params['use_color']\n",
    "        self.use_weight_ver = params['use_weight_ver']\n",
    "        self.bu_reg = params['bu_reg']\n",
    "        self.bi_reg = params['bi_reg']\n",
    "        self.pu_reg = params['pu_reg']\n",
    "        self.qi_reg = params['qi_reg']\n",
    "        self.x_reg = params['x_reg']\n",
    "        self.cb_reg = params['cb_reg']\n",
    "        self.ver_weight = params['ver_weight']\n",
    "        self.stop = params['stop']\n",
    "        self.random_state = params['random_state']\n",
    "        self.total_users = total_users\n",
    "        self.total_items = total_items\n",
    "        self.params = params\n",
    "        self.mu = 0 \n",
    "        self.N = []\n",
    "        self.N_test = []\n",
    "        self.t = pd.DataFrame()\n",
    "        self.c = pd.DataFrame()\n",
    "        self.F = pd.DataFrame()\n",
    "\n",
    "        self.train_data = pd.DataFrame()\n",
    "        self.test_data = pd.DataFrame()\n",
    "        self.val_data = pd.DataFrame()\n",
    "        self.train_time = 0\n",
    "        self.best_model = {}\n",
    "        self.model = {}\n",
    "        self.test_results = {}\n",
    "\n",
    "    def fit(self, train_data, val_data=[], verbose=1, plot=True, plot_name=''):\n",
    "        self.train_data = train_data\n",
    "        self.val_data = val_data\n",
    "        self.SVD(train_data=train_data, val_data=val_data, verbose=verbose, plot=plot, plot_name=plot_name)\n",
    "        return self\n",
    "\n",
    "    \n",
    "###############################################################################################\n",
    "    \n",
    "    def SVD(self, train_data, val_data, verbose, plot, plot_name):\n",
    "        \"\"\"\"The SVD algorithm with sgd\n",
    "        input: rating dataset with columns:['rating', 'user_id', 'item_id']\n",
    "        output: the resulting p, q, bi, bu matrices\"\"\"\n",
    "        self.mu = self.create_mu(train_data)\n",
    "        train_matrix = self.create_matrix(train_data, self.total_users, self.total_items)\n",
    "        \n",
    "        tuples_train = [tuple(x) for x in train_data[['new_user_id', 'new_item_id', 'rating']].to_numpy()]\n",
    "        \n",
    "        p = np.random.normal(0, .1, (total_users, self.nolf))  # users\n",
    "        q = np.random.normal(0, .1, (total_items, self.nolf))  # items\n",
    "        \n",
    "        # user and item biases\n",
    "        b_user = np.zeros(total_users)\n",
    "        b_item = np.zeros(total_items)\n",
    "        \n",
    "        # using color (pareto split (0,1,2)) attribute bias\n",
    "        if self.use_color:\n",
    "            print('Creating F and c, for incorporating color bias')\n",
    "            self.F, self.c = self.init_color(train_data)\n",
    "\n",
    "        # implicit fb rated, not rated\n",
    "        x = np.random.normal(0, .1, (total_items, self.nolf))\n",
    "        impl_fb_u = np.zeros(self.nolf)\n",
    "        if self.use_impl_fb:\n",
    "            print('Creating N, for incorporating implicit feedback')\n",
    "            self.N = train_data.groupby('new_user_id')['new_item_id'].apply(list)\n",
    "        \n",
    "        # 0.5 weight on the errors of verified = False user item combinations\n",
    "        if self.use_weight_ver:\n",
    "            i_verified = train_data.set_index(['new_user_id', 'new_item_id'])['verified']\n",
    "            i_verified = i_verified.loc[~i_verified.index.duplicated(keep='first')]\n",
    "        \n",
    "        sqrt_Nu = 0\n",
    "        cb = 0\n",
    "        rmses = []\n",
    "        val_rmses = []\n",
    "        smallest_val_rmse = 10000\n",
    "        val_rmse = \"na\"\n",
    "        start = time.time()\n",
    "        for epoch in range(self.n_epochs):\n",
    "            total_sq_error = 0\n",
    "            for u, i, r_ui in tuples_train:\n",
    "                u = int(u)\n",
    "                i = int(i)\n",
    "                \n",
    "                if self.use_impl_fb:\n",
    "                    impl_fb_u = np.zeros(self.nolf)\n",
    "                    sqrt_Nu = np.sqrt(len(self.N[u]))\n",
    "                    for j in self.N[u]:\n",
    "                        impl_fb_u += x[j] / sqrt_Nu\n",
    "\n",
    "                if self.use_color and epoch > 5:\n",
    "                    F_ui =  self.F[u,i] #Set of items associated with i and rated by u\n",
    "                    u_mu = self.mu + b_user[u]\n",
    "                    sqrt_F_ui = np.sqrt(len(F_ui))\n",
    "                    for index, f in enumerate(F_ui):\n",
    "                        r_uf = train_data[(train_data['new_user_id']==u) & (train_data['new_item_id']==f)]['rating'].iloc[0]\n",
    "                        cb += (r_uf - u_mu) * self.c[u,i][index]\n",
    "                    cb /=  sqrt_F_ui\n",
    "                        \n",
    "                if self.use_bias:   \n",
    "                    error = r_ui - ((self.mu + b_user[u] + b_item[i] + cb) + np.dot(p[u] + impl_fb_u, q[i]))\n",
    "                    if self.use_weight_ver and not i_verified[u,i]:\n",
    "                        error = self.ver_weight * error\n",
    "                    \n",
    "                    b_user[u] += self.alpha_b * (error - self.bu_reg * b_user[u])\n",
    "                    b_item[i] += self.alpha_b * (error - self.bi_reg * b_item[i])\n",
    "                else:\n",
    "                    error = r_ui - np.dot(p[u], q[i])\n",
    "\n",
    "                p[u] += self.alpha * (error * q[i] - self.pu_reg * p[u])\n",
    "                q[i] += self.alpha * (error * (p[u] + impl_fb_u) - self.qi_reg * q[i])\n",
    "                total_sq_error += np.square(error)\n",
    "            \n",
    "                if self.use_impl_fb:\n",
    "                    for j in self.N[u]:\n",
    "                        x[j] += self.alpha * (error * q[i] / sqrt_Nu - self.x_reg * x[j])\n",
    "                \n",
    "                if self.use_color and epoch > 5:\n",
    "                    for index, f in enumerate(F_ui):\n",
    "                        r_uf = train_data[(train_data['new_user_id']==u) & (train_data['new_item_id']==f)]['rating'].iloc[0]\n",
    "                        u_mu = self.mu + b_user[u]\n",
    "                        self.c[u,i][index] += self.alpha_cb * (error * (1/sqrt_F_ui) * (r_uf - u_mu) - self.cb_reg * self.c[u,i][index])\n",
    "                \n",
    "            rmse = np.sqrt(total_sq_error / len(tuples_train))\n",
    "            rmses.append(rmse)\n",
    "            \n",
    "            self.model = {'p': p, 'q': q, 'bu':b_user, 'bi':b_item, 'cbu': self.c, 'x':x, 'rmse':rmses, 'val_rmse':val_rmses}\n",
    "            \n",
    "            # Validation\n",
    "            if len(val_data) > 0:\n",
    "                new_val_rmse = self.test(val_data, val=True)\n",
    "                val_rmses.append(new_val_rmse)\n",
    "                if new_val_rmse < smallest_val_rmse:\n",
    "                    smallest_val_rmse = new_val_rmse\n",
    "                    self.best_model = copy.deepcopy(self.model)\n",
    "                val_rmse = new_val_rmse\n",
    "                \n",
    "            # Epoch Printing\n",
    "            if epoch % verbose == 0:\n",
    "                if len(val_data) > 0:\n",
    "                    print('Epoch:', epoch, '  RMSE:', rmse, ' Val_RMSE:', val_rmse)\n",
    "                else:\n",
    "                    print('Epoch:', epoch, '  RMSE:', rmse)\n",
    "            \n",
    "            if self.stop and val_rmses[-2:][0] < val_rmse:\n",
    "                print('BREAK: Validation set not improving anymore')\n",
    "                break\n",
    "                \n",
    "        if plot:\n",
    "            self.plot_rmse(rmses, val_rmses, plot_name)\n",
    "\n",
    "        self.train_time = time.time() - start\n",
    "        self.model = {'p': p, 'q': q, 'bu':b_user, 'bi':b_item, 'cbu': self.c, 'x':x, 'rmse':rmses, 'val_rmse':val_rmses}\n",
    "#################################################################################################\n",
    "\n",
    "    def init_color(self, data_set):\n",
    "        self.t = data_set.groupby(['new_user_id', 'par_col2'])['new_item_id'].apply(list)\n",
    "        F = data_set.groupby(['new_user_id', 'new_item_id'])['par_col2'].apply(self.sim_items)\n",
    "        c = data_set.groupby(['new_user_id', 'new_item_id'])['par_col2'].apply(self.sim_items, random=True)\n",
    "        return F, c\n",
    "\n",
    "    def sim_items(self, x, random=False):\n",
    "        u_id = x.name[0]\n",
    "        col = x.iloc[0]\n",
    "        if random:\n",
    "            return np.random.normal(0,.1,len(self.t[u_id, col]))\n",
    "        return self.t[u_id, col]\n",
    "    \n",
    "    def create_matrix(self, X_train, n_users, n_items):\n",
    "        r = X_train['new_user_id']\n",
    "        c = X_train['new_item_id']\n",
    "        d = X_train['rating']\n",
    "        train_matrix = sparse.coo_matrix((d, (r, c)), shape=(n_users, n_items))\n",
    "    \n",
    "        return train_matrix.tocsr()\n",
    "    \n",
    "    def create_mu(self, train_set):\n",
    "        # Better mean calculation according to https://sifter.org/~simon/journal/20061211.html\n",
    "        va = train_set.groupby('new_user_id')['rating'].mean().var() #variance mean ratings users\n",
    "        vb = train_set.groupby('new_item_id')['rating'].mean().var() #variance mean ratings items\n",
    "        k = va/vb #variance proportion\n",
    "        better_mu = (train_set['rating'].mean() + train_set['rating'].sum()) / (k+len(train_set))\n",
    "        return better_mu\n",
    "    \n",
    "    def plot_rmse(self, rmse, val_rmses=[], plot_name=''):\n",
    "        plt.plot(np.arange(len(rmse)), rmse)\n",
    "        if len(val_rmses) > 0:\n",
    "            plt.plot(np.arange(len(val_rmses)), val_rmses, color='red')\n",
    "        plt.title('RMSE')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('RMSE')\n",
    "        plt.legend(['Train', 'Validation'])\n",
    "        if len(plot_name) > 0:\n",
    "            plt.savefig('Plots/' + plot_name + '.png')\n",
    "        plt.show()\n",
    "\n",
    "    def test(self, test_data, val=False):\n",
    "        if not val:\n",
    "            self.test_data = test_data\n",
    "        tuples_test = [tuple(x) for x in test_data[['new_user_id', 'new_item_id', 'rating']].to_numpy()]\n",
    "        test_matrix = self.create_matrix(test_data, self.total_users, self.total_items)\n",
    "        \n",
    "        if self.use_impl_fb and val:\n",
    "            self.N_test = self.val_data.groupby('new_user_id')['new_item_id'].apply(list)\n",
    "        elif self.use_impl_fb:\n",
    "            self.N_test = self.test_data.groupby('new_user_id')['new_item_id'].apply(list)\n",
    "            \n",
    "        total_error = 0\n",
    "        estimates = []\n",
    "        for u, i, r_ui in tuples_test:\n",
    "            u = int(u)\n",
    "            i = int(i)\n",
    "            est = self.estimate(u, i, test_matrix, test_data)\n",
    "            estimates.append(est)\n",
    "            total_error += np.square(r_ui - est)\n",
    "        \n",
    "        rmse = np.sqrt(total_error / len(tuples_test))\n",
    "        \n",
    "        if not val:\n",
    "            self.test_results = {'rmse': rmse, 'estimates':estimates}\n",
    "            print('RMSE on test set:', self.test_results['rmse'])\n",
    "        else:\n",
    "            return rmse\n",
    "\n",
    "    def estimate(self, u, i, test_matrix, test_data):\n",
    "        est = self.mu + self.model['bu'][u] + self.model['bi'][i]\n",
    "        impl_fb_u = np.zeros(self.nolf)\n",
    "        cb = 0\n",
    "        if u in self.train_data['new_user_id'] and i in self.train_data['new_item_id']:\n",
    "            \n",
    "            if self.use_impl_fb and u in self.N.index:\n",
    "                sqrt_Nu = np.sqrt(len(self.N[u]))\n",
    "                for j in self.N[u]:   \n",
    "                    impl_fb_u += self.model['x'][j] / sqrt_Nu\n",
    "            \n",
    "            if self.use_color and (u,i) in self.model['cbu']:\n",
    "                F_ui =  self.F[u,i] #Set of items associated with i and rated by u\n",
    "                u_mu = self.mu + self.model['bu'][u]\n",
    "                sqrt_F_ui = np.sqrt(len(F_ui))\n",
    "                for index, f in enumerate(F_ui):\n",
    "                    r_uf = self.train_data[(self.train_data['new_user_id']==u) & (self.train_data['new_item_id']==f)]['rating'].iloc[0]\n",
    "                    cb += (r_uf - u_mu) * self.model['cbu'][u,i][index]\n",
    "                cb /=  sqrt_F_ui\n",
    "                \n",
    "            est += cb + np.dot(self.model['p'][u] + impl_fb_u, self.model['q'][i])\n",
    "\n",
    "        return est\n",
    "    \n",
    "    def store_results(self, log_path, res_name, user_thres, item_thres):\n",
    "        train_size = round((len(self.train_data) / (len(self.train_data) + len(self.test_data) + len(self.val_data))),1)\n",
    "        test_size = round((len(self.test_data) / (len(self.train_data) + len(self.test_data) + len(self.val_data))),1)\n",
    "        val_size = round((len(self.val_data) / (len(self.train_data) + len(self.test_data) + len(self.val_data))),1)\n",
    "        \n",
    "        result_info = {'RMSE_test': self.test_results['rmse'], 'train_speed': round(self.train_time,2)}\n",
    "        other_info = {'u_thres': user_thres,'i_thres': item_thres, 'train_size':train_size, 'test_size':test_size, 'val_size':val_size, 'train_rmse':self.model['rmse'], 'val_rmse':self.model['val_rmse']}\n",
    "        final_log = dict(result_info, **self.params, **other_info)\n",
    "\n",
    "        if not os.path.exists(log_path + res_name):\n",
    "            df_results = pd.DataFrame(columns=final_log.keys())\n",
    "            print('new results created')\n",
    "\n",
    "        else:\n",
    "            df_results = pd.read_pickle(log_path + res_name)\n",
    "            print('results added')\n",
    "\n",
    "        df_results = df_results.append(final_log, ignore_index=True)\n",
    "        pd.to_pickle(df_results, log_path + res_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPR():\n",
    "    def __init__(self, total_users, total_items, params):\n",
    "        self.total_users = total_users\n",
    "        self.total_items = total_items\n",
    "        self.nolf = params['nolf']\n",
    "        self.n_iterations = params['n_iterations']\n",
    "        self.alpha = params['alpha']\n",
    "        self.reg_user = params['reg_user']\n",
    "        self.reg_item = params['reg_item']\n",
    "        self.reg_bias = params['reg_bias']\n",
    "        self.alpha_decay = self.alpha / self.n_iterations\n",
    "        self.model = {'loss_list':[], 'learning_rate':[]}\n",
    "        \n",
    "    def fit(self, train_set, val_set, val_rank, batch_size=1000):\n",
    "        #Init\n",
    "        s = time.time()\n",
    "        self.model['p'] = np.random.normal(0, .1, (self.total_users, self.nolf))  # users\n",
    "        self.model['q'] = np.random.normal(0, .1, (self.total_items, self.nolf))  # items\n",
    "        self.model['b'] = np.zeros(self.total_items)\n",
    "        \n",
    "#         val_prec_at = []\n",
    "#         val_rec_at = []\n",
    "#         val_hitcount = []\n",
    "        \n",
    "        # Create samples \n",
    "        n_sgd_samples = len(train_set) * self.n_iterations\n",
    "        \n",
    "        z = 0\n",
    "        self.model['train_time'] = 0\n",
    "        print('init and sampling done:', time.time() - s, 'seconds')\n",
    "        for i in range(self.n_iterations):\n",
    "            sgd_users, sgd_pos_items, sgd_neg_items = self.user_sampling(train_set, n_sgd_samples)\n",
    "        \n",
    "        while (z+1)*batch_size < n_sgd_samples:\n",
    "            s_it = time.time()\n",
    "            it_loss = self.train(sgd_users[z*batch_size:(z+1)*batch_size], sgd_pos_items[z*batch_size:(z+1)*batch_size], sgd_neg_items[z*batch_size:(z+1)*batch_size])\n",
    "            \n",
    "            if z > 0:\n",
    "                self.update_alpha(it_loss)\n",
    "            \n",
    "            z += 1\n",
    "            self.model['loss_list'].append(it_loss) \n",
    "\n",
    "#             rec_at, prec_at, hitcount = self.eval(val_set, val_rank)\n",
    "            t_it = time.time()- s_it\n",
    "            self.model['train_time'] += t_it\n",
    "            print('batch:', z, ' loss:', round(it_loss,4), 'iteration time:', round(t_it/2,2))#, ' val prec@' + str(val_rank), ':', round(prec_at,5), ' val rec@' + str(val_rank), ':', round(rec_at,5), '  Hits:', hitcount)#'  alpha:', self.alpha)\n",
    "    \n",
    "#             val_prec_at.append(prec_at)\n",
    "#             val_rec_at.append(rec_at)\n",
    "#             val_hitcount.append(hitcount)\n",
    "            \n",
    "#         self.model['val_prec_at'] = val_prec_at\n",
    "#         self.model['val_rec_at'] = val_rec_at\n",
    "#         self.model['val_hitcount'] = val_hitcount\n",
    "        \n",
    "        \n",
    "    def create_matrices(self, data):\n",
    "        r = data['new_user_id']\n",
    "        c = data['new_item_id']\n",
    "        d = data['rating']\n",
    "        m = sparse.csr_matrix((d, (r, c)), shape=(self.total_users, self.total_items))\n",
    "        m_ones = m.copy()\n",
    "        m_ones[m_ones > 0] = 1                 \n",
    "        return m, m_ones\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + math.exp(-x))\n",
    "    \n",
    "    def user_sampling(self, data, n_samples):\n",
    "        train_ratings, train_ones = self.create_matrices(train_set)\n",
    "        user_items = train_set.groupby('new_user_id')['new_item_id'].apply(list)\n",
    "        train_users  = train_set.new_user_id.unique()\n",
    "        train_items = train_set.new_item_id.unique()\n",
    "        \n",
    "        sgd_users, sgd_pos_items, sgd_neg_items = [], [], []\n",
    "        for sample in range(n_samples):\n",
    "            u = np.random.choice(train_users)\n",
    "            i = random.choice(user_items[u])\n",
    "\n",
    "            j = int(np.random.choice(train_items)) # neg item\n",
    "#             j_v = int(train_ones[u,j]) # Value, NEEDED?\n",
    "\n",
    "            while j in user_items[u]: # j cannot be the same item or an item with a 1\n",
    "                j = int(np.random.choice(train_items))\n",
    "#                 j_v = int(train_ones[u,j])\n",
    "            \n",
    "            sgd_users.append(u)\n",
    "            sgd_pos_items.append(i)\n",
    "            sgd_neg_items.append(j)\n",
    "            \n",
    "        return sgd_users, sgd_pos_items, sgd_neg_items\n",
    "        \n",
    "    def train(self, users, pos_items, neg_items):\n",
    "        for u, i, j in zip(users, pos_items, neg_items):\n",
    "            pos_item_pred = self.model['b'][i] + np.dot(self.model['p'][u], self.model['q'][i].T)\n",
    "            neg_item_pred = self.model['b'][j] + np.dot(self.model['p'][u], self.model['q'][j].T)\n",
    "            diff = pos_item_pred - neg_item_pred\n",
    "\n",
    "            loss_value = - np.log(self.sigmoid(diff)) #NEGATIVE?\n",
    "            regulariser = self.reg_user * np.dot(self.model['p'][u], self.model['p'][u]) + self.reg_item * np.dot(self.model['q'][i],self.model['q'][i]) + self.reg_item/10 * np.dot(self.model['q'][j], self.model['q'][j]) + self.reg_bias * (self.model['b'][i]**2 + self.model['b'][j]**2) \n",
    "            it_loss = loss_value + regulariser\n",
    "\n",
    "            diff_deriv = self.sigmoid(- diff)\n",
    "            \n",
    "            #SGD update\n",
    "            for f in range(self.nolf): # update each factor (see notes for derivatives)\n",
    "                self.model['p'][u,f] += self.alpha * (diff_deriv * (self.model['q'][i,f] - self.model['q'][j,f]) - self.reg_user * self.model['p'][u,f])\n",
    "                self.model['q'][i,f] += self.alpha * (diff_deriv * self.model['p'][u,f] - self.reg_item * self.model['q'][i,f])\n",
    "                self.model['q'][j,f] += self.alpha * (diff_deriv * (-self.model['p'][u,f]) - self.reg_item / 10 * self.model['q'][j,f])\n",
    "                self.model['b'][i] += self.alpha * (diff_deriv * self.reg_bias * self.model['b'][i])\n",
    "                self.model['b'][j] += self.alpha * (- diff_deriv * (- self.reg_bias) * self.model['b'][j])\n",
    "\n",
    "#                 it_loss += self.reg_user * self.model['p'][u,f] * self.model['p'][u,f] + self.reg_item * self.model['q'][i,f] * self.model['q'][i,f] + self.reg_item * self.model['q'][j,f] * self.model['q'][j,f]\n",
    "        return it_loss\n",
    "        \n",
    "    def update_alpha(self, it_loss):\n",
    "        last_loss = self.model['loss_list'][-1]\n",
    "        if(last_loss < it_loss): #bold driver\n",
    "            self.alpha = 0.5 * self.alpha\n",
    "            return\n",
    "        \n",
    "        self.alpha = (1 - self.alpha_decay) * self.alpha\n",
    "        self.model['learning_rate'].append(self.alpha)\n",
    "        \n",
    "    def eval(self, val_set, max_rank):\n",
    "        import eval_rank\n",
    "        val_ratings, val_ones = create_matrices(val_set, self.total_users, self.total_items)\n",
    "        result = self.model\n",
    "        users = val_set.new_user_id.unique()\n",
    "        items = val_set.new_item_id.unique()\n",
    "\n",
    "        s = time.time()\n",
    "        rank_at = max_rank\n",
    "        mp_splits = 4\n",
    "        users_split = np.array_split(users, mp_splits)\n",
    "\n",
    "        if __name__ == '__main__':\n",
    "            pool = mp.Pool(processes = mp_splits)\n",
    "            ranked = pool.map(eval_rank.eval_rank, [[result, users_split[0], items, val_ones, rank_at], \n",
    "                                                    [result, users_split[1], items, val_ones, rank_at], \n",
    "                                                    [result, users_split[2], items, val_ones, rank_at], \n",
    "                                                    [result, users_split[3], items, val_ones, rank_at]])\n",
    "            pool.close()\n",
    "\n",
    "            ranked_df = pd.DataFrame()\n",
    "\n",
    "            for i in range(mp_splits):\n",
    "                ranked_df = pd.concat([ranked_df, ranked[i]])\n",
    "\n",
    "            t = time.time() - s\n",
    "            hitcount = 0\n",
    "            for u in ranked_df.index:\n",
    "                hitcount += len(set(ranked_df.loc[u]['true_id']) & set(ranked_df.loc[u]['pred_items_ranked']))\n",
    "\n",
    "            prec_at =  hitcount / (len(ranked_df) * rank_at)\n",
    "            rec_at = hitcount / (len(ranked_df) * len(ranked_df.loc[0]['true_id']))\n",
    "            \n",
    "            return prec_at, rec_at, hitcount\n",
    "#             print(t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "375.742px",
    "left": "1048.75px",
    "top": "110.57px",
    "width": "200.292px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
