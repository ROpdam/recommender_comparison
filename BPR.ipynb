{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "import multiprocessing as mp\n",
    "import random\n",
    "\n",
    "from scipy import sparse\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.core.display import display, HTML\n",
    "# display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Users/robin.opdam/Google Drive/Thesis (Msc)/Thesis_shared_files/'\n",
    "# path = '/Users/Robin/Google Drive/Thesis (Msc)/Thesis_shared_files/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon Fashion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/Amazon/'\n",
    "# file_name = 'Amazon_full' \n",
    "# file_name = 'Amazon_05_users' \n",
    "# file_name = 'Amazon_01_users'\n",
    "# file_name = 'Amazon_005_users'\n",
    "file_name = 'Amazon_001_users'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MovieLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = 'data/ML/'\n",
    "# file_name = 'ML_full' \n",
    "# file_name = 'ML_05_users'\n",
    "# file_name = 'ML_01_users'\n",
    "# file_name = 'ML_005_users'\n",
    "# file_name = 'ML_001_users'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "      <th>datetime</th>\n",
       "      <th>rating</th>\n",
       "      <th>item_id</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5282697</th>\n",
       "      <td>A1QP7FHEZRZ1LZ</td>\n",
       "      <td>B00HMXYKMM</td>\n",
       "      <td>2017-02-08</td>\n",
       "      <td>5.0</td>\n",
       "      <td>168923</td>\n",
       "      <td>238328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8819596</th>\n",
       "      <td>A1QP7FHEZRZ1LZ</td>\n",
       "      <td>B01A88MEV6</td>\n",
       "      <td>2017-04-10</td>\n",
       "      <td>5.0</td>\n",
       "      <td>335203</td>\n",
       "      <td>238328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3335926</th>\n",
       "      <td>A1QP7FHEZRZ1LZ</td>\n",
       "      <td>B007900UZY</td>\n",
       "      <td>2017-04-10</td>\n",
       "      <td>5.0</td>\n",
       "      <td>77864</td>\n",
       "      <td>238328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6822032</th>\n",
       "      <td>A1QP7FHEZRZ1LZ</td>\n",
       "      <td>B00RI9TL7E</td>\n",
       "      <td>2017-04-10</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245981</td>\n",
       "      <td>238328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4500311</th>\n",
       "      <td>A1QP7FHEZRZ1LZ</td>\n",
       "      <td>B00DBUVIVQ</td>\n",
       "      <td>2017-04-10</td>\n",
       "      <td>5.0</td>\n",
       "      <td>131704</td>\n",
       "      <td>238328</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   user        item   datetime  rating item_id user_id\n",
       "5282697  A1QP7FHEZRZ1LZ  B00HMXYKMM 2017-02-08     5.0  168923  238328\n",
       "8819596  A1QP7FHEZRZ1LZ  B01A88MEV6 2017-04-10     5.0  335203  238328\n",
       "3335926  A1QP7FHEZRZ1LZ  B007900UZY 2017-04-10     5.0   77864  238328\n",
       "6822032  A1QP7FHEZRZ1LZ  B00RI9TL7E 2017-04-10     4.0  245981  238328\n",
       "4500311  A1QP7FHEZRZ1LZ  B00DBUVIVQ 2017-04-10     5.0  131704  238328"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle(path + data_path + file_name)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create new ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['item_id'] = df.item.astype('category').cat.codes\n",
    "df['user_id'] = df.user.astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split\n",
    "### Leave last item out of subset of users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leave_users_out(full_data, leave_out, seed=1234):\n",
    "    np.random.seed(seed)\n",
    "    full_data['index'] = full_data.index\n",
    "    user_index_df = full_data.groupby('user')['index'].apply(list)\n",
    "    users = np.random.choice(list(user_index_df.index), leave_out, replace=False)\n",
    "    users_indices = []\n",
    "    \n",
    "    for user in users:\n",
    "        users_indices.extend(user_index_df.loc[user])\n",
    "    \n",
    "    sub_set = full_data.loc[users_indices]\n",
    "    remaining = full_data.drop(users_indices)\n",
    "    \n",
    "    return remaining.drop(columns=['index']), sub_set.drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leave_last_x_out(full_data, n_users, leave_out=1, seed=1234):\n",
    "    # Input: data must contain user_id\n",
    "    # Output: full_data = without all last (time order) entries in leave one out set\n",
    "    #         leave_one_out_set = data with one user and one item from full_data\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    full_data['index'] = full_data.index\n",
    "    user_items_ind = full_data.groupby('user_id')['index'].apply(list)\n",
    "    users = full_data.user_id.unique()\n",
    "    leave_out_indices = []\n",
    "    users_picked = []\n",
    "    \n",
    "    for i in range(n_users):\n",
    "        random_user = np.random.choice(users)\n",
    "        item_indices = user_items_ind[random_user] # random user's items indices\n",
    "        while len(item_indices) <= leave_out or random_user not in users_picked: # needs to have more items than to leave out, or deleting users\n",
    "            random_user = np.random.choice(users)\n",
    "            item_indices = user_items_ind[random_user]\n",
    "            \n",
    "        users_picked.append(random_user)\n",
    "        leave_out_indices.extend(item_indices[-leave_out:])\n",
    "    \n",
    "    leave_out_set = full_data.loc[leave_out_indices] # the last items of n_users users with n_item > leave_out\n",
    "    full_data_leave_one_out = full_data.drop(leave_out_indices) # drops last items for n_users users\n",
    "    \n",
    "    return full_data_leave_one_out.drop(columns=['index']), leave_out_set.drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine Sizes\n",
    "- Batch Size (From CFRNN)\n",
    "- Test Size\n",
    "- Validation Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_og = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_to_remove = len(df_og.user_id.unique())%BATCH_SIZE #Batch size compatible for CFRNN\n",
    "df, deleted_users = leave_users_out(df_og, users_to_remove)\n",
    "\n",
    "# df['user_id'] = df.user.astype('category').cat.codes #Re-id for BPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0zc319fRc-UP"
   },
   "outputs": [],
   "source": [
    "total_users = len(df_og.user_id.unique()) # Need all users for BPR\n",
    "total_items = len(df_og.item_id.unique()) # Need all items for CFRNN\n",
    "\n",
    "test_users = int(0.1*total_users) # Number of users to be used for testing\n",
    "test_last_items = 1 # Items to be removed from test users in train set and used in test set\n",
    "\n",
    "val_users = int(0.1*total_users) -1\n",
    "val_last_items = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = leave_last_x_out(df, test_users, test_last_items)\n",
    "train_set, val_set = leave_last_x_out(train_set, val_users, val_last_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.to_pickle(path + data_path + file_name + '_train')\n",
    "val_set.to_pickle(path + data_path + file_name + '_val')\n",
    "test_set.to_pickle(path + data_path + file_name + '_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of items: 61616\n",
      "Total users: 12137\n",
      "Number of train users: 12096\n",
      "Number of test users: 1213\n",
      "Number of validation users: 1212 \n",
      "\n",
      "Users deleted: 41\n"
     ]
    }
   ],
   "source": [
    "print('Total number of items:', total_items)\n",
    "print('Total users:', total_users)\n",
    "print('Number of train users:', len(train_set.user_id.unique()))\n",
    "print('Number of test users:', test_users)\n",
    "print('Number of validation users:', val_users, '\\n')\n",
    "print('Users deleted:', len(deleted_users.user_id.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Personalized Ranking\n",
    "- Paper: https://arxiv.org/pdf/1205.2618.pdf\n",
    "- Code:  https://github.com/valerystrizh/bpr/blob/master/BPR.java"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPR():\n",
    "    def __init__(self, total_users, total_items, params):\n",
    "        self.total_users = total_users\n",
    "        self.total_items = total_items\n",
    "        self.nolf = params['nolf']\n",
    "        self.n_iterations = params['n_iterations']\n",
    "        self.sample_size = params['sample_size']\n",
    "        self.seed = params['seed']\n",
    "        self.alpha = params['alpha']\n",
    "        self.reg_user = params['reg_user']\n",
    "        self.reg_item = params['reg_item']\n",
    "        self.alpha_decay = self.alpha / self.n_iterations\n",
    "        \n",
    "        self.model = {}\n",
    "        self.model['val_auc'] = []\n",
    "        self.user_items = pd.DataFrame()\n",
    "        self.train_users = []\n",
    "        self.train_items = []\n",
    "        \n",
    "        self.val_user_items = pd.DataFrame()\n",
    "        self.val_users = []\n",
    "        \n",
    "    def fit(self, train_set, val_set=[]):\n",
    "        # Init\n",
    "        s = time.time()\n",
    "        np.random.seed(self.seed)\n",
    "        p = np.random.normal(0, .1, (self.total_users, self.nolf))  # users\n",
    "        q = np.random.normal(0, .1, (self.total_items, self.nolf))  # items\n",
    "        \n",
    "        ## Used for sampling\n",
    "        self.user_items = train_set.groupby('user_id')['item_id'].apply(list)\n",
    "        self.train_users  = train_set.user_id.unique()\n",
    "        self.train_items = train_set.item_id.unique()\n",
    "        \n",
    "        if len(val_set) > 0:\n",
    "            ## Used for validating\n",
    "            self.val_user_items = val_set.groupby('user_id')['item_id'].apply(list)\n",
    "            self.val_users = val_set.user_id.unique()\n",
    "        \n",
    "        ## Track losses and alphas used\n",
    "        loss_list = []\n",
    "        alphas = []\n",
    "        \n",
    "        ## Create samples for all iterations\n",
    "        all_uij_samples = self.sample()\n",
    "        \n",
    "        # Training Loop\n",
    "        for iteration in range(self.n_iterations):\n",
    "            it_loss = 0\n",
    "            uij_samples = all_uij_samples[iteration]\n",
    "            \n",
    "            for uij_sample in uij_samples:\n",
    "                u = uij_sample[0]\n",
    "                i = uij_sample[1]\n",
    "                j = uij_sample[2]\n",
    "                \n",
    "                ## Calculate the difference between positive and negative item\n",
    "                diff = np.dot(p[u], (q[i] - q[j]).T)\n",
    "                \n",
    "                ## Obtain loss \n",
    "                loss_value = - np.log(self.sigmoid(diff))\n",
    "                regulariser = self.reg_user * np.dot(p[u], p[u]) + self.reg_item * np.dot(q[i],q[i]) + self.reg_item/10 * np.dot(q[j], q[j])\n",
    "                it_loss += loss_value + regulariser\n",
    "                \n",
    "                ## Derivative of the difference for update \n",
    "                diff_deriv = self.sigmoid(- diff)\n",
    "                \n",
    "                ## Update the factors of the latent features, using their respective derivatives\n",
    "                ## See http://ethen8181.github.io/machine-learning/recsys/4_bpr.html\n",
    "                p[u] += self.alpha * (diff_deriv * (q[i] - q[j]) - self.reg_user * p[u])\n",
    "                q[i] += self.alpha * (diff_deriv * p[u] - self.reg_item * q[i])\n",
    "                q[j] += self.alpha * (diff_deriv * (-p[u]) - self.reg_item * q[j])\n",
    "            \n",
    "            ## Store iteration variables\n",
    "            self.model['p'] = p\n",
    "            self.model['q'] = q\n",
    "            \n",
    "            if len(val_set) > 0: # TO DO: safe best & early stopping\n",
    "                val_auc = self.AUC()\n",
    "                self.model['val_auc'].append(val_auc)\n",
    "                print('iteration:', iteration, ' loss:', round(it_loss/self.sample_size,6), ' val AUC:', val_auc)#, ' val prec@' + str(val_rank), ':', round(prec_at,5), ' val rec@' + str(val_rank), ':', round(rec_at,5), '  Hits:', hitcount)#'  alpha:', self.alpha)\n",
    "            else:\n",
    "                print('iteration:', iteration, ' loss:', round(it_loss/self.sample_size,6))\n",
    "                \n",
    "            if iteration > 0:\n",
    "                self.update_alpha(loss_list[-1], it_loss)\n",
    "                \n",
    "            alphas.append(self.alpha)\n",
    "            loss_list.append(it_loss)\n",
    "        \n",
    "        # Store train values\n",
    "        train_time = time.time() - s\n",
    "        self.model['train_loss'] = loss_list\n",
    "        self.model['learning_rate'] = alphas\n",
    "        self.model['train_time'] = train_time\n",
    "        \n",
    "        \n",
    "    def sample(self):\n",
    "        n_samples = self.n_iterations\n",
    "        sample_size = int(self.sample_size)\n",
    "        print('Creating', str(n_samples), 'samples of length', str(sample_size))\n",
    "        all_uij_samples = []\n",
    "        \n",
    "        for n in range(n_samples):\n",
    "            uij_samples = []\n",
    "            for s in range(sample_size):\n",
    "                u = int(np.random.choice(self.train_users))\n",
    "                u_items = self.user_items[u]\n",
    "                i = random.choice(u_items)\n",
    "                j = int(np.random.choice(self.train_items)) \n",
    "                while j in u_items: #neg item j cannot be in the set of pos items of user u\n",
    "                    j = int(np.random.choice(self.train_items))\n",
    "                \n",
    "                uij_samples.append([u,i,j])\n",
    "                \n",
    "            all_uij_samples.append(uij_samples)\n",
    "            \n",
    "        return all_uij_samples\n",
    "        \n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + math.exp(-x))\n",
    "    \n",
    "    \n",
    "    def update_alpha(self, last_loss, it_loss):\n",
    "        if(last_loss < it_loss): #bold driver\n",
    "            self.alpha = 0.5 * self.alpha\n",
    "            return\n",
    "        \n",
    "        self.alpha = (1 - self.alpha_decay) * self.alpha\n",
    "    \n",
    "    def AUC(self):\n",
    "        auc = 0.0\n",
    "        n_users = len(self.val_users)\n",
    "\n",
    "        for u in self.val_users:\n",
    "            y_pred = np.dot(self.model['p'][u], self.model['q'].T)\n",
    "            y_true = np.zeros(self.total_items)\n",
    "            y_true[self.val_user_items[u]] = 1\n",
    "            auc += roc_auc_score(y_true, y_pred)\n",
    "        \n",
    "        auc /= n_users\n",
    "        return auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "\"nolf\":20, #Size of latent feature vectors\n",
    "\"n_iterations\":40, #around 20 is sufficient\n",
    "\"sample_size\":0.1*len(train_set),\n",
    "\"seed\":1234,\n",
    "\"alpha\":0.1, # Learning rate\n",
    "          \n",
    "#Regularizers, still tweaking the values\n",
    "\"reg_user\":0.1,\n",
    "\"reg_item\":0.1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BPR_amazon_005 = BPR(total_users, total_items, params)\n",
    "BPR_amazon_005.fit(train_set, val_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_results(results, log_path, res_name, file_name, params):\n",
    "        result_info = {'train_loss': results['train_loss'], 'train_speed': results['train_time'], 'lr':results['learning_rate'], 'file':file_name}\n",
    "        other_info = {'p':results['p'], 'q':results['q']} #'train_size':train_size, 'test_size':test_size, 'val_size':val_size}\n",
    "        final_log = dict(result_info, **params, **other_info)\n",
    "\n",
    "        if not os.path.exists(log_path + res_name):\n",
    "            df_results = pd.DataFrame(columns=final_log.keys())\n",
    "            print('new results created')\n",
    "\n",
    "        else:\n",
    "            df_results = pd.read_pickle(log_path + res_name)\n",
    "            print('results added')\n",
    "\n",
    "        df_results = df_results.append(final_log, ignore_index=True)\n",
    "        pd.to_pickle(df_results, log_path + res_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = path + 'Results/BPR/'\n",
    "res_name = 'BPR_models'\n",
    "# store_results(BPR_amazon_001.model, log_path, res_name, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = pd.read_pickle(log_path + res_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last = len(df_res['train_loss']) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df_res['train_loss'][last])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df_res['lr'][last])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_predictions(model, test_set, rank_at, multiprocessing=True):\n",
    "    import eval_rank_bpr\n",
    "    \n",
    "    s = time.time()\n",
    "    users = test_set.user_id.unique()\n",
    "    items = test_set.item_id.unique()\n",
    "    test_user_items = test_set.groupby('user_id')['item_id'].apply(list)\n",
    "    ranked_df = pd.DataFrame(columns=['pred_items_ranked', 'true_id'], index=users)\n",
    "    \n",
    "    if multiprocessing:\n",
    "        mp_splits = 4\n",
    "        users_split = np.array_split(users, mp_splits)\n",
    "\n",
    "        if __name__ == '__main__':\n",
    "            pool = mp.Pool(processes = mp_splits)\n",
    "            ranked = pool.map(eval_rank_bpr.eval_rank_bpr, [\n",
    "                                                    [model, users_split[0], items, test_user_items, rank_at], \n",
    "                                                    [model, users_split[1], items, test_user_items, rank_at], \n",
    "                                                    [model, users_split[2], items, test_user_items, rank_at], \n",
    "                                                    [model, users_split[3], items, test_user_items, rank_at]\n",
    "                                                            ])\n",
    "            pool.close()\n",
    "\n",
    "            for i in range(mp_splits):\n",
    "                ranked_df = pd.concat([ranked_df, ranked[i]])\n",
    "    \n",
    "    else:\n",
    "        pred_items_ranked = []\n",
    "        true_items_list = []\n",
    "        \n",
    "        for u in users:\n",
    "            user_item_pred_score = []\n",
    "            true_items = []\n",
    "            for true_item in test_user_items.loc[u]:\n",
    "                true_items.append(true_item)\n",
    "\n",
    "            predictions = np.dot(model['p'][u], model['q'].T)\n",
    "            ids = np.argpartition(predictions, -rank_at)[-rank_at:]\n",
    "            best_ids = np.argsort(predictions[ids])[::-1]\n",
    "            best = ids[best_ids]\n",
    "            \n",
    "            pred_items_ranked.append(best)\n",
    "            true_items_list.append(true_items)\n",
    "\n",
    "        ranked_df['pred_items_ranked'] = pred_items_ranked\n",
    "        ranked_df['true_id'] = true_items_list\n",
    "\n",
    "    print('Ranking time:', round(time.time() - s,2))\n",
    "    \n",
    "    return ranked_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(ranked_df, steps, max_rank):\n",
    "    s = time.time()\n",
    "    ranks_at = [1] + [i for i in range(steps, max_rank + steps, steps)]\n",
    "    hitcounts = []\n",
    "    recs_at = []\n",
    "    precs_at = []\n",
    "    metrics = pd.DataFrame(columns=['rank_at', 'hitcounts', 'recall', 'precision'])\n",
    "    for rank in ranks_at:\n",
    "        hitcount = 0\n",
    "        for i, row in ranked_df.iterrows():\n",
    "            hitcount +=  len(set(row['true_id']) & set(row['pred_items_ranked'][:rank]))\n",
    "\n",
    "        prec_at = hitcount / rank / len(ranked_df)\n",
    "        rec_at = hitcount / len(ranked_df.iloc[0]['true_id']) / len(ranked_df)\n",
    "\n",
    "        hitcounts.append(hitcount)                     \n",
    "        recs_at.append(rec_at)\n",
    "        precs_at.append(prec_at)\n",
    "\n",
    "    metrics['rank_at'] = ranks_at\n",
    "    metrics['hitcounts'] = hitcounts\n",
    "    metrics['recall'] = recs_at\n",
    "    metrics['precision'] = precs_at\n",
    "    print('Obtaining metrics time:', round(time.time() - s,2))\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_set.user_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = {'p':df_res['p'][1], 'q':df_res['q'][1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_at = 20\n",
    "ranked_df = rank_predictions(model, test_set, rank_at, True)\n",
    "\n",
    "steps = 5\n",
    "metrics = get_metrics(ranked_df, steps, rank_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.to_pickle(path + 'Results/BPR/metrics_' + file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_pickle(path + 'Results/metrics_' + file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(metrics['rank_at'], metrics['precision'])\n",
    "plt.xlabel('Rank')\n",
    "plt.ylabel('precision@')\n",
    "plt.title('Precision for different rank values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(metrics['rank_at'], metrics['recall'])\n",
    "plt.xlabel('Rank')\n",
    "plt.ylabel('recall@')\n",
    "plt.title('Recall')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_run(file_names, file_paths, all_params, sample_percs):\n",
    "    runs = 0\n",
    "    for file_name, file_path, params, sample_perc in zip(file_names, file_paths, all_params, sample_percs):\n",
    "        # Init \n",
    "        print('\\n\\n', '='*50)\n",
    "        df = pd.read_pickle(file_path + file_name)\n",
    "        df['item_id'] = df.item.astype('category').cat.codes\n",
    "        df['user_id'] = df.user.astype('category').cat.codes\n",
    "        \n",
    "        # Create Train and Test sets\n",
    "        BATCH_SIZE = 64\n",
    "        df_og = df\n",
    "        \n",
    "        users_to_remove = len(df_og.user_id.unique())%BATCH_SIZE #Batch size compatible for CFRNN\n",
    "        df, deleted_users = leave_users_out(df_og, users_to_remove)\n",
    "\n",
    "        total_users = len(df_og.user_id.unique()) # Need all users for BPR\n",
    "        total_items = len(df_og.item_id.unique()) # Need all items for CFRNN\n",
    "\n",
    "        test_users = int(0.1*total_users) # Number of users to be used for testing\n",
    "        test_last_items = 1 # Items to be removed from test users in train set and used in test set\n",
    "\n",
    "        val_users = int(0.1*total_users) -1\n",
    "        val_last_items = 1\n",
    "        \n",
    "        train_set, test_set = leave_last_x_out(df, test_users, test_last_items)\n",
    "        train_set, val_set = leave_last_x_out(train_set, val_users, val_last_items)\n",
    "        \n",
    "        train_users = len(train_set.user_id.unique())\n",
    "        \n",
    "        print('Run:', runs + 1, '\\nFile:', file_name,\n",
    "              '\\nTrain users:', train_users, '\\nTest users:', test_users,\n",
    "              '\\nVal users:', val_users)\n",
    "        \n",
    "        # Model Run\n",
    "        params['sample_size'] = sample_perc*len(train_set)\n",
    "        model = BPR(total_users, total_items, params)\n",
    "        model.fit(train_set)\n",
    "        \n",
    "        log_path = path + 'Results/BPR/'\n",
    "        res_name = 'BPR_models'\n",
    "        store_results(model.model, log_path, res_name, file_name, params)\n",
    "        \n",
    "        rank_at = 20\n",
    "        steps = 5\n",
    "        \n",
    "        ranked_df = rank_predictions(model.model, test_set, rank_at, False)\n",
    "        metrics = get_metrics(ranked_df, steps, rank_at)\n",
    "        print('Results')\n",
    "        print(metrics)\n",
    "        \n",
    "        metrics.to_pickle(path + 'Results/BPR/metrics_' + file_name)\n",
    "        \n",
    "        runs += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_amazon_001 = {\n",
    "\"nolf\":20, # Size of latent feature vectors\n",
    "\"n_iterations\":60, # Depends on number of items and users\n",
    "\"sample_size\":1, # Adjusted in loop to 0.5*len(train_set)\n",
    "\"seed\":1234,\n",
    "\"alpha\":0.1, # Learning rate\n",
    "          \n",
    "#Regularizers\n",
    "\"reg_user\":0.08, # should be in proportion to the number of items \n",
    "\"reg_item\":0.08,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_amazon_005 = {\n",
    "\"nolf\":20, # Size of latent feature vectors\n",
    "\"n_iterations\":60, # Depends on number of items and users\n",
    "\"sample_size\":1, # Adjusted in loop to 0.5*len(train_set)\n",
    "\"seed\":1234,\n",
    "\"alpha\":0.1, # Learning rate\n",
    "          \n",
    "#Regularizers\n",
    "\"reg_user\":0.05, # should be in proportion to the number of items \n",
    "\"reg_item\":0.05,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_amazon_01 = {\n",
    "\"nolf\":20, # Size of latent feature vectors\n",
    "\"n_iterations\":60, # Depends on number of items and users\n",
    "\"sample_size\":1, # Adjusted in loop to 0.5*len(train_set)\n",
    "\"seed\":1234,\n",
    "\"alpha\":0.1, # Learning rate\n",
    "          \n",
    "#Regularizers\n",
    "\"reg_user\":0.05, # should be in proportion to the number of items \n",
    "\"reg_item\":0.05,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " ==================================================\n"
     ]
    }
   ],
   "source": [
    "file_names = ['Amazon_01_users', 'Amazon_005_users', 'Amazon_001_users']\n",
    "file_paths = 3*[path + 'Data/Amazon/']\n",
    "all_params = [params_amazon_01, params_amazon_005, params_amazon_001]\n",
    "sample_percs = [0.1, 0.3, 0.5]\n",
    "\n",
    "seq_run(file_names, file_paths, all_params, sample_percs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 005 sample_prec = 0.2\n",
    "# Results\n",
    "#    rank_at  hitcounts    recall  precision\n",
    "# 0        1         45  0.007794   0.007794\n",
    "# 1        5        265  0.045895   0.009179\n",
    "# 2       10        339  0.058711   0.005871\n",
    "# 3       15        364  0.063041   0.004203\n",
    "# 4       20        392  0.067891   0.003395\n",
    "\n",
    "\n",
    "# 001 sample_prec = 0.5\n",
    "# Results\n",
    "#    rank_at  hitcounts    recall  precision\n",
    "# 0        1         11  0.009532   0.009532\n",
    "# 1        5         53  0.045927   0.009185\n",
    "# 2       10         70  0.060659   0.006066\n",
    "# 3       15         75  0.064991   0.004333\n",
    "# 4       20         76  0.065858   0.003293"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_pickle('Data/amazon_clothing_shoes_jewelry_data')\n",
    "# users = df.user.unique()\n",
    "# to_keep = users[:300000]\n",
    "\n",
    "# user_indices = df.groupby('user')['index'].apply(list)\n",
    "# to_keep_indices = []\n",
    "# for u in user_indices[to_keep]:\n",
    "#     to_keep_indices.extend(u)\n",
    "\n",
    "# new_df = df_og.loc[to_keep_indices]\n",
    "# len(to_keep_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## standard SVD model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVD:\n",
    "    \"\"\"\" All functions used to run, test, plot and store the\n",
    "    Singular Value Decomposition Model\"\"\"\n",
    "\n",
    "    def __init__(self, params, total_users, total_items):\n",
    "        self.nolf = params['nolf']\n",
    "        self.n_epochs = params['n_epochs']\n",
    "        self.alpha = params['alpha']\n",
    "        self.alpha_b = params['alpha_b']\n",
    "        self.alpha_cb = params['alpha_cb']\n",
    "        self.use_bias = params['use_bias']\n",
    "        self.use_impl_fb = params['use_impl_fb']\n",
    "        self.use_color = params['use_color']\n",
    "        self.use_weight_ver = params['use_weight_ver']\n",
    "        self.bu_reg = params['bu_reg']\n",
    "        self.bi_reg = params['bi_reg']\n",
    "        self.pu_reg = params['pu_reg']\n",
    "        self.qi_reg = params['qi_reg']\n",
    "        self.x_reg = params['x_reg']\n",
    "        self.cb_reg = params['cb_reg']\n",
    "        self.ver_weight = params['ver_weight']\n",
    "        self.stop = params['stop']\n",
    "        self.random_state = params['random_state']\n",
    "        self.total_users = total_users\n",
    "        self.total_items = total_items\n",
    "        self.params = params\n",
    "        self.mu = 0 \n",
    "        self.N = []\n",
    "        self.N_test = []\n",
    "        self.t = pd.DataFrame()\n",
    "        self.c = pd.DataFrame()\n",
    "        self.F = pd.DataFrame()\n",
    "\n",
    "        self.train_data = pd.DataFrame()\n",
    "        self.test_data = pd.DataFrame()\n",
    "        self.val_data = pd.DataFrame()\n",
    "        self.train_time = 0\n",
    "        self.best_model = {}\n",
    "        self.model = {}\n",
    "        self.test_results = {}\n",
    "\n",
    "    def fit(self, train_data, val_data=[], verbose=1, plot=True, plot_name=''):\n",
    "        self.train_data = train_data\n",
    "        self.val_data = val_data\n",
    "        self.SVD(train_data=train_data, val_data=val_data, verbose=verbose, plot=plot, plot_name=plot_name)\n",
    "        return self\n",
    "\n",
    "    \n",
    "###############################################################################################\n",
    "    \n",
    "    def SVD(self, train_data, val_data, verbose, plot, plot_name):\n",
    "        \"\"\"\"The SVD algorithm with sgd\n",
    "        input: rating dataset with columns:['rating', 'user_id', 'item_id']\n",
    "        output: the resulting p, q, bi, bu matrices\"\"\"\n",
    "        self.mu = self.create_mu(train_data)\n",
    "        train_matrix = self.create_matrix(train_data, self.total_users, self.total_items)\n",
    "        \n",
    "        tuples_train = [tuple(x) for x in train_data[['new_user_id', 'new_item_id', 'rating']].to_numpy()]\n",
    "        \n",
    "        p = np.random.normal(0, .1, (total_users, self.nolf))  # users\n",
    "        q = np.random.normal(0, .1, (total_items, self.nolf))  # items\n",
    "        \n",
    "        # user and item biases\n",
    "        b_user = np.zeros(total_users)\n",
    "        b_item = np.zeros(total_items)\n",
    "        \n",
    "        # using color (pareto split (0,1,2)) attribute bias\n",
    "        if self.use_color:\n",
    "            print('Creating F and c, for incorporating color bias')\n",
    "            self.F, self.c = self.init_color(train_data)\n",
    "\n",
    "        # implicit fb rated, not rated\n",
    "        x = np.random.normal(0, .1, (total_items, self.nolf))\n",
    "        impl_fb_u = np.zeros(self.nolf)\n",
    "        if self.use_impl_fb:\n",
    "            print('Creating N, for incorporating implicit feedback')\n",
    "            self.N = train_data.groupby('new_user_id')['new_item_id'].apply(list)\n",
    "        \n",
    "        # 0.5 weight on the errors of verified = False user item combinations\n",
    "        if self.use_weight_ver:\n",
    "            i_verified = train_data.set_index(['new_user_id', 'new_item_id'])['verified']\n",
    "            i_verified = i_verified.loc[~i_verified.index.duplicated(keep='first')]\n",
    "        \n",
    "        sqrt_Nu = 0\n",
    "        cb = 0\n",
    "        rmses = []\n",
    "        val_rmses = []\n",
    "        smallest_val_rmse = 10000\n",
    "        val_rmse = \"na\"\n",
    "        start = time.time()\n",
    "        for epoch in range(self.n_epochs):\n",
    "            total_sq_error = 0\n",
    "            for u, i, r_ui in tuples_train:\n",
    "                u = int(u)\n",
    "                i = int(i)\n",
    "                \n",
    "                if self.use_impl_fb:\n",
    "                    impl_fb_u = np.zeros(self.nolf)\n",
    "                    sqrt_Nu = np.sqrt(len(self.N[u]))\n",
    "                    for j in self.N[u]:\n",
    "                        impl_fb_u += x[j] / sqrt_Nu\n",
    "\n",
    "                if self.use_color and epoch > 5:\n",
    "                    F_ui =  self.F[u,i] #Set of items associated with i and rated by u\n",
    "                    u_mu = self.mu + b_user[u]\n",
    "                    sqrt_F_ui = np.sqrt(len(F_ui))\n",
    "                    for index, f in enumerate(F_ui):\n",
    "                        r_uf = train_data[(train_data['new_user_id']==u) & (train_data['new_item_id']==f)]['rating'].iloc[0]\n",
    "                        cb += (r_uf - u_mu) * self.c[u,i][index]\n",
    "                    cb /=  sqrt_F_ui\n",
    "                        \n",
    "                if self.use_bias:   \n",
    "                    error = r_ui - ((self.mu + b_user[u] + b_item[i] + cb) + np.dot(p[u] + impl_fb_u, q[i]))\n",
    "                    if self.use_weight_ver and not i_verified[u,i]:\n",
    "                        error = self.ver_weight * error\n",
    "                    \n",
    "                    b_user[u] += self.alpha_b * (error - self.bu_reg * b_user[u])\n",
    "                    b_item[i] += self.alpha_b * (error - self.bi_reg * b_item[i])\n",
    "                else:\n",
    "                    error = r_ui - np.dot(p[u], q[i])\n",
    "\n",
    "                p[u] += self.alpha * (error * q[i] - self.pu_reg * p[u])\n",
    "                q[i] += self.alpha * (error * (p[u] + impl_fb_u) - self.qi_reg * q[i])\n",
    "                total_sq_error += np.square(error)\n",
    "            \n",
    "                if self.use_impl_fb:\n",
    "                    for j in self.N[u]:\n",
    "                        x[j] += self.alpha * (error * q[i] / sqrt_Nu - self.x_reg * x[j])\n",
    "                \n",
    "                if self.use_color and epoch > 5:\n",
    "                    for index, f in enumerate(F_ui):\n",
    "                        r_uf = train_data[(train_data['new_user_id']==u) & (train_data['new_item_id']==f)]['rating'].iloc[0]\n",
    "                        u_mu = self.mu + b_user[u]\n",
    "                        self.c[u,i][index] += self.alpha_cb * (error * (1/sqrt_F_ui) * (r_uf - u_mu) - self.cb_reg * self.c[u,i][index])\n",
    "                \n",
    "            rmse = np.sqrt(total_sq_error / len(tuples_train))\n",
    "            rmses.append(rmse)\n",
    "            \n",
    "            self.model = {'p': p, 'q': q, 'bu':b_user, 'bi':b_item, 'cbu': self.c, 'x':x, 'rmse':rmses, 'val_rmse':val_rmses}\n",
    "            \n",
    "            # Validation\n",
    "            if len(val_data) > 0:\n",
    "                new_val_rmse = self.test(val_data, val=True)\n",
    "                val_rmses.append(new_val_rmse)\n",
    "                if new_val_rmse < smallest_val_rmse:\n",
    "                    smallest_val_rmse = new_val_rmse\n",
    "                    self.best_model = copy.deepcopy(self.model)\n",
    "                val_rmse = new_val_rmse\n",
    "                \n",
    "            # Epoch Printing\n",
    "            if epoch % verbose == 0:\n",
    "                if len(val_data) > 0:\n",
    "                    print('Epoch:', epoch, '  RMSE:', rmse, ' Val_RMSE:', val_rmse)\n",
    "                else:\n",
    "                    print('Epoch:', epoch, '  RMSE:', rmse)\n",
    "            \n",
    "            if self.stop and val_rmses[-2:][0] < val_rmse:\n",
    "                print('BREAK: Validation set not improving anymore')\n",
    "                break\n",
    "                \n",
    "        if plot:\n",
    "            self.plot_rmse(rmses, val_rmses, plot_name)\n",
    "\n",
    "        self.train_time = time.time() - start\n",
    "        self.model = {'p': p, 'q': q, 'bu':b_user, 'bi':b_item, 'cbu': self.c, 'x':x, 'rmse':rmses, 'val_rmse':val_rmses}\n",
    "#################################################################################################\n",
    "\n",
    "    def init_color(self, data_set):\n",
    "        self.t = data_set.groupby(['new_user_id', 'par_col2'])['new_item_id'].apply(list)\n",
    "        F = data_set.groupby(['new_user_id', 'new_item_id'])['par_col2'].apply(self.sim_items)\n",
    "        c = data_set.groupby(['new_user_id', 'new_item_id'])['par_col2'].apply(self.sim_items, random=True)\n",
    "        return F, c\n",
    "\n",
    "    def sim_items(self, x, random=False):\n",
    "        u_id = x.name[0]\n",
    "        col = x.iloc[0]\n",
    "        if random:\n",
    "            return np.random.normal(0,.1,len(self.t[u_id, col]))\n",
    "        return self.t[u_id, col]\n",
    "    \n",
    "    def create_matrix(self, X_train, n_users, n_items):\n",
    "        r = X_train['new_user_id']\n",
    "        c = X_train['new_item_id']\n",
    "        d = X_train['rating']\n",
    "        train_matrix = sparse.coo_matrix((d, (r, c)), shape=(n_users, n_items))\n",
    "    \n",
    "        return train_matrix.tocsr()\n",
    "    \n",
    "    def create_mu(self, train_set):\n",
    "        # Better mean calculation according to https://sifter.org/~simon/journal/20061211.html\n",
    "        va = train_set.groupby('new_user_id')['rating'].mean().var() #variance mean ratings users\n",
    "        vb = train_set.groupby('new_item_id')['rating'].mean().var() #variance mean ratings items\n",
    "        k = va/vb #variance proportion\n",
    "        better_mu = (train_set['rating'].mean() + train_set['rating'].sum()) / (k+len(train_set))\n",
    "        return better_mu\n",
    "    \n",
    "    def plot_rmse(self, rmse, val_rmses=[], plot_name=''):\n",
    "        plt.plot(np.arange(len(rmse)), rmse)\n",
    "        if len(val_rmses) > 0:\n",
    "            plt.plot(np.arange(len(val_rmses)), val_rmses, color='red')\n",
    "        plt.title('RMSE')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('RMSE')\n",
    "        plt.legend(['Train', 'Validation'])\n",
    "        if len(plot_name) > 0:\n",
    "            plt.savefig('Plots/' + plot_name + '.png')\n",
    "        plt.show()\n",
    "\n",
    "    def test(self, test_data, val=False):\n",
    "        if not val:\n",
    "            self.test_data = test_data\n",
    "        tuples_test = [tuple(x) for x in test_data[['new_user_id', 'new_item_id', 'rating']].to_numpy()]\n",
    "        test_matrix = self.create_matrix(test_data, self.total_users, self.total_items)\n",
    "        \n",
    "        if self.use_impl_fb and val:\n",
    "            self.N_test = self.val_data.groupby('new_user_id')['new_item_id'].apply(list)\n",
    "        elif self.use_impl_fb:\n",
    "            self.N_test = self.test_data.groupby('new_user_id')['new_item_id'].apply(list)\n",
    "            \n",
    "        total_error = 0\n",
    "        estimates = []\n",
    "        for u, i, r_ui in tuples_test:\n",
    "            u = int(u)\n",
    "            i = int(i)\n",
    "            est = self.estimate(u, i, test_matrix, test_data)\n",
    "            estimates.append(est)\n",
    "            total_error += np.square(r_ui - est)\n",
    "        \n",
    "        rmse = np.sqrt(total_error / len(tuples_test))\n",
    "        \n",
    "        if not val:\n",
    "            self.test_results = {'rmse': rmse, 'estimates':estimates}\n",
    "            print('RMSE on test set:', self.test_results['rmse'])\n",
    "        else:\n",
    "            return rmse\n",
    "\n",
    "    def estimate(self, u, i, test_matrix, test_data):\n",
    "        est = self.mu + self.model['bu'][u] + self.model['bi'][i]\n",
    "        impl_fb_u = np.zeros(self.nolf)\n",
    "        cb = 0\n",
    "        if u in self.train_data['new_user_id'] and i in self.train_data['new_item_id']:\n",
    "            \n",
    "            if self.use_impl_fb and u in self.N.index:\n",
    "                sqrt_Nu = np.sqrt(len(self.N[u]))\n",
    "                for j in self.N[u]:   \n",
    "                    impl_fb_u += self.model['x'][j] / sqrt_Nu\n",
    "            \n",
    "            if self.use_color and (u,i) in self.model['cbu']:\n",
    "                F_ui =  self.F[u,i] #Set of items associated with i and rated by u\n",
    "                u_mu = self.mu + self.model['bu'][u]\n",
    "                sqrt_F_ui = np.sqrt(len(F_ui))\n",
    "                for index, f in enumerate(F_ui):\n",
    "                    r_uf = self.train_data[(self.train_data['new_user_id']==u) & (self.train_data['new_item_id']==f)]['rating'].iloc[0]\n",
    "                    cb += (r_uf - u_mu) * self.model['cbu'][u,i][index]\n",
    "                cb /=  sqrt_F_ui\n",
    "                \n",
    "            est += cb + np.dot(self.model['p'][u] + impl_fb_u, self.model['q'][i])\n",
    "\n",
    "        return est\n",
    "    \n",
    "    def store_results(self, log_path, res_name, user_thres, item_thres):\n",
    "        train_size = round((len(self.train_data) / (len(self.train_data) + len(self.test_data) + len(self.val_data))),1)\n",
    "        test_size = round((len(self.test_data) / (len(self.train_data) + len(self.test_data) + len(self.val_data))),1)\n",
    "        val_size = round((len(self.val_data) / (len(self.train_data) + len(self.test_data) + len(self.val_data))),1)\n",
    "        \n",
    "        result_info = {'RMSE_test': self.test_results['rmse'], 'train_speed': round(self.train_time,2)}\n",
    "        other_info = {'u_thres': user_thres,'i_thres': item_thres, 'train_size':train_size, 'test_size':test_size, 'val_size':val_size, 'train_rmse':self.model['rmse'], 'val_rmse':self.model['val_rmse']}\n",
    "        final_log = dict(result_info, **self.params, **other_info)\n",
    "\n",
    "        if not os.path.exists(log_path + res_name):\n",
    "            df_results = pd.DataFrame(columns=final_log.keys())\n",
    "            print('new results created')\n",
    "\n",
    "        else:\n",
    "            df_results = pd.read_pickle(log_path + res_name)\n",
    "            print('results added')\n",
    "\n",
    "        df_results = df_results.append(final_log, ignore_index=True)\n",
    "        pd.to_pickle(df_results, log_path + res_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPR():\n",
    "    def __init__(self, total_users, total_items, params):\n",
    "        self.total_users = total_users\n",
    "        self.total_items = total_items\n",
    "        self.nolf = params['nolf']\n",
    "        self.n_iterations = params['n_iterations']\n",
    "        self.alpha = params['alpha']\n",
    "        self.reg_user = params['reg_user']\n",
    "        self.reg_item = params['reg_item']\n",
    "        self.reg_bias = params['reg_bias']\n",
    "        self.alpha_decay = self.alpha / self.n_iterations\n",
    "        self.model = {'loss_list':[], 'learning_rate':[]}\n",
    "        \n",
    "    def fit(self, train_set, val_set, val_rank, batch_size=1000):\n",
    "        #Init\n",
    "        s = time.time()\n",
    "        self.model['p'] = np.random.normal(0, .1, (self.total_users, self.nolf))  # users\n",
    "        self.model['q'] = np.random.normal(0, .1, (self.total_items, self.nolf))  # items\n",
    "        self.model['b'] = np.zeros(self.total_items)\n",
    "        \n",
    "#         val_prec_at = []\n",
    "#         val_rec_at = []\n",
    "#         val_hitcount = []\n",
    "        \n",
    "        # Create samples \n",
    "        n_sgd_samples = len(train_set) * self.n_iterations\n",
    "        \n",
    "        z = 0\n",
    "        self.model['train_time'] = 0\n",
    "        print('init and sampling done:', time.time() - s, 'seconds')\n",
    "        for i in range(self.n_iterations):\n",
    "            sgd_users, sgd_pos_items, sgd_neg_items = self.user_sampling(train_set, n_sgd_samples)\n",
    "        \n",
    "        while (z+1)*batch_size < n_sgd_samples:\n",
    "            s_it = time.time()\n",
    "            it_loss = self.train(sgd_users[z*batch_size:(z+1)*batch_size], sgd_pos_items[z*batch_size:(z+1)*batch_size], sgd_neg_items[z*batch_size:(z+1)*batch_size])\n",
    "            \n",
    "            if z > 0:\n",
    "                self.update_alpha(it_loss)\n",
    "            \n",
    "            z += 1\n",
    "            self.model['loss_list'].append(it_loss) \n",
    "\n",
    "#             rec_at, prec_at, hitcount = self.eval(val_set, val_rank)\n",
    "            t_it = time.time()- s_it\n",
    "            self.model['train_time'] += t_it\n",
    "            print('batch:', z, ' loss:', round(it_loss,4), 'iteration time:', round(t_it/2,2))#, ' val prec@' + str(val_rank), ':', round(prec_at,5), ' val rec@' + str(val_rank), ':', round(rec_at,5), '  Hits:', hitcount)#'  alpha:', self.alpha)\n",
    "    \n",
    "#             val_prec_at.append(prec_at)\n",
    "#             val_rec_at.append(rec_at)\n",
    "#             val_hitcount.append(hitcount)\n",
    "            \n",
    "#         self.model['val_prec_at'] = val_prec_at\n",
    "#         self.model['val_rec_at'] = val_rec_at\n",
    "#         self.model['val_hitcount'] = val_hitcount\n",
    "        \n",
    "        \n",
    "    def create_matrices(self, data):\n",
    "        r = data['new_user_id']\n",
    "        c = data['new_item_id']\n",
    "        d = data['rating']\n",
    "        m = sparse.csr_matrix((d, (r, c)), shape=(self.total_users, self.total_items))\n",
    "        m_ones = m.copy()\n",
    "        m_ones[m_ones > 0] = 1                 \n",
    "        return m, m_ones\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + math.exp(-x))\n",
    "    \n",
    "    def user_sampling(self, data, n_samples):\n",
    "        train_ratings, train_ones = self.create_matrices(train_set)\n",
    "        user_items = train_set.groupby('new_user_id')['new_item_id'].apply(list)\n",
    "        train_users  = train_set.new_user_id.unique()\n",
    "        train_items = train_set.new_item_id.unique()\n",
    "        \n",
    "        sgd_users, sgd_pos_items, sgd_neg_items = [], [], []\n",
    "        for sample in range(n_samples):\n",
    "            u = np.random.choice(train_users)\n",
    "            i = random.choice(user_items[u])\n",
    "\n",
    "            j = int(np.random.choice(train_items)) # neg item\n",
    "#             j_v = int(train_ones[u,j]) # Value, NEEDED?\n",
    "\n",
    "            while j in user_items[u]: # j cannot be the same item or an item with a 1\n",
    "                j = int(np.random.choice(train_items))\n",
    "#                 j_v = int(train_ones[u,j])\n",
    "            \n",
    "            sgd_users.append(u)\n",
    "            sgd_pos_items.append(i)\n",
    "            sgd_neg_items.append(j)\n",
    "            \n",
    "        return sgd_users, sgd_pos_items, sgd_neg_items\n",
    "        \n",
    "    def train(self, users, pos_items, neg_items):\n",
    "        for u, i, j in zip(users, pos_items, neg_items):\n",
    "            pos_item_pred = self.model['b'][i] + np.dot(self.model['p'][u], self.model['q'][i].T)\n",
    "            neg_item_pred = self.model['b'][j] + np.dot(self.model['p'][u], self.model['q'][j].T)\n",
    "            diff = pos_item_pred - neg_item_pred\n",
    "\n",
    "            loss_value = - np.log(self.sigmoid(diff)) #NEGATIVE?\n",
    "            regulariser = self.reg_user * np.dot(self.model['p'][u], self.model['p'][u]) + self.reg_item * np.dot(self.model['q'][i],self.model['q'][i]) + self.reg_item/10 * np.dot(self.model['q'][j], self.model['q'][j]) + self.reg_bias * (self.model['b'][i]**2 + self.model['b'][j]**2) \n",
    "            it_loss = loss_value + regulariser\n",
    "\n",
    "            diff_deriv = self.sigmoid(- diff)\n",
    "            \n",
    "            #SGD update\n",
    "            for f in range(self.nolf): # update each factor (see notes for derivatives)\n",
    "                self.model['p'][u,f] += self.alpha * (diff_deriv * (self.model['q'][i,f] - self.model['q'][j,f]) - self.reg_user * self.model['p'][u,f])\n",
    "                self.model['q'][i,f] += self.alpha * (diff_deriv * self.model['p'][u,f] - self.reg_item * self.model['q'][i,f])\n",
    "                self.model['q'][j,f] += self.alpha * (diff_deriv * (-self.model['p'][u,f]) - self.reg_item / 10 * self.model['q'][j,f])\n",
    "                self.model['b'][i] += self.alpha * (diff_deriv * self.reg_bias * self.model['b'][i])\n",
    "                self.model['b'][j] += self.alpha * (- diff_deriv * (- self.reg_bias) * self.model['b'][j])\n",
    "\n",
    "#                 it_loss += self.reg_user * self.model['p'][u,f] * self.model['p'][u,f] + self.reg_item * self.model['q'][i,f] * self.model['q'][i,f] + self.reg_item * self.model['q'][j,f] * self.model['q'][j,f]\n",
    "        return it_loss\n",
    "        \n",
    "    def update_alpha(self, it_loss):\n",
    "        last_loss = self.model['loss_list'][-1]\n",
    "        if(last_loss < it_loss): #bold driver\n",
    "            self.alpha = 0.5 * self.alpha\n",
    "            return\n",
    "        \n",
    "        self.alpha = (1 - self.alpha_decay) * self.alpha\n",
    "        self.model['learning_rate'].append(self.alpha)\n",
    "        \n",
    "    def eval(self, val_set, max_rank):\n",
    "        import eval_rank\n",
    "        val_ratings, val_ones = create_matrices(val_set, self.total_users, self.total_items)\n",
    "        result = self.model\n",
    "        users = val_set.new_user_id.unique()\n",
    "        items = val_set.new_item_id.unique()\n",
    "\n",
    "        s = time.time()\n",
    "        rank_at = max_rank\n",
    "        mp_splits = 4\n",
    "        users_split = np.array_split(users, mp_splits)\n",
    "\n",
    "        if __name__ == '__main__':\n",
    "            pool = mp.Pool(processes = mp_splits)\n",
    "            ranked = pool.map(eval_rank.eval_rank, [[result, users_split[0], items, val_ones, rank_at], \n",
    "                                                    [result, users_split[1], items, val_ones, rank_at], \n",
    "                                                    [result, users_split[2], items, val_ones, rank_at], \n",
    "                                                    [result, users_split[3], items, val_ones, rank_at]])\n",
    "            pool.close()\n",
    "\n",
    "            ranked_df = pd.DataFrame()\n",
    "\n",
    "            for i in range(mp_splits):\n",
    "                ranked_df = pd.concat([ranked_df, ranked[i]])\n",
    "\n",
    "            t = time.time() - s\n",
    "            hitcount = 0\n",
    "            for u in ranked_df.index:\n",
    "                hitcount += len(set(ranked_df.loc[u]['true_id']) & set(ranked_df.loc[u]['pred_items_ranked']))\n",
    "\n",
    "            prec_at =  hitcount / (len(ranked_df) * rank_at)\n",
    "            rec_at = hitcount / (len(ranked_df) * len(ranked_df.loc[0]['true_id']))\n",
    "            \n",
    "            return prec_at, rec_at, hitcount\n",
    "#             print(t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "375.742px",
    "left": "1048.75px",
    "top": "110.57px",
    "width": "200.292px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
