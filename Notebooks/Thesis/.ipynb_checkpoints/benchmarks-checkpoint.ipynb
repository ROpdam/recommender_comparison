{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions Used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split\n",
    "### Leave last item out of subset of users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leave_users_out(full_data, leave_out, seed=1234):\n",
    "    np.random.seed(seed)\n",
    "    full_data['index'] = full_data.index\n",
    "    user_index_df = full_data.groupby('user')['index'].apply(list)\n",
    "    users = np.random.choice(list(user_index_df.index), leave_out, replace=False)\n",
    "    users_indices = []\n",
    "    \n",
    "    for user in users:\n",
    "        users_indices.extend(user_index_df.loc[user])\n",
    "    \n",
    "    sub_set = full_data.loc[users_indices]\n",
    "    remaining = full_data.drop(users_indices)\n",
    "    \n",
    "    return remaining.drop(columns=['index']), sub_set.drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leave_last_x_out(full_data, n_users, leave_out=1, seed=1234):\n",
    "    # Input: data must contain user_id\n",
    "    # Output: full_data = without all last (time order) entries in leave one out set\n",
    "    #         leave_one_out_set = data with one user and one item from full_data\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    full_data['index'] = full_data.index\n",
    "    user_items_ind = full_data.groupby('user_id')['index'].apply(list)\n",
    "    users = full_data.user_id.unique()\n",
    "    leave_out_indices = []\n",
    "    users_picked = []\n",
    "    \n",
    "    for i in range(len(full_data.user_id.unique())):\n",
    "        random_user = np.random.choice(users)\n",
    "        item_indices = user_items_ind[random_user] # random user's items indices\n",
    "        if random_user in users_picked or len(item_indices) <= leave_out:\n",
    "            random_user = np.random.choice(users)\n",
    "            item_indices = user_items_ind[random_user] # random user's items indices\n",
    "        else:\n",
    "            users_picked.append(random_user)\n",
    "            leave_out_indices.extend(item_indices[-leave_out:])\n",
    "        \n",
    "        if len(users_picked) == n_users:\n",
    "            break\n",
    "        \n",
    "    if len(users_picked) < n_users:\n",
    "        error = 'Cannot pick ' + str(n_users) + ' users with more than ' + str(leave_out) + ' items'\n",
    "        solution = '\\nTry a smaller test and/or validation percentage of the data'\n",
    "        raise ValueError(error + solution) \n",
    "            \n",
    "    leave_out_set = full_data.loc[leave_out_indices] # the last items of n_users users with n_item > leave_out\n",
    "    full_data_leave_one_out = full_data.drop(leave_out_indices) # drops last items for n_users users\n",
    "    \n",
    "    return full_data_leave_one_out.drop(columns=['index']), leave_out_set.drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split(df, batch_size, val_perc, test_perc, n_items_val, n_items_test, stats=True):\n",
    "    # Input: df with user and item id, batch size for CFRNN data, val and test perc of users\n",
    "    #        number of last items to leave out for val and test set\n",
    "    # Output:full_data = total users and items of the original df, \n",
    "    #        Train, validation and test sets\n",
    "    \n",
    "    total_users = len(df.user_id.unique()) # Need all users for BPR\n",
    "    total_items = len(df.item_id.unique()) # Need all items for CFRNN\n",
    "    \n",
    "    users_to_remove = len(df.user_id.unique())%batch_size #Batch size compatible for CFRNN\n",
    "    df_new, deleted_users = leave_users_out(df, users_to_remove)\n",
    "\n",
    "    test_users = int(test_perc*total_users / 64 + 1) * 64 # Number of users to be used for testing\n",
    "    test_last_items = n_items_test # Items to be removed from test users in train set and used in test set\n",
    "\n",
    "    val_users = int(val_perc*total_users / 64 + 1) * 64\n",
    "    val_last_items = n_items_val\n",
    "    \n",
    "    train_set, test_set = leave_last_x_out(df_new, test_users, test_last_items)\n",
    "    train_set, val_set = leave_last_x_out(train_set, val_users, val_last_items)\n",
    "    \n",
    "    if stats:\n",
    "        print('Total number of items:', total_items)\n",
    "        print('Total users:', total_users)\n",
    "        print('Number of train users:', len(train_set.user_id.unique()))\n",
    "        print('Number of test users:', test_users)\n",
    "        print('Number of validation users:', val_users, '\\n')\n",
    "        print('Users deleted:', len(deleted_users.user_id.unique()))\n",
    "    \n",
    "    return total_users, total_items, train_set, val_set, test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Final Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(ranked_df, steps, max_rank, stats=False):\n",
    "    s = time.time()\n",
    "    ranks_at = [1] + [i for i in range(steps, max_rank + steps, steps)]\n",
    "    hitcounts = []\n",
    "    recs_at = []\n",
    "    precs_at = []\n",
    "    metrics = pd.DataFrame(columns=['rank_at', 'hitcounts', 'recall', 'precision'])\n",
    "    for rank in ranks_at:\n",
    "        hitcount = 0\n",
    "        for i, row in ranked_df.iterrows():\n",
    "            hitcount +=  len(set(row['true_id']) & set(row['pred_items_ranked'][:rank]))\n",
    "\n",
    "        prec_at = hitcount / rank / len(ranked_df)\n",
    "        rec_at = hitcount / len(ranked_df.iloc[0]['true_id']) / len(ranked_df)\n",
    "\n",
    "        hitcounts.append(hitcount)                     \n",
    "        recs_at.append(rec_at)\n",
    "        precs_at.append(prec_at)\n",
    "\n",
    "    metrics['rank_at'] = ranks_at\n",
    "    metrics['hitcounts'] = hitcounts\n",
    "    metrics['recall'] = recs_at\n",
    "    metrics['precision'] = precs_at\n",
    "    if stats:\n",
    "        print('Obtaining metrics time:', round(time.time() - s,2))\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Popularity Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pop_bench(dataset, train_set, rank_at, steps):\n",
    "    \"\"\"\n",
    "    Creates pop_df (pandas) in which pred_items_ranked will be filled with the most popular items,\n",
    "    ranked based on their rate count in train_set. True_id in random_df is a list of the items per \n",
    "    user from dataset.\n",
    "    \n",
    "    Args:\n",
    "    dataset: the data to create a popularity benchmark with (most likely a test/val set)\n",
    "    train_set: data on which to base the counts \n",
    "    steps: stepsize for the ranking of the results\n",
    "    rank_at: max rank to produce the metrics on\n",
    "    \"\"\"\n",
    "    most_pop = list(train_set.groupby('item_id')['rating'].count().sort_values(ascending=False)[:20].index)\n",
    "    users = dataset.user_id.unique()\n",
    "    pop_df = pd.DataFrame(columns=['pred_items_ranked', 'true_id'], index = users)\n",
    "    \n",
    "    for u in users:\n",
    "        pop_df.loc[u]['pred_items_ranked'] = most_pop\n",
    "        pop_df.loc[u]['true_id'] = list(dataset[dataset['user_id']==u]['item_id'])\n",
    "    \n",
    "    metrics = get_metrics(pop_df, steps, rank_at)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_bench(dataset, total_items, rank_at, steps):\n",
    "    \"\"\"\n",
    "    Creates random_df (pandas) in which pred_items_ranked will be filled with a randomized subset of \n",
    "    the item_id's of size rank_at. true_id in random_df is a list of the items per user from dataset.\n",
    "    \n",
    "    Args:\n",
    "    dataset: the data to create a random benchmark with (most likely a test/val set)\n",
    "    total_items: all items a user can pick from\n",
    "    steps: stepsize for the ranking of the results\n",
    "    rank_at: max rank to produce the metrics on\n",
    "    \"\"\"\n",
    "    users = dataset.user_id.unique()\n",
    "    random_df = pd.DataFrame(columns=['pred_items_ranked', 'true_id'], index=users)\n",
    "    \n",
    "    for u in users:\n",
    "        random_df.loc[u]['pred_items_ranked'] = np.random.choice(total_items, size=rank_at)\n",
    "        random_df.loc[u]['true_id'] = list(dataset[dataset['user_id']==u]['item_id'])\n",
    "        \n",
    "    metrics = get_metrics(random_df, steps, rank_at)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Both Benchmark Metrics Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_benchmarks(path, file_name, rank_at, steps, rand_trials, cut_ml_rate): \n",
    "    \"\"\"\n",
    "    Given the path and file_name, creates a popularity benchmark and a random benchmark, benchmarks show \n",
    "    Recall@steps, Precision@steps and Hitcount@steps\n",
    "    \n",
    "    Args:\n",
    "    path: the path where the dataset is located\n",
    "    file_name: name of file to read (should be a ratings pickle file with cols: item, user, rating)\n",
    "    steps: stepsize for the ranking of the results\n",
    "    rank_at: max rank to produce the metrics on\n",
    "    \n",
    "    Returns:\n",
    "    final_random_bench based on rand_trials randomized result metrics\n",
    "    pop_bench, benchmark based on the number of rating counts of the dataset\n",
    "    \"\"\"\n",
    "    # Read\n",
    "    df = pd.read_pickle(path + file_name)\n",
    "    df.head()\n",
    "    \n",
    "    # Keep only ratings > 3 for movielens dataset\n",
    "    if cut_ml_rate and file_name[0] == 'M':\n",
    "        print('prev length:', len(df))\n",
    "        df = df[df['rating'] > 3]\n",
    "        print('new length:', len(df))\n",
    "    \n",
    "    # Get new user and item ids\n",
    "    df['item_id'] = df.item.astype('category').cat.codes\n",
    "    df['user_id'] = df.user.astype('category').cat.codes\n",
    "\n",
    "    # Create train test splits\n",
    "    BATCH_SIZE = 64\n",
    "    val_perc = test_perc = 0.1\n",
    "    n_last_items_val = n_last_items_test = 1\n",
    "    total_users, total_items, train_set, val_set, test_set = \\\n",
    "    train_val_test_split(df, BATCH_SIZE, val_perc, test_perc, n_last_items_val, n_last_items_test)\n",
    "\n",
    "    # Get random bench, avg of 10 runs\n",
    "    random_bench = get_random_bench(test_set, len(df.item_id.unique()), 20, 5)\n",
    "    if rand_trials > 0:\n",
    "        for i in range(rand_trials - 1):\n",
    "            random_bench += get_random_bench(test_set, len(df.item_id.unique()), 20, 5)\n",
    "\n",
    "    final_random_bench = random_bench / trials\n",
    "    # Get pop bench, based on counts in train set\n",
    "    pop_bench = get_pop_bench(test_set, train_set, rank_at, steps)\n",
    "    \n",
    "    return final_random_bench, pop_bench"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init Path and Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = 'C:/Users/robin.opdam/Google Drive/Thesis (Msc)/Thesis_shared_files/'\n",
    "path = '/Users/Robin/Google Drive/Thesis (Msc)/Thesis_shared_files/'\n",
    "names_am = ['Amazon_01_users']\n",
    "names_ml = ['ML_01_users']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create all metrics for Benchmarks\n",
    "- Random\n",
    "- Popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ==================================================\n",
      "ML_01_users\n",
      "prev length: 2446037\n",
      "new length: 1542930\n",
      "Total number of items: 21569\n",
      "Total users: 16241\n",
      "Number of train users: 16192\n",
      "Number of test users: 1664\n",
      "Number of validation users: 1664 \n",
      "\n",
      "Users deleted: 49\n",
      "RANDOM\n",
      "    rank_at  hitcounts    recall  precision\n",
      "0      1.0        0.0  0.000000   0.000000\n",
      "1      5.0        0.4  0.000240   0.000048\n",
      "2     10.0        0.8  0.000481   0.000048\n",
      "3     15.0        1.0  0.000601   0.000040\n",
      "4     20.0        1.2  0.000721   0.000036\n",
      "POPULAR\n",
      "    rank_at  hitcounts    recall  precision\n",
      "0        1          4  0.002404   0.002404\n",
      "1        5         35  0.021034   0.004207\n",
      "2       10         61  0.036659   0.003666\n",
      "3       15         92  0.055288   0.003686\n",
      "4       20        119  0.071514   0.003576\n"
     ]
    }
   ],
   "source": [
    "amazon_path = path + 'Data/Amazon/'\n",
    "ml_path = path + 'Data/ML/'\n",
    "res_path = path + 'Results/Results_17_04/'\n",
    "\n",
    "file_paths = [amazon_path]*len(names_am) + [ml_path]*len(names_ml)\n",
    "file_names = names_am + names_ml\n",
    "\n",
    "file_paths = [ml_path]*len(names_ml)\n",
    "file_names = names_ml\n",
    "\n",
    "rank_at = 20\n",
    "steps = 5\n",
    "rand_trials = 10\n",
    "\n",
    "for file_name, file_path in zip(file_names, file_paths):\n",
    "    print('\\n','='*50)\n",
    "    print(file_name)\n",
    "    random_bench, pop_bench = create_benchmarks(file_path, file_name, rank_at, steps, rand_trials, True)\n",
    "    random_bench.to_pickle(res_path + 'rand_bench_' + file_name + '_rate_above_3')\n",
    "    pop_bench.to_pickle(res_path + 'pop_bench_' + file_name + '_rate_above_3')\n",
    "    print('RANDOM\\n', random_bench)\n",
    "    print('POPULAR\\n', pop_bench)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
