{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions Used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Train Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leave_users_out(full_data, leave_out, seed=1234):\n",
    "    np.random.seed(seed)\n",
    "    full_data['index'] = full_data.index\n",
    "    user_index_df = full_data.groupby('user')['index'].apply(list)\n",
    "    users = np.random.choice(list(user_index_df.index), leave_out, replace=False)\n",
    "    users_indices = []\n",
    "    \n",
    "    for user in users:\n",
    "        users_indices.extend(user_index_df.loc[user])\n",
    "    \n",
    "    sub_set = full_data.loc[users_indices]\n",
    "    remaining = full_data.drop(users_indices)\n",
    "    \n",
    "    return remaining.drop(columns=['index']), sub_set.drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leave_last_x_out(full_data, n_users, leave_out=1, seed=1234):\n",
    "    # Input: data must contain user_id\n",
    "    # Output: full_data = without all last (time order) entries in leave one out set\n",
    "    #         leave_one_out_set = data with one user and one item from full_data\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    full_data['index'] = full_data.index\n",
    "    user_items_ind = full_data.groupby('user_id')['index'].apply(list)\n",
    "    users = full_data.user_id.unique()\n",
    "    leave_out_indices = []\n",
    "    users_picked = []\n",
    "    \n",
    "    for i in range(n_users):\n",
    "        random_user = np.random.choice(users)\n",
    "        item_indices = user_items_ind[random_user] # random user's items indices\n",
    "        while random_user not in users_picked and len(item_indices) <= leave_out: # needs to have more items than to leave out, or deleting users\n",
    "            random_user = np.random.choice(users)\n",
    "            item_indices = user_items_ind[random_user]\n",
    "            \n",
    "        users_picked.append(random_user)\n",
    "        leave_out_indices.extend(item_indices[-leave_out:])\n",
    "    \n",
    "    leave_out_set = full_data.loc[leave_out_indices] # the last items of n_users users with n_item > leave_out\n",
    "    full_data_leave_one_out = full_data.drop(leave_out_indices) # drops last items for n_users users\n",
    "    \n",
    "    return full_data_leave_one_out.drop(columns=['index']), leave_out_set.drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Final Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(ranked_df, steps, max_rank):\n",
    "    s = time.time()\n",
    "    ranks_at = [1] + [i for i in range(steps, max_rank + steps, steps)]\n",
    "    hitcounts = []\n",
    "    recs_at = []\n",
    "    precs_at = []\n",
    "    metrics = pd.DataFrame(columns=['rank_at', 'hitcounts', 'recall', 'precision'])\n",
    "    for rank in ranks_at:\n",
    "        hitcount = 0\n",
    "        for i, row in ranked_df.iterrows():\n",
    "            hitcount +=  len(set(row['true_id']) & set(row['pred_items_ranked'][:rank]))\n",
    "\n",
    "        prec_at = hitcount / rank / len(ranked_df)\n",
    "        rec_at = hitcount / len(ranked_df.iloc[0]['true_id']) / len(ranked_df)\n",
    "\n",
    "        hitcounts.append(hitcount)                     \n",
    "        recs_at.append(rec_at)\n",
    "        precs_at.append(prec_at)\n",
    "\n",
    "    metrics['rank_at'] = ranks_at\n",
    "    metrics['hitcounts'] = hitcounts\n",
    "    metrics['recall'] = recs_at\n",
    "    metrics['precision'] = precs_at\n",
    "    print('Obtaining metrics time:', round(time.time() - s,2))\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Popularity Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pop_bench(dataset, train_set, rank_at, steps):\n",
    "    counts = train_set.groupby('item_id')['item'].count()\n",
    "    most_pop_items = counts.sort_values(ascending=False)[:20]\n",
    "    users = dataset.user_id.unique()\n",
    "    pop_df = pd.DataFrame(columns=['pred_items_ranked', 'true_id'], index=users)    \n",
    "    \n",
    "    for u in users:\n",
    "        pop_df.loc[u]['pred_items_ranked'] = list(most_pop_items)\n",
    "        pop_df.loc[u]['true_id'] = list(dataset[dataset['user_id']==u]['item_id'])\n",
    "    \n",
    "    metrics = get_metrics(pop_df, steps, rank_at)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_bench(dataset, total_items, rank_at, steps):\n",
    "    users = dataset.user_id.unique()\n",
    "    random_df = pd.DataFrame(columns=['pred_items_ranked', 'true_id'], index=users)\n",
    "    \n",
    "    for u in users:\n",
    "        random_df.loc[u]['pred_items_ranked'] = np.random.choice(total_items, size=rank_at)\n",
    "        random_df.loc[u]['true_id'] = list(dataset[dataset['user_id']==u]['item_id'])\n",
    "        \n",
    "    metrics = get_metrics(random_df, steps, rank_at)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Both Benchmark Metrics Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_benchmarks(path, file_name, rank_at, steps):   \n",
    "    # Read\n",
    "    df = pd.read_pickle(path + file_name)\n",
    "    df.head()\n",
    "\n",
    "    df['item_id'] = df.item.astype('category').cat.codes\n",
    "    df['user_id'] = df.user.astype('category').cat.codes\n",
    "\n",
    "    # Create train test splits\n",
    "    BATCH_SIZE = 64\n",
    "\n",
    "    df_og = df\n",
    "\n",
    "    users_to_remove = len(df_og.user_id.unique())%BATCH_SIZE #Batch size compatible for CFRNN\n",
    "    df, deleted_users = leave_users_out(df_og, users_to_remove)\n",
    "\n",
    "    total_users = len(df_og.user_id.unique()) # Need all users for BPR\n",
    "    total_items = len(df_og.item_id.unique()) # Need all items for CFRNN\n",
    "\n",
    "    test_users = int(0.1*total_users) # Number of users to be used for testing\n",
    "    test_last_items = 1 # Items to be removed from test users in train set and used in test set\n",
    "\n",
    "    val_users = int(0.1*total_users) -1\n",
    "    val_last_items = 1\n",
    "\n",
    "    # Split\n",
    "    train_set, test_set = leave_last_x_out(df, test_users, test_last_items)\n",
    "    train_set, val_set = leave_last_x_out(train_set, val_users, val_last_items)\n",
    "\n",
    "    print('Total number of items:', total_items)\n",
    "    print('Total users:', total_users)\n",
    "    print('Number of train users:', len(train_set.user_id.unique()))\n",
    "    print('Number of test users:', test_users)\n",
    "    print('Number of validation users:', val_users, '\\n')\n",
    "    print('Users deleted:', len(deleted_users.user_id.unique()))\n",
    "\n",
    "    # Get random bench\n",
    "    random_bench = get_random_bench(test_set, df_og.item_id.unique(), rank_at, steps)\n",
    "    pop_bench = get_pop_bench(test_set, train_set, rank_at, steps)\n",
    "    \n",
    "    return random_bench, pop_bench"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init Path and Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = 'C:/Users/robin.opdam/Google Drive/Thesis (Msc)/Thesis_shared_files/'\n",
    "path = '/Users/Robin/Google Drive/Thesis (Msc)/Thesis_shared_files/'\n",
    "names_am = ['Amazon_01_users', 'Amazon_005_users']\n",
    "names_ml = ['ML_01_users', 'ML_005_users']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create all metrics for Benchmarks\n",
    "- Random\n",
    "- Popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ==================================================\n",
      "Amazon_01_users\n",
      "Total number of items: 247465\n",
      "Total users: 121372\n",
      "Number of train users: 121344\n",
      "Number of test users: 12137\n",
      "Number of validation users: 12136 \n",
      "\n",
      "Users deleted: 28\n",
      "Obtaining metrics time: 10.5\n",
      "Obtaining metrics time: 9.51\n",
      "RANDOM\n",
      "    rank_at  hitcounts    recall  precision\n",
      "0        1          0  0.000000   0.000000\n",
      "1        5          1  0.000087   0.000017\n",
      "2       10          1  0.000087   0.000009\n",
      "3       15          2  0.000173   0.000012\n",
      "4       20          2  0.000173   0.000009\n",
      "POPULAR\n",
      "    rank_at  hitcounts    recall  precision\n",
      "0        1          0  0.000000   0.000000\n",
      "1        5          1  0.000087   0.000017\n",
      "2       10          1  0.000087   0.000009\n",
      "3       15          2  0.000173   0.000012\n",
      "4       20          2  0.000173   0.000009\n",
      "\n",
      " ==================================================\n",
      "Amazon_005_users\n",
      "Total number of items: 176152\n",
      "Total users: 60686\n",
      "Number of train users: 60672\n",
      "Number of test users: 6068\n",
      "Number of validation users: 6067 \n",
      "\n",
      "Users deleted: 14\n",
      "Obtaining metrics time: 4.22\n",
      "Obtaining metrics time: 4.14\n",
      "RANDOM\n",
      "    rank_at  hitcounts    recall  precision\n",
      "0        1          0  0.000000   0.000000\n",
      "1        5          0  0.000000   0.000000\n",
      "2       10          0  0.000000   0.000000\n",
      "3       15          2  0.000346   0.000023\n",
      "4       20          2  0.000346   0.000017\n",
      "POPULAR\n",
      "    rank_at  hitcounts    recall  precision\n",
      "0        1          0  0.000000   0.000000\n",
      "1        5          0  0.000000   0.000000\n",
      "2       10          1  0.000173   0.000017\n",
      "3       15          2  0.000346   0.000023\n",
      "4       20          3  0.000520   0.000026\n",
      "\n",
      " ==================================================\n",
      "ML_01_users\n",
      "Total number of items: 27387\n",
      "Total users: 16254\n",
      "Number of train users: 16192\n",
      "Number of test users: 1625\n",
      "Number of validation users: 1624 \n",
      "\n",
      "Users deleted: 62\n",
      "Obtaining metrics time: 1.43\n",
      "Obtaining metrics time: 1.26\n",
      "RANDOM\n",
      "    rank_at  hitcounts    recall  precision\n",
      "0        1          0  0.000000   0.000000\n",
      "1        5          1  0.000645   0.000129\n",
      "2       10          1  0.000645   0.000065\n",
      "3       15          1  0.000645   0.000043\n",
      "4       20          1  0.000645   0.000032\n",
      "POPULAR\n",
      "    rank_at  hitcounts   recall  precision\n",
      "0        1          0  0.00000   0.000000\n",
      "1        5          0  0.00000   0.000000\n",
      "2       10          0  0.00000   0.000000\n",
      "3       15          2  0.00129   0.000086\n",
      "4       20          2  0.00129   0.000065\n",
      "\n",
      " ==================================================\n",
      "ML_005_users\n",
      "Total number of items: 22312\n",
      "Total users: 8127\n",
      "Number of train users: 8064\n",
      "Number of test users: 812\n",
      "Number of validation users: 811 \n",
      "\n",
      "Users deleted: 63\n",
      "Obtaining metrics time: 0.74\n",
      "Obtaining metrics time: 0.65\n",
      "RANDOM\n",
      "    rank_at  hitcounts    recall  precision\n",
      "0        1          0  0.000000   0.000000\n",
      "1        5          1  0.001309   0.000262\n",
      "2       10          1  0.001309   0.000131\n",
      "3       15          1  0.001309   0.000087\n",
      "4       20          1  0.001309   0.000065\n",
      "POPULAR\n",
      "    rank_at  hitcounts    recall  precision\n",
      "0        1          0  0.000000   0.000000\n",
      "1        5          0  0.000000   0.000000\n",
      "2       10          1  0.001309   0.000131\n",
      "3       15          2  0.002618   0.000175\n",
      "4       20          2  0.002618   0.000131\n"
     ]
    }
   ],
   "source": [
    "amazon_path = path + 'Data/Amazon/'\n",
    "ml_path = path + 'Data/ML/'\n",
    "res_path = path + 'Results/Benchmarks/'\n",
    "\n",
    "file_paths = [amazon_path]*len(names_am) + [ml_path]*len(names_ml)\n",
    "file_names = names_am + names_ml\n",
    "\n",
    "rank_at = 20\n",
    "steps = 5\n",
    "\n",
    "for file_name, file_path in zip(file_names, file_paths):\n",
    "    print('\\n','='*50)\n",
    "    print(file_name)\n",
    "    random_bench, pop_bench = create_benchmarks(file_path, file_name, rank_at, steps)\n",
    "    random_bench.to_pickle(res_path + 'rand_bench_' + file_name)\n",
    "    pop_bench.to_pickle(res_path + 'pop_bench_' + file_name)\n",
    "    print('RANDOM\\n', random_bench)\n",
    "    print('POPULAR\\n', pop_bench)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_at = 20\n",
    "steps = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_path = path + 'Data/ML/'\n",
    "df = pd.read_pickle(ml_path + 'ML_full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7500028"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['item_id'] = df.item.astype('category').cat.codes\n",
    "df['user_id'] = df.user.astype('category').cat.codes\n",
    "\n",
    "# Create train test splits\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "df_og = df\n",
    "\n",
    "users_to_remove = len(df_og.user_id.unique())%BATCH_SIZE #Batch size compatible for CFRNN\n",
    "df, deleted_users = leave_users_out(df_og, users_to_remove)\n",
    "\n",
    "total_users = len(df_og.user_id.unique()) # Need all users for BPR\n",
    "total_items = len(df_og.item_id.unique()) # Need all items for CFRNN\n",
    "\n",
    "test_users = int(0.1*total_users) # Number of users to be used for testing\n",
    "test_last_items = 1 # Items to be removed from test users in train set and used in test set\n",
    "\n",
    "val_users = int(0.1*total_users) -1\n",
    "val_last_items = 1\n",
    "\n",
    "# Split\n",
    "train_set, test_set = leave_last_x_out(df, test_users, test_last_items)\n",
    "train_set, val_set = leave_last_x_out(train_set, val_users, val_last_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining metrics time: 26.92\n",
      "Obtaining metrics time: 16.23\n"
     ]
    }
   ],
   "source": [
    "random_df, random_bench = get_random_bench(test_set, df_og.item_id.unique(), rank_at, steps)\n",
    "pop_df, pop_bench = get_pop_bench(test_set, train_set, rank_at, steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[24358,\n",
       " 24245,\n",
       " 23875,\n",
       " 22141,\n",
       " 21757,\n",
       " 20492,\n",
       " 19188,\n",
       " 18045,\n",
       " 17704,\n",
       " 17423,\n",
       " 17224,\n",
       " 17062,\n",
       " 17013,\n",
       " 16731,\n",
       " 16671,\n",
       " 16384,\n",
       " 16318,\n",
       " 15792,\n",
       " 15505,\n",
       " 15267]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pop_df.iloc[0]['pred_items_ranked']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Popularity Benchmark SHOULD BE COUNTS OF TRAIN SET\n",
    "Popularity decides rank of item for everyone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_rank_at = 20\n",
    "steps = 5\n",
    "ranks_at = [1] + [i for i in range(steps, rank_at + steps, steps)]\n",
    "items_in_test_set = 1\n",
    "pop_results = pd.DataFrame(columns=file_names)\n",
    "\n",
    "for name in file_names:\n",
    "    print('\\n', name)\n",
    "    df = pd.read_pickle('Data/' + name)\n",
    "    df_new_ids = transform(df)\n",
    "    df_new_ids['item_counts'] = df_new_ids.groupby('item_id')['user_id'].transform('count') #for populairty\n",
    "    train_set, test_set = leave_x_out(df_new_ids, items_in_test_set*2)\n",
    "    val_set, test_set = leave_x_out(test_set, items_in_test_set)\n",
    "    \n",
    "    most_pop_items = test_set.sort_values('item_counts')['item_id'].unique()[-max_rank_at:]\n",
    "    user_items = test_set.groupby('user_id')['item_id'].apply(list)\n",
    "    \n",
    "    hitcounts = []\n",
    "    for rank in ranks_at:\n",
    "        hitcount = 0\n",
    "        for u in test_set.user_id.unique():\n",
    "            for item in user_items[u]:\n",
    "                if item in most_pop_items[:rank]:\n",
    "                    hitcount += 1\n",
    "        print('rank_at', rank, ' hitcount:', hitcount)\n",
    "        hitcounts.append(hitcount)\n",
    "    \n",
    "    pop_results[name] = hitcounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_results_old = pd.read_pickle('Results/BPR/pop_rank_hits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_results_old['ml_0.7_u_above_5_3_r_thres'] = pop_results['ml_0.7_u_above_5_3_r_thres']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_results.to_pickle('Results/BPR/pop_rank_hits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_results_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
