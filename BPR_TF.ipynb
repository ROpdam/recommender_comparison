{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "import calc_vector\n",
    "import multiprocessing as mp\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "from scipy import sparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.sparse.linalg import spsolve\n",
    "\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.core.display import display, HTML\n",
    "# display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Files in DropBox for easy sharing Work and Personal laptop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/Robin/Dropbox/Data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon Fashion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Full data\n",
    "# file_name = 'amazon_clothing_shoes_jewelry_data' \n",
    "\n",
    "#2m user above 5 ratings\n",
    "# file_name = 'amazon_2m'\n",
    "\n",
    "#0.63m user above 5 ratings\n",
    "file_name = 'amazon_063m' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MovieLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full data\n",
    "# file_name = '25m_ml'\n",
    "\n",
    "# 2m subset\n",
    "# file_name = '2m-ml'\n",
    "# With 3.0 as rating threshold for a 1\n",
    "# file_name = '2m-ml_3_r_thres'\n",
    "# ml_07m subset\n",
    "# file_name = 'ml_07'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "      <th>rating</th>\n",
       "      <th>verified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A2OLY7TMIYHOQQ</td>\n",
       "      <td>B00EAKJUUW</td>\n",
       "      <td>5.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A3F6ZP5VM8QUC6</td>\n",
       "      <td>B00D98EGE6</td>\n",
       "      <td>5.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A21PFJA2O7Z5GY</td>\n",
       "      <td>B01DTEXSHA</td>\n",
       "      <td>2.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AV9HIUYXBZODJ</td>\n",
       "      <td>B0045DBUBQ</td>\n",
       "      <td>3.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A73X3PFCRTJVX</td>\n",
       "      <td>B00DEWBMU8</td>\n",
       "      <td>5.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             user        item  rating  verified\n",
       "0  A2OLY7TMIYHOQQ  B00EAKJUUW     5.0      True\n",
       "1  A3F6ZP5VM8QUC6  B00D98EGE6     5.0      True\n",
       "2  A21PFJA2O7Z5GY  B01DTEXSHA     2.0      True\n",
       "3   AV9HIUYXBZODJ  B0045DBUBQ     3.0      True\n",
       "4   A73X3PFCRTJVX  B00DEWBMU8     5.0      True"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'Data/'\n",
    "df = pd.read_pickle(path + file_name)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep\n",
    "Create new ids for users and items that match the row and column indices of the user-item interaction matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['item_id'] = df.item.astype('category').cat.codes\n",
    "df['user_id'] = df.user.astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115062"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.item_id.unique().max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leave item out train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leave_x_out(full_data, leave_out):\n",
    "    # Input: data must be formatted by func: tranfsorm\n",
    "    # Output: full_data = without all entries in leave one out set\n",
    "    #         leave_one_out_set = data with one user and one item from full_data\n",
    "    \n",
    "    full_data['index'] = full_data.index\n",
    "    user_items_ind = full_data.groupby('user_id')['index'].apply(list)\n",
    "    index_to_drop = []\n",
    "    \n",
    "    for indices in user_items_ind:\n",
    "        if len(indices) > leave_out:\n",
    "            for to_leave_out in range(leave_out):\n",
    "                index = indices[- to_leave_out]\n",
    "                index_to_drop.append(index)\n",
    "    \n",
    "    leave_one_out_set = full_data.loc[index_to_drop]\n",
    "    full_data_leave_one_out = full_data.drop(index_to_drop)\n",
    "    \n",
    "    return full_data_leave_one_out.drop(columns=['index']), leave_one_out_set.drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_matrices(data, n_users, n_items):\n",
    "        r = data['new_user_id']\n",
    "        c = data['new_item_id']\n",
    "        d = data['rating']\n",
    "        m = sparse.csr_matrix((d, (r, c)), shape=(n_users, n_items))\n",
    "        m_ones = m.copy()\n",
    "        m_ones[m_ones > 0] = 1\n",
    "                               \n",
    "        return m, m_ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = leave_x_out(df, 2)\n",
    "val_set, test_set = leave_x_out(test_set, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leave one item out per user \n",
      "\n",
      "Full Df length:\t\t 70745 \n",
      "Training Set length:\t 54343 \n",
      "Testing Set length:\t 8201 \n",
      "Validation Set length:\t 8201\n"
     ]
    }
   ],
   "source": [
    "print('Leave one item out per user',\n",
    "      '\\n\\nFull Df length:\\t\\t', len(df),\n",
    "      '\\nTraining Set length:\\t', len(train_set),\n",
    "      '\\nTesting Set length:\\t', len(test_set),\n",
    "      '\\nValidation Set length:\\t', len(val_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: BPR MF in TF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Paper: https://arxiv.org/pdf/1205.2618.pdf\n",
    "- Code:  https://github.com/valerystrizh/bpr/blob/master/BPR.java"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Init: Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_users = len(df.user_id.unique())\n",
    "total_items = len(df.item_id.unique())\n",
    "latent_dim = 20\n",
    "learning_rate = 0.05\n",
    "\n",
    "n_epochs = 10\n",
    "\n",
    "user_items = train_set.groupby('user_id')['item_id'].apply(list)\n",
    "train_users = train_set.user_id.unique()\n",
    "train_items = train_set.item_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Users: 34844 \n",
      "Total Items: 7545\n"
     ]
    }
   ],
   "source": [
    "print('Total Users:', total_users,\n",
    "      '\\nTotal Items:', total_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Define Model, Loss, Train Step and Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPR_MF(tf.keras.Model):\n",
    "    def __init__(self, init_func, total_users, total_items, latent_dim): #b_init_func\n",
    "        super(BPR_MF, self).__init__()\n",
    "        self.p = tf.Variable(init_func(total_users, latent_dim), name=\"p\")\n",
    "        self.q = tf.Variable(init_func(total_items, latent_dim), name=\"q\")\n",
    "#         self.b = tf.Variable(b_init_func((total_items, 1)), name=\"b\")\n",
    "    \n",
    "    def call(self, uij):\n",
    "        return tf.math.multiply(self.p[uij[0]], (self.q[uij[1]] - self.q[uij[2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    def __call__(self, x):\n",
    "        return - tf.math.log_sigmoid(tf.math.reduce_sum(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(model, optimizer, uij):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        x = model(uij) \n",
    "        loss = loss_obj(x) # -x or not? \n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_sample(train_users, train_items, user_items):\n",
    "    u = int(np.random.choice(train_users))\n",
    "    u_items = user_items[u]\n",
    "    i = random.choice(u_items)\n",
    "    j = int(np.random.choice(train_items)) # neg item\n",
    "\n",
    "    while j in u_items: # j cannot be the same item or an item with a 1\n",
    "        j = int(np.random.choice(train_items))\n",
    "        \n",
    "    return tf.Variable([u, i, j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU')]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init Model and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_init = lambda v, d: tf.random.normal((v, d), 0.0, 1.0/d)\n",
    "zero_init = lambda v: tf.zeros(v,1)\n",
    "\n",
    "model = BPR_MF(normal_init, total_users, total_items, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_obj = Loss()\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-106-e29893af5557>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mep_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0muij_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdraw_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_users\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_items\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step_manual\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muij_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-102-8af52c8c6dcb>\u001b[0m in \u001b[0;36mdraw_sample\u001b[0;34m(train_users, train_items, user_items)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_items\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/envs/re_research_m/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    258\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v1_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariableMetaclass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/re_research_m/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py\u001b[0m in \u001b[0;36m_variable_v2_call\u001b[0;34m(cls, initial_value, trainable, validate_shape, caching_device, name, variable_def, dtype, import_scope, constraint, synchronization, aggregation, shape)\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m         shape=shape)\n\u001b[0m\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/re_research_m/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(**kws)\u001b[0m\n\u001b[1;32m    233\u001b[0m                         shape=None):\n\u001b[1;32m    234\u001b[0m     \u001b[0;34m\"\"\"Call on Variable class. Useful to force the signature.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m     \u001b[0mprevious_getter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkws\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdefault_variable_creator_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkws\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creator_stack\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m       \u001b[0mprevious_getter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_getter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_getter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/re_research_m/lib/python3.7/site-packages/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mdefault_variable_creator_v2\u001b[0;34m(next_creator, **kwargs)\u001b[0m\n\u001b[1;32m   2643\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2644\u001b[0m       \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2645\u001b[0;31m       shape=shape)\n\u001b[0m\u001b[1;32m   2646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/re_research_m/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariableMetaclass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/re_research_m/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint, distribute_strategy, synchronization, aggregation, shape)\u001b[0m\n\u001b[1;32m   1409\u001b[0m           \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1410\u001b[0m           \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1411\u001b[0;31m           distribute_strategy=distribute_strategy)\n\u001b[0m\u001b[1;32m   1412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1413\u001b[0m   def _init_from_args(self,\n",
      "\u001b[0;32m/opt/anaconda3/envs/re_research_m/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m_init_from_args\u001b[0;34m(self, initial_value, trainable, collections, caching_device, name, dtype, constraint, synchronization, aggregation, distribute_strategy, shape)\u001b[0m\n\u001b[1;32m   1633\u001b[0m           \u001b[0mgraph_element\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgraph_element\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1634\u001b[0m           \u001b[0minitializer_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_initialized_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_initialized_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1635\u001b[0;31m           cached_value=cached_value)\n\u001b[0m\u001b[1;32m   1636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1637\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_init_from_proto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariable_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimport_scope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/re_research_m/lib/python3.7/site-packages/tensorflow_core/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    505\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 instructions)\n\u001b[0;32m--> 507\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     doc = _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[0;32m/opt/anaconda3/envs/re_research_m/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, trainable, shape, dtype, handle, constraint, synchronization, aggregation, distribute_strategy, name, unique_id, handle_name, graph_element, initial_value, initializer_op, is_initialized_op, cached_value, save_slice_info, handle_deleter, **unused_kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0mhandle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m     \"\"\"\n\u001b[0;32m--> 391\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_graph_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m     synchronization, aggregation, trainable = (\n",
      "\u001b[0;32m/opt/anaconda3/envs/re_research_m/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/re_research_m/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36minit_scope\u001b[0;34m()\u001b[0m\n\u001b[1;32m   5487\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5488\u001b[0m     \u001b[0;31m# Fastpath.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5489\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5490\u001b[0m       \u001b[0;32myield\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5491\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/re_research_m/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36mhelper\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhelper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_GeneratorContextManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhelper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/re_research_m/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, args, kwds)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0;31m# Issue 19330: ensure context manager instances have good docstrings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__doc__\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_time_s = time.time()\n",
    "total_losses = []\n",
    "for epoch in range(n_epochs):\n",
    "    ep_time_s = time.time()\n",
    "    ep_losses = []\n",
    "    for sample in range(len(train_set)):\n",
    "        uij_sample = draw_sample(train_users, train_items, user_items)\n",
    "        loss = train_step_manual(model, optimizer, uij_sample)\n",
    "        \n",
    "        ep_losses.append(loss)\n",
    "    ep_loss = np.average(ep_losses)\n",
    "    \n",
    "    print('Epoch:', epoch.numpy(), '\\tLoss:', round(ep_loss,4), '\\tEpoch Time:', time.time() - ep_time_s)\n",
    "    total_losses.append(ep_loss)\n",
    "\n",
    "print('Total training time:', time.time() - train_time_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD4CAYAAADvsV2wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3dd3SUZd7G8e9vJg1CIJSEFkIvIgSQ0ERQFBVFRbBhd1117eK6oq6777rVtawKduyrqCh2UVFwaaJAAAklQugJIIQWaiDJ3O8fySK4gSCT5JnMXJ9z5kgmd+a+mCPXPLmfZs45REQk/Pm8DiAiIlVDhS8iEiFU+CIiEUKFLyISIVT4IiIRIsrrAEfSoEED16JFC69jiIhUG3Pnzt3snEsq63shXfgtWrQgIyPD6xgiItWGma053Pe0pCMiEiFU+CIiEUKFLyISIVT4IiIRQoUvIhIhVPgiIhFChS8iEiFC+jj8YzV6cjb1a8XQJqkWbRsmUC8+xutIIiKeC7vCLyoO8ML0lewsKDrwXP34GFon16Jtci3aJNeibXICbRvWIjkhFjPzMK2ISNUJu8KP8vtY8H9nsGFHAdkbd7J8064Dj08zN5C/t/DA2ITYKNo0rFX6m0DJB0Gb5Fo0TayBz6cPAhEJL2FX+AA+n9E0sQZNE2twSvvkA88758jbte+QD4Hsjbv4z9I83p2be2BcXLSPNsm1DiwJtS79QGheryZRfu32EJHqKSwL/3DMjOSEOJIT4jixdYNDvrd9z/6fPgRKH3NWb+PD79cfGBPtN1o2iD/wm0Cb5JIPgpYN4omN8lf1X0dE5BeJqMI/ksSaMaS3qEd6i3qHPL97XxEr8kp+E8gu/UBYvD6fzxdtIFB6O2CfQfP68T99CJTuJ2idHE/NGL3FIhIa1EbliI+NIi0lkbSUxEOeLygsZtXm3Qc+BJZvKtlfMGXpJgqLf7oxfKukeLqmJJKWUoe0Zol0bFybuGj9NiAiVU+Ff4ziov0c17g2xzWufcjzhcUB1mzZw/JNu1i2cScL1+UzY/lm3p+/DoAon9GhcQJpKYl0SalDl2aJtE1OwK+dxCJSycw5V/4oj6Snp7twuR7+j/kFLMjdzoKc7WTm5pOZu50dpYeO1oj206lpbbqkJJLWrOSDILVeTR0yKiK/mJnNdc6ll/k9Fb43AgHH6i27yczNP/BBsHj9DvYVBQBIrBl94LeA//43uXacx6lFJNSp8KuJwuIAyzbuLPkQyNnOgtx8lm3cSXHp3uHGdeJK9gWkJNK1WSKdU+pQOy7a49QiEkqOVPhaww8h0X4fxzepw/FN6nBpz1QA9u4vZvH6fBaULgNl5uYzcfHGAz/TqkE8XZolHvggOL6JdgqLSNlU+CGuRoz/fw4Xzd9TSOa6kvL/Pmc7M1ds5oODdgq3b/TzncK1dMKYiAS/pGNm9YBxQAtgNXCxc25bGeMeBgZTcoXOr4A7XDmTR9qSTjD+u1P4v78FLMj5aadwfIyf87o24bKezemcUsfjpCJSmSp1Db+0yLc65/5pZvcCdZ1z9/xszInAI0D/0qdmAPc556Yc6bVV+MfOOcfqLXvIzN3O9OzNfJq5noLCAJ2b1uHyXqmc26UJ8bH6BU8k3FR24S8FTnHObTCzxsAU51z7n43pAzwFnAQYMA240jmXdaTXVuFXnPy9hXw4fx1jZ61h2cZd1IqNYmi3plzWK/V/ziUQkeqrsgt/u3Mu8aCvtznn6pYx7lHgOkoK/ynn3P2Heb0bgBsAUlNTu69ZsyaofHIo5xxz12xj7Ky1TFi4gf1FAbqlJnJZz1TOSWtCjRjt8BWpzoIufDObBDQq41v3A6+VV/hm1gYYBVxS+tRXwD3OuWlHmldb+JVr2+79vDcvlzdnr2Vl3m5qx0Ux7IQULu+VStuGCV7HE5FjEPRhmc65gUd48Y1m1vigJZ1NZQwbCnznnNtV+jOfA70pWdoRj9SNj+G6fq349Ukt+W7lVt6cvZaxs9bw6szV9GxRj8t6pTKoUyMd5ikSJiriWL2PgatL/3w18FEZY9YCJ5tZlJlFAycDR1y/l6pjZvRpXZ8nL+3Gt/edxr1ndWDjzgJGjPuePg9O5u8TlrAyb5fXMUUkSBWxhl8feAdIpaTYL3LObTWzdOBG59x1ZuYHnqHkKB0HfOGc+215r60lHe8EAo6ZK7YwdtYavlqykaKAo0+r+lzeO5UzOjYiJkrH9YuEIl1aQYKyaWcB72bk8uastazbvpcGtWK4sHszLuuZSmr9ml7HE5GDqPClQhQHHNOy83hz1lomZ20k4KBf2wZc3iuV045rSLTO5hXxnApfKtyG/L2Mm5PDuDk5bMgvIDkhlkt6NOOSHs1IqautfhGvqPCl0hQVB5iyNI+xs9YwZVkeAAPaJ3NZz1QGdEjWjV1EqpgKX6pE7rY9jJuTw9tzcsjbuY/GdeIY3iOVS3o0o1EdXctfpCqo8KVKFRYHmJy1kbGz1jI9ezN+n3Fqh2Su7N2cfm0b6E5eIpVI18OXKhXt9zGoU2MGdWrMmi27eWt2Du9m5PDVko2kN6/L3We2p1er+l7HFIk42sKXKrG/KMC7c3MYPTmbjTv20a9tA+4+sz1pKYnl/7CIHDUt6UjIKCgs5vVv1/DMlOVs21PIoOMbcdcZ7XTtHpEKosKXkLOzoJCXZqzixemr2L2/iKFdmzJiYDudyCUSJBW+hKxtu/fz3NQVvDpzNcUBx/Cezbjt1LY0rK2jekSOhQpfQt7GHQU8+XU2b8/Owe8zrj6xBTee3Jp68TFeRxOpVlT4Um2s3bKHJyYv44P564iPieLXJ7Xkun4tSYiL9jqaSLWgwpdqZ9nGnTz25TK+WPwjdWtGc9MprbmqTwtdm1+kHCp8qbYyc7fz6JfLmLYsj4a1Y7n11LZckt5Ml2cWOQwVvlR7s1Zu4dEvlzJn9Taa1avBnQPbMaRrU12rR+RnjlT42kySaqFXq/q885s+vPKrHtSOi+a37yxg0BPT+GLRBkJ5o0UklKjwpdowMwa0T+aTW0/imctPIOAcN74xjyFPf8O0ZXkqfpFyqPCl2vH5jLM7N2biiP48cmEaW3bt56qXZzN8zHdkrN7qdTyRkKU1fKn29hUVM25ODqMnL2fzrn0MaJ/EXWe0p1PTOl5HE6ly2mkrEWHP/iJem7mG56auIH9vIYPTGvPb09vROqmW19FEqkyl7bQ1s4vMbLGZBcyszAlKxw0ys6VmttzM7g1mTpHDqRkTxU2ntGbayAHcfmob/vPDJk5/bCp3v7uA3G17vI4n4rmgtvDN7DggADwP/M459z+b42bmB5YBpwO5wBzgUufckvJeX1v4EozNu/bx7JQVvP7dGpxzXN6rOTcPaE1ygq7TI+Gr0rbwnXNZzrml5QzrCSx3zq10zu0H3gaGBDOvyNFoUCuWP57Tkal3n8KF3Zvx+ndrOPnhKTz8xQ/sKCj0Op5IlauKo3SaAjkHfZ1b+lyZzOwGM8sws4y8vLxKDyfhr3GdGjw4rDOTf3syp3dsyDNTVnDKI1N49ZtV7C8KeB1PpMqUW/hmNsnMFpXxONqt9LJOhTzsOpJzboxzLt05l56UlHSUU4iUr0WDeEZf2o1PbzuJDo0SeOCTJZzx+FQ+X6iTtyQylHtPW+fcwCDnyAWaHfR1CrA+yNcUOWadmtZh7HW9mLI0jwc/z+KmsfPo3rwuvz+7A92b1/M6nkilqYolnTlAWzNraWYxwHDg4yqYV+SwzIwBHZL57PZ+/HNYZ3K27uGCZ7/lpjfmsnrzbq/jiVSKYA/LHGpmuUAfYIKZTSx9vomZfQbgnCsCbgUmAlnAO865xcHFFqkYUX4fw3umMuXuU7hzYDumLstj4GNTeeDjxWzdvd/reCIVSideiRxk084CnpiUzduz1xIfE8VNA1pzbd+Wug6/VBu6WqbIUUpOiOMfQzvz5Z396dWqHg9/sZRTH53Ce3NzCQRCd+NI5Gio8EXK0CY5gRev7sFb1/emQUIsd727gHOenMGM7M1eRxM5Zip8kSPo07o+H97cl1HDu7KjoJArXprF1S/P5ocfd3gdTeQXU+GLlMPnM4Z0bcrku07m/rOPY/7abZw9ajojxy/gx/wCr+OJHDXttBX5hbbv2c9TXy/n39+uweeD6/u14jcnt6ZWbLmntYhUOl0eWaQS5Gzdw8MTl/LJgvXUj49hxMC2DO+ZSrRfvziLd3SUjkglaFavJk9e2o2PbulL6+Ra/PGjxZz5xDQmLv5Rl2qQkKTCFwlSl2aJjLuhNy9clY4Bv3l9Lpc8/x3z127zOprIIVT4IhXAzDi9Y0MmjujP387vxMrNuxj6zExueXMea7fo5isSGrSGL1IJdu0rYszUFbwwfRVFgQBX9m7Bbae2oW58jNfRJMxpp62IRzbuKOCxL5fx7twcasVGccuANlx9YgtdqkEqjXbainikYe04Hrowjc/v6M8Jzevy4Oc/cNq/pvLh/HW6VINUORW+SBVo3yiBV3/Vk7HX9SKxZjQjxn3PZS9+x6YdOnFLqo4KX6QK9W3TgE9uPYl/DuvMgpx8zh49g5nLdX0eqRoqfJEq5vMZw3um8tGtfUmsGc3lL81i1KRsirXEI5VMhS/ikXYNE/jolr6c37Upj09axjWvzGbzrn1ex5IwpsIX8VB8bBSPXdyFfw7rzKxVWxk8ejqzV231OpaEKRW+iMfMSpZ4Prj5RGpE+7n0he94dsoKHcUjFU6FLxIijm9Sh09uO4lBxzfioS9+4Lp/Z7BN99WVCqTCFwkhCXHRPHVZN/4y5HimZ+cxePR05umaPFJBgip8M7vIzBabWcDMyj6V16yZmf3HzLJKx94RzJwi4c7MuKpPC9676UR8PuPi577lpRmrdAVOCVqwW/iLgGHAtCOMKQLucs4dB/QGbjGzjkHOKxL20lISmXBbPwZ0SOavny7hxjfmkr+30OtYUo0FVfjOuSzn3NJyxmxwzs0r/fNOIAtoGsy8IpGiTs1oxlzZnT8MPo7JWZs458npLMzN9zqWVFNVuoZvZi2AbsCsI4y5wcwyzCwjLy+vqqKJhCwz47p+rRj3mz4UFTsueHYmr3+7Wks88ouVW/hmNsnMFpXxGPJLJjKzWsB7wAjn3I7DjXPOjXHOpTvn0pOSkn7JFCJhrXvzuky4vR8ntqnPHz9azG1vzWfXviKvY0k1Uu5dl51zA4OdxMyiKSn7sc6594N9PZFIVS8+hpev7sGzU1fwry+XsmT9Dp6+/ASOa1zb62hSDVT6ko6ZGfASkOWce6yy5xMJdz6fccuANrx5fW927Svi/Ke/YdyctVrikXIFe1jmUDPLBfoAE8xsYunzTczss9JhfYErgVPN7PvSx9lBpRYRereqz4Tb+5Heoi73vLeQu95dwJ79WuKRw9Mdr0SqueKA48mvsxk1OZs2SbV49ooTaJOc4HUs8YjueCUSxvw+Y8TAdrx+bS+27t7PuU9+wwfzc72OJSFIhS8SJk5q24DP7uhH55Q63DluAfe9n0lBYbHXsSSEqPBFwkjD2nG8eV0vbj6lNW/NzmHoMzNZmbfL61gSIlT4ImEmyu9j5KAOvHJNDzbk7+W8p77h08z1XseSEKDCFwlTAzokM+H2frRrWItb35zPnz5axL4iLfFEMhW+SBhrmliDcb/pw/X9WvLat2u46Llvydm6x+tY4hEVvkiYi/b7uH9wR56/sjurNu/m7NHT+XLxj17HEg+o8EUixJnHN2LCbf1oUT+eG16fy98+XUJhccDrWFKFVPgiESS1fk3G39SHq/s058UZq7j4+W/ZuKPA61hSRVT4IhEmNsrPn4d04qnLurHsx50MH/Mdm1T6EUGFLxKhzklrwr9/3ZNNOwq49IXvyNu5z+tIUslU+CIRrHvzerzyq56s317AZS98x+ZdKv1wpsIXiXA9W9bj5Wt6kLNtD1e8OIutu/d7HUkqiQpfROjTuj4vXd2DVZt3c8WLs9i+R6UfjlT4IgJA3zYNGHNVOss37eLKl2aTv7fQ60hSwVT4InLAye2SeP7K7vzw4w6uenk2OwpU+uFEhS8ihxjQIZlnL+/O4nX5XPPybN0oPYyo8EXkfwzs2JCnLuvGgtx8fvXKbHar9MOCCl9EyjSoU2NGD+/GvLXbufbVObpfbhhQ4YvIYQ1Oa8xjF3dhzuqtXPdahu6gVc0FVfhmdpGZLTazgJmVedPcg8b6zWy+mX0azJwiUrWGdG3Koxd14duVW7j+3yr96izYLfxFwDBg2lGMvQPICnI+EfHAsBNSeOiCNKZnb+bGN+bqRirVVFCF75zLcs4tLW+cmaUAg4EXg5lPRLxzcXozHhzWmSlL87hl7Dz2F+nSytVNVa3hPwGMBMr9P8TMbjCzDDPLyMvLq/xkInLULu2Zyl/P78SkrE3c9tY8XU+/mim38M1skpktKuMx5GgmMLNzgE3OublHM945N8Y5l+6cS09KSjqaHxGRKnRl7+Y8cG5HJi7eyIi3v6dIpV9tRJU3wDk3MMg5+gLnmdnZQBxQ28zecM5dEeTriohHrunbkqKA428TsvD5jMcv7kKUXwf9hbpyCz9Yzrn7gPsAzOwU4Hcqe5Hq77p+rSgKOP75+Q9E+YxHL+qC32dex5IjCPawzKFmlgv0ASaY2cTS55uY2WcVEVBEQteNJ7fm7jPb88H8ddzzXiaBgPM6khxBUFv4zrkPgA/KeH49cHYZz08BpgQzp4iEllsGtKGwOMATk7KJ8hn/GNoZn7b0Q1KlL+mISPi747S2FAccT369HL/P+Nv5nTBT6YcaFb6IBM3M+O3p7Sgsdjw3dQVRPuOB845X6YcYFb6IVAgz455B7SkqDvDijFVE+X38YfBxKv0QosIXkQpjZtw/+DiKAo6XZqwiymfce1YHlX6IUOGLSIUyM/50bkeKA47np60kym/87oz2Kv0QoMIXkQpnZvz5vOMpCjie/s8Konw+7jy9ndexIp4KX0Qqhc9n/P38ThQHAoyaXHLI5m2ntfU6VkRT4YtIpfH5jAeHpVEUcPzrq2X4/cbNp7TxOlbEUuGLSKXy+4xHLuxCccDx8BdLifb5uL5/K69jRSQVvohUOr/P+NdFXSgKOP7+WRZ+n3HtSS29jhVxVPgiUiWi/D6euKQrgYDjL58uIcpvXNWnhdexIoquZyoiVSba72PU8G4MPK4h//fRYt6ctdbrSBFFhS8iVSomysfTl3fj1A7J/P6DhbwzJ8frSBFDhS8iVS42ys8zl59A/3ZJ3PN+JuPn5nodKSKo8EXEE3HRfsZc2Z2+rRtw9/gFPPbVMt0jt5Kp8EXEM3HRfl64Kp2hXZsyenI2w56ZyfJNO72OFbZU+CLiqRoxfh67pCvPXn4Cudv2MHj0DF6esUp3z6oEKnwRCQlndW7MxDv7c1KbBvzl0yVc8dIs1m3f63WssKLCF5GQkZwQx4tXp/PPYZ1ZkLOdQY9P4/15uTinrf2KoMIXkZBiZgzvmcrnd/SnQ+MEfvvOAm56Yx5bdu3zOlq1F1Thm9lFZrbYzAJmln6EcYlmNt7MfjCzLDPrE8y8IhL+UuvX5O0b+nDvWR34+odNnPnEdCZnbfQ6VrUW7Bb+ImAYMK2ccaOAL5xzHYAuQFaQ84pIBPD7jBtPbs1Ht/alQa0Yfv1aBve+l8mufUVeR6uWgip851yWc27pkcaYWW2gP/BS6c/sd85tD2ZeEYksxzWuzUe39uXGk1szLiOHs0ZNY/aqrV7HqnaqYg2/FZAHvGJm883sRTOLP9xgM7vBzDLMLCMvL68K4olIdRAb5efeszrwzm/6YBiXjPmWBz/PYl9RsdfRqo1yC9/MJpnZojIeQ45yjijgBOBZ51w3YDdw7+EGO+fGOOfSnXPpSUlJRzmFiESKHi3q8fkd/RjeI5Xnp65kyFPfsGT9Dq9jVQvlFr5zbqBzrlMZj4+Oco5cINc5N6v06/GUfACIiByT+NgoHhzWmZevSWfzrv0MeXoGz0xZTrFO1jqiSl/Scc79COSYWfvSp04DllT2vCIS/k7t0JAv7+zP6R0b8vAXS7n4+W9Zs2W317FCVrCHZQ41s1ygDzDBzCaWPt/EzD47aOhtwFgzywS6Av8IZl4Rkf+qFx/D05edwOOXdGHZxp2cNWo6b85aq5O1ymCh/Kakp6e7jIwMr2OISDWxfvte7h6/gG+Wb2FA+yQeuiCN5NpxXseqUmY21zlX5nlROtNWRMJGk8QavH5tLx44tyMzV2zhzCem8dnCDV7HChkqfBEJKz6fcU3flky4vR+p9Wpy89h5jHh7Pvl7Cr2O5jkVvoiEpTbJtRh/04mMGNiWTzI3cOYT05iRvdnrWJ5S4YtI2Ir2+xgxsB0f3Hwi8bF+rnhpFg98vJi9+yPzZC0VvoiEvbSURCbc3o9rTmzBqzNXM/jJ6SzIibwrvKjwRSQixEX7eeC84xl7XS/27i9m2LMzeTzC7qOrwheRiNK3TQO+GNGfIV2aMGpyNhc8O5Plm3Z5HatKqPBFJOLUqRF94D66OVv3MHj09Ii4j64KX0Qi1n/vo9u39D66V708m4LC8N2hq8IXkYiWnBDHS1en89fzOzFj+WZGT872OlKlUeGLSMQzM67s3ZwLu6fw/LSVLFqX73WkSqHCFxEp9cfBHakXH8PI8ZlhefSOCl9EpFSdmtH8dUgnlmzYwfNTV3gdp8Kp8EVEDjKoUyMGd27M6MnLyd640+s4FUqFLyLyMw+cdzw1Y/2MfC8zrO6ipcIXEfmZpIRY/nRuR+av3c6rM1d7HafCqPBFRMpwftemDGifxKMTl7J2yx6v41QIFb6ISBnMjL8P7YzfZ9z7fmZY3DJRhS8ichhNEmtw39kdmLliC2/PyfE6TtBU+CIiR3Bpj1R6t6rHPyZk8WN+gddxghJU4ZvZRWa22MwCZlbmTXNLx91ZOm6Rmb1lZpF1V2ERqbZ8PuOhC9IoDAT4w4cLq/XSTrBb+IuAYcC0ww0ws6bA7UC6c64T4AeGBzmviEiVaV4/nt+d0Z5JWZv4eMF6r+Mcs6AK3zmX5ZxbehRDo4AaZhYF1ASq7zsmIhHpV31b0rVZIn/+ZAlbdu3zOs4xqfQ1fOfcOuBRYC2wAch3zn15uPFmdoOZZZhZRl5eXmXHExE5Kn6f8fCFaewsKOSBT5Z4HeeYlFv4ZjapdO39548hRzOBmdUFhgAtgSZAvJldcbjxzrkxzrl051x6UlLS0f49REQqXbuGCdx2als+WbCer5Zs9DrOLxZV3gDn3MAg5xgIrHLO5QGY2fvAicAbQb6uiEiVu/Hk1ny2cAP3f7CQni3rUadGtNeRjlpVHJa5FuhtZjXNzIDTgKwqmFdEpMLFRPl4+MI0Nu/axz8mVK8qC/awzKFmlgv0ASaY2cTS55uY2WcAzrlZwHhgHrCwdM4xQaUWEfFQWkoi1/dvxbiMHGZkb/Y6zlGzUD6mND093WVkZHgdQ0TkfxQUFnPWqOkUFgeYOKI/8bHlrpBXCTOb65wr87wonWkrInIM4qL9PHRBGrnb9vLIxKM5Ot17KnwRkWPUs2U9rurTnNe+XU3G6q1exymXCl9EJAgjB3WgSZ0ajHwvk4LCYq/jHJEKX0QkCLVio3hwWGdW5u1m9ORsr+MckQpfRCRI/dslcWH3FJ6ftpJF6/K9jnNYKnwRkQrwx8EdqRcfw8jxmRQWB7yOUyYVvohIBahTM5q/DunEkg07GDNtpddxyqTCFxGpIIM6NWJw58aMmpTN8k07vY7zP1T4IiIV6IHzjqdmrJ+R4zMpDoTWia0qfBGRCpSUEMufzu3IvLXbeW3maq/jHEKFLyJSwc7v2pQB7ZN4ZOJS1m7Z43WcA1T4IiIVzMz4+9DO+H3GfR9khsx9cFX4IiKVoEliDe47uwPfLN/CuDk5XscBVPgiIpXm0h6p9GpZj79PyOLH/AKv46jwRUQqi89nPHRBGoWBAH/4cKHnSzsqfBGRStSiQTx3nd6eSVmb+HjBek+zqPBFRCrZtSe1pEuzRP78yRK27NrnWQ4VvohIJfP7jEcuTGNnQSEPfLLEsxwqfBGRKtCuYQK3DmjLJwvW89WSjZ5kUOGLiFSRm05pTYdGCfzhw4Xk7y2s8vmDKnwze8TMfjCzTDP7wMwSDzNukJktNbPlZnZvMHOKiFRXMVE+Hr4wjbyd+3jws6wqnz/YLfyvgE7OuTRgGXDfzweYmR94GjgL6AhcamYdg5xXRKRaSktJ5Pr+rXh7Tg7fLN9cpXMHVfjOuS+dc0WlX34HpJQxrCew3Dm30jm3H3gbGBLMvCIi1dmdA9vRskE8976fyZ79ReX/QAWpyDX8a4HPy3i+KXDwecW5pc+VycxuMLMMM8vIy8urwHgiIqEhLtrPQxekkbN1L49MXFpl85Zb+GY2ycwWlfEYctCY+4EiYGxZL1HGc4c93cw5N8Y5l+6cS09KSjqav4OISLXTs2U9rurTnFdnrmbumq1VMmdUeQOccwOP9H0zuxo4BzjNlX3ecC7Q7KCvUwBvTzcTEQkBIwd1YHLWJkaOz2TC7f2Ii/ZX6nzBHqUzCLgHOM85d7iLPs8B2ppZSzOLAYYDHwczr4hIOKgVG8WDwzqzIm83T36dXenzBbuG/xSQAHxlZt+b2XMAZtbEzD4DKN2peyswEcgC3nHOLQ5yXhGRsNC/XRIXdk/huakrWbQuv1LnMq+v3nYk6enpLiMjw+sYIiKVKn9PIQMfn0pSrVg+urUv0f5j3xY3s7nOufSyvqczbUVEPFanZjR/HdKJJRt2MGbaykqbR4UvIhICBnVqxNmdGzFqUjbLN+2slDlU+CIiIeLP53WiZqyfkeMzKQ5U/HK7Cl9EJEQkJcTyp3M70rFJbQqLAxX++uUehy8iIlVnaLcUhnYr6yo1wdMWvohIhFDhi4hECBW+iEiEUOGLiEQIFb6ISIRQ4YuIRAgVvohIhFDhi4hEiJC+WqaZ5QFrjvHHGwBVe4fg0KX34lB6Pw6l9+Mn4fBeNHfOlXm7wJAu/GCYWcbhLhEaafReHErvx6H0fvwk3N8LLemIiEQIFb6ISIQI58If43WAEKL34lB6Pw6l98nxv8UAAAKeSURBVOMnYf1ehO0avoiIHCqct/BFROQgKnwRkQgRdoVvZoPMbKmZLTeze73O4yUza2Zm/zGzLDNbbGZ3eJ3Ja2bmN7P5Zvap11m8ZmaJZjbezH4o/X+kj9eZvGRmd5b+O1lkZm+ZWZzXmSpaWBW+mfmBp4GzgI7ApWbW0dtUnioC7nLOHQf0Bm6J8PcD4A4gy+sQIWIU8IVzrgPQhQh+X8ysKXA7kO6c6wT4geHepqp4YVX4QE9guXNupXNuP/A2MMTjTJ5xzm1wzs0r/fNOSv5BN/U2lXfMLAUYDLzodRavmVltoD/wEoBzbr9zbru3qTwXBdQwsyigJrDe4zwVLtwKvymQc9DXuURwwR3MzFoA3YBZ3ibx1BPASKDi7w5d/bQC8oBXSpe4XjSzeK9DecU5tw54FFgLbADynXNfepuq4oVb4VsZz0X8cadmVgt4DxjhnNvhdR4vmNk5wCbn3Fyvs4SIKOAE4FnnXDdgNxCx+7zMrC4lqwEtgSZAvJld4W2qihduhZ8LNDvo6xTC8NeyX8LMoikp+7HOufe9zuOhvsB5ZraakqW+U83sDW8jeSoXyHXO/fc3vvGUfABEqoHAKudcnnOuEHgfONHjTBUu3Ap/DtDWzFqaWQwlO10+9jiTZ8zMKFmjzXLOPeZ1Hi855+5zzqU451pQ8v/F1865sNuCO1rOuR+BHDNrX/rUacASDyN5bS3Q28xqlv67OY0w3Ikd5XWAiuScKzKzW4GJlOxlf9k5t9jjWF7qC1wJLDSz70uf+71z7jMPM0nouA0YW7pxtBL4lcd5POOcm2Vm44F5lBzdNp8wvMyCLq0gIhIhwm1JR0REDkOFLyISIVT4IiIRQoUvIhIhVPgiIhFChS8iEiFU+CIiEeL/AeU6VnQNK5FaAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(total_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.q[1001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BPR_MF(normal_init, total_users, total_items, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01688648,  0.0505784 , -0.02644658, ..., -0.01099488,\n",
       "         0.01813504, -0.07018466],\n",
       "       [ 0.02288403, -0.02241064,  0.06765871, ...,  0.02549377,\n",
       "        -0.00330726,  0.05510823],\n",
       "       [ 0.02313883,  0.0388522 ,  0.11835849, ..., -0.02955933,\n",
       "         0.07207369, -0.01683548],\n",
       "       ...,\n",
       "       [ 0.03210168,  0.03930054, -0.05666804, ...,  0.04753113,\n",
       "         0.01625811,  0.04558265],\n",
       "       [-0.02696693,  0.06174047,  0.00139138, ..., -0.02127414,\n",
       "        -0.02689567,  0.01205104],\n",
       "       [-0.00726422, -0.0592706 ,  0.03855373, ...,  0.00179589,\n",
       "         0.02941306,  0.09110733]], dtype=float32)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.q.numpy()\n",
    "model.p.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train(opt, u, i, j):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        pu = p_full[u]\n",
    "        qi = q_full[i]\n",
    "        qj = q_full[j]\n",
    "        x = tf.math.multiply(pu, (qi - qj))\n",
    "        loss = loss_obj(x)\n",
    "    opt.minimize(loss_obj, )\n",
    "    gradients = [tape.gradient(loss, pu), tape.gradient(loss, qi), tape.gradient(loss, qj)]    \n",
    "#     gradients = tape.gradient(x, [pu, qi, qj])\n",
    "    print(gradients[0])\n",
    "    grads_and_vars = zip(gradients, [pu, qi, qj])\n",
    "#     opt.apply_gradients(grads_and_vars)\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = train(opt, u, i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qi - qj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dx_dqj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dx_dpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_full[i] - q_full[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def add(a, b):\n",
    "    c = tf.math.add(a, b)\n",
    "    for i in tf.range(10):\n",
    "        c = tf.math.add(a, c)\n",
    "        tf.print(tf.math.add(a,c))\n",
    "\n",
    "\n",
    "a = tf.Variable([1,2,3])\n",
    "b = tf.Variable([3,2,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def f(x,y):\n",
    "    result = tf.constant(10.0)\n",
    "    for i in tf.range(y):\n",
    "        result = tf.math.multiply(x, y)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(x, y):\n",
    "    with tf.GradientTape() as t:\n",
    "        t.watch(x)\n",
    "        out = f(x,y)\n",
    "    return t.gradient(out, tf.dtypes.cast(y, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.convert_to_tensor(2.0)\n",
    "y = tf.convert_to_tensor(3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grad(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.ones((2,2))\n",
    "\n",
    "with tf.GradientTape() as t:\n",
    "    t.watch(x)\n",
    "    y = tf.reduce_sum(x)\n",
    "    z = tf.multiply(y, y)\n",
    "\n",
    "# Use the tape to compute the derivative of z with respect to the\n",
    "# intermediate value y.\n",
    "dz_dy = t.gradient(z, y)\n",
    "print(dz_dy)\n",
    "print(dz_dy.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant(3.0)\n",
    "with tf.GradientTape(persistent=True) as t:\n",
    "  t.watch(x)\n",
    "  y = x * x\n",
    "  z = y * y\n",
    "dz_dx = t.gradient(z, x)  # 108.0 (4*x^3 at x = 3)\n",
    "dy_dx = t.gradient(y, x)  # 6.0\n",
    "del t  # Drop the reference to the tape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dz_dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPR_MF(tf.keras.Model):\n",
    "    def __init__(self, init_func, b_init_func, total_users, total_items, latent_dim):\n",
    "        super(BPR_MF, self).__init__()\n",
    "        self.p = tf.Variable(init_func(total_users, latent_dim), name=\"p\")\n",
    "        self.q = tf.Variable(init_func(total_items, latent_dim), name=\"q\")\n",
    "        self.b = tf.Variable(b_init_func((total_items, 1)))\n",
    "    \n",
    "    def call(self, u, i, j):\n",
    "        return self.b[i] - self.b[j] + tf.matmul(self.p[u], (self.q[i] - self.q[j]), transpose_b=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_init = lambda v, d: tf.random.normal((v, d), 0.0, 1.0/d)\n",
    "zero_init = lambda v: tf.zeros(v,1)\n",
    "model = BPR_MF(init_func = normal_init,\n",
    "               b_init_func = zero_init,\n",
    "               total_users = total_users,\n",
    "               total_items = total_items,\n",
    "               latent_dim = latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "#     def __init__(self):\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return tf.math.log_sigmoid(-x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_obj = Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_step():\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=0.1)\n",
    "    train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(scope_model):\n",
    "        with tf.GradientTape() as tape:\n",
    "            xhat = scope_model(u=1, i=1, j=2)\n",
    "            loss = tf.reduce_sum(loss_obj(xhat))\n",
    "        gradients = tape.gratients(loss, scope_model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, scope_model.trainable_variables))\n",
    "        train_loss(loss)\n",
    "\n",
    "    return train_loss, train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "n_iters = 500 + 1\n",
    "print_every = 50\n",
    "for i in range(n_iters): \n",
    "    train_step(model)\n",
    "    res = train_loss.result()  \n",
    "    results.append(res)\n",
    "\n",
    "    if i % print_every == 0:\n",
    "        print('help')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_pickle('Data/amazon_clothing_shoes_jewelry_data')\n",
    "# users = df.user.unique()\n",
    "# to_keep = users[:300000]\n",
    "\n",
    "# user_indices = df.groupby('user')['index'].apply(list)\n",
    "# to_keep_indices = []\n",
    "# for u in user_indices[to_keep]:\n",
    "#     to_keep_indices.extend(u)\n",
    "\n",
    "# new_df = df_og.loc[to_keep_indices]\n",
    "# len(to_keep_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin ordered tf BPR_MF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPR_MF():    \n",
    "    def __init__(self, total_users, total_items, latent_dim):\n",
    "        self.total_users = total_users\n",
    "        self.total_items = total_items\n",
    "        self.latent_dim = latent_dim\n",
    "        self.p = tf.random.normal(shape=(total_users, latent_dim), mean=0.0, stddev=0.1)\n",
    "        self.q = tf.random.normal(shape=(total_items, latent_dim), mean=0.0, stddev=0.1)\n",
    "        self.b = tf.zeros(shape=(total_items, 1))\n",
    "        self.train_users = None\n",
    "        self.train_items = None\n",
    "        self.user_items = None\n",
    "\n",
    "    def get_train_batch(self, user):\n",
    "        item_list_user = self.user_items[user]\n",
    "        i = np.random.choice(item_list_user)\n",
    "        j = np.random.choice(self.train_items)\n",
    "        while j in item_list_user: # j cannot be an item liked by user\n",
    "            j = np.random.choice(self.train_items)\n",
    "            \n",
    "        return [int(user), int(i), int(j)]\n",
    "        \n",
    "    def update(self, x):\n",
    "        \n",
    "    \n",
    "    def train(self, u, i, j):\n",
    "        x = self.b[i] - self.b[j] + tf.math.reduce_sum(tf.math.multiply(self.p[u], (self.q[i] - self.q[j])))\n",
    "        loss = - tf.math.log(tf.math.sigmoid(x))\n",
    "        self.update(x)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def fit(self, train_data, epochs):\n",
    "        self.train_users = train_data.user_id.unique()\n",
    "        self.train_items = train_data.item_id.unique()\n",
    "        self.user_items = train_data.groupby('user_id')['item_id'].apply(list)\n",
    "        for epoch in range(epochs):\n",
    "            epoch_losses = []\n",
    "            for pos_examples in range(len(train_data)): # number of positive examples\n",
    "                uij = self.get_train_batch(np.random.choice(self.train_users))\n",
    "                epoch_losses.append(self.train(*uij))\n",
    "            \n",
    "            epoch_loss = np.average(epoch_losses)\n",
    "            print('Epoch:', epoch, 'Loss:', epoch_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## standard SVD model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVD:\n",
    "    \"\"\"\" All functions used to run, test, plot and store the\n",
    "    Singular Value Decomposition Model\"\"\"\n",
    "\n",
    "    def __init__(self, params, total_users, total_items):\n",
    "        self.nolf = params['nolf']\n",
    "        self.n_epochs = params['n_epochs']\n",
    "        self.alpha = params['alpha']\n",
    "        self.alpha_b = params['alpha_b']\n",
    "        self.alpha_cb = params['alpha_cb']\n",
    "        self.use_bias = params['use_bias']\n",
    "        self.use_impl_fb = params['use_impl_fb']\n",
    "        self.use_color = params['use_color']\n",
    "        self.use_weight_ver = params['use_weight_ver']\n",
    "        self.bu_reg = params['bu_reg']\n",
    "        self.bi_reg = params['bi_reg']\n",
    "        self.pu_reg = params['pu_reg']\n",
    "        self.qi_reg = params['qi_reg']\n",
    "        self.x_reg = params['x_reg']\n",
    "        self.cb_reg = params['cb_reg']\n",
    "        self.ver_weight = params['ver_weight']\n",
    "        self.stop = params['stop']\n",
    "        self.random_state = params['random_state']\n",
    "        self.total_users = total_users\n",
    "        self.total_items = total_items\n",
    "        self.params = params\n",
    "        self.mu = 0 \n",
    "        self.N = []\n",
    "        self.N_test = []\n",
    "        self.t = pd.DataFrame()\n",
    "        self.c = pd.DataFrame()\n",
    "        self.F = pd.DataFrame()\n",
    "\n",
    "        self.train_data = pd.DataFrame()\n",
    "        self.test_data = pd.DataFrame()\n",
    "        self.val_data = pd.DataFrame()\n",
    "        self.train_time = 0\n",
    "        self.best_model = {}\n",
    "        self.model = {}\n",
    "        self.test_results = {}\n",
    "\n",
    "    def fit(self, train_data, val_data=[], verbose=1, plot=True, plot_name=''):\n",
    "        self.train_data = train_data\n",
    "        self.val_data = val_data\n",
    "        self.SVD(train_data=train_data, val_data=val_data, verbose=verbose, plot=plot, plot_name=plot_name)\n",
    "        return self\n",
    "\n",
    "    \n",
    "###############################################################################################\n",
    "    \n",
    "    def SVD(self, train_data, val_data, verbose, plot, plot_name):\n",
    "        \"\"\"\"The SVD algorithm with sgd\n",
    "        input: rating dataset with columns:['rating', 'user_id', 'item_id']\n",
    "        output: the resulting p, q, bi, bu matrices\"\"\"\n",
    "        self.mu = self.create_mu(train_data)\n",
    "        train_matrix = self.create_matrix(train_data, self.total_users, self.total_items)\n",
    "        \n",
    "        tuples_train = [tuple(x) for x in train_data[['new_user_id', 'new_item_id', 'rating']].to_numpy()]\n",
    "        \n",
    "        p = np.random.normal(0, .1, (total_users, self.nolf))  # users\n",
    "        q = np.random.normal(0, .1, (total_items, self.nolf))  # items\n",
    "        \n",
    "        # user and item biases\n",
    "        b_user = np.zeros(total_users)\n",
    "        b_item = np.zeros(total_items)\n",
    "        \n",
    "        # using color (pareto split (0,1,2)) attribute bias\n",
    "        if self.use_color:\n",
    "            print('Creating F and c, for incorporating color bias')\n",
    "            self.F, self.c = self.init_color(train_data)\n",
    "\n",
    "        # implicit fb rated, not rated\n",
    "        x = np.random.normal(0, .1, (total_items, self.nolf))\n",
    "        impl_fb_u = np.zeros(self.nolf)\n",
    "        if self.use_impl_fb:\n",
    "            print('Creating N, for incorporating implicit feedback')\n",
    "            self.N = train_data.groupby('new_user_id')['new_item_id'].apply(list)\n",
    "        \n",
    "        # 0.5 weight on the errors of verified = False user item combinations\n",
    "        if self.use_weight_ver:\n",
    "            i_verified = train_data.set_index(['new_user_id', 'new_item_id'])['verified']\n",
    "            i_verified = i_verified.loc[~i_verified.index.duplicated(keep='first')]\n",
    "        \n",
    "        sqrt_Nu = 0\n",
    "        cb = 0\n",
    "        rmses = []\n",
    "        val_rmses = []\n",
    "        smallest_val_rmse = 10000\n",
    "        val_rmse = \"na\"\n",
    "        start = time.time()\n",
    "        for epoch in range(self.n_epochs):\n",
    "            total_sq_error = 0\n",
    "            for u, i, r_ui in tuples_train:\n",
    "                u = int(u)\n",
    "                i = int(i)\n",
    "                \n",
    "                if self.use_impl_fb:\n",
    "                    impl_fb_u = np.zeros(self.nolf)\n",
    "                    sqrt_Nu = np.sqrt(len(self.N[u]))\n",
    "                    for j in self.N[u]:\n",
    "                        impl_fb_u += x[j] / sqrt_Nu\n",
    "\n",
    "                if self.use_color and epoch > 5:\n",
    "                    F_ui =  self.F[u,i] #Set of items associated with i and rated by u\n",
    "                    u_mu = self.mu + b_user[u]\n",
    "                    sqrt_F_ui = np.sqrt(len(F_ui))\n",
    "                    for index, f in enumerate(F_ui):\n",
    "                        r_uf = train_data[(train_data['new_user_id']==u) & (train_data['new_item_id']==f)]['rating'].iloc[0]\n",
    "                        cb += (r_uf - u_mu) * self.c[u,i][index]\n",
    "                    cb /=  sqrt_F_ui\n",
    "                        \n",
    "                if self.use_bias:   \n",
    "                    error = r_ui - ((self.mu + b_user[u] + b_item[i] + cb) + np.dot(p[u] + impl_fb_u, q[i]))\n",
    "                    if self.use_weight_ver and not i_verified[u,i]:\n",
    "                        error = self.ver_weight * error\n",
    "                    \n",
    "                    b_user[u] += self.alpha_b * (error - self.bu_reg * b_user[u])\n",
    "                    b_item[i] += self.alpha_b * (error - self.bi_reg * b_item[i])\n",
    "                else:\n",
    "                    error = r_ui - np.dot(p[u], q[i])\n",
    "\n",
    "                p[u] += self.alpha * (error * q[i] - self.pu_reg * p[u])\n",
    "                q[i] += self.alpha * (error * (p[u] + impl_fb_u) - self.qi_reg * q[i])\n",
    "                total_sq_error += np.square(error)\n",
    "            \n",
    "                if self.use_impl_fb:\n",
    "                    for j in self.N[u]:\n",
    "                        x[j] += self.alpha * (error * q[i] / sqrt_Nu - self.x_reg * x[j])\n",
    "                \n",
    "                if self.use_color and epoch > 5:\n",
    "                    for index, f in enumerate(F_ui):\n",
    "                        r_uf = train_data[(train_data['new_user_id']==u) & (train_data['new_item_id']==f)]['rating'].iloc[0]\n",
    "                        u_mu = self.mu + b_user[u]\n",
    "                        self.c[u,i][index] += self.alpha_cb * (error * (1/sqrt_F_ui) * (r_uf - u_mu) - self.cb_reg * self.c[u,i][index])\n",
    "                \n",
    "            rmse = np.sqrt(total_sq_error / len(tuples_train))\n",
    "            rmses.append(rmse)\n",
    "            \n",
    "            self.model = {'p': p, 'q': q, 'bu':b_user, 'bi':b_item, 'cbu': self.c, 'x':x, 'rmse':rmses, 'val_rmse':val_rmses}\n",
    "            \n",
    "            # Validation\n",
    "            if len(val_data) > 0:\n",
    "                new_val_rmse = self.test(val_data, val=True)\n",
    "                val_rmses.append(new_val_rmse)\n",
    "                if new_val_rmse < smallest_val_rmse:\n",
    "                    smallest_val_rmse = new_val_rmse\n",
    "                    self.best_model = copy.deepcopy(self.model)\n",
    "                val_rmse = new_val_rmse\n",
    "                \n",
    "            # Epoch Printing\n",
    "            if epoch % verbose == 0:\n",
    "                if len(val_data) > 0:\n",
    "                    print('Epoch:', epoch, '  RMSE:', rmse, ' Val_RMSE:', val_rmse)\n",
    "                else:\n",
    "                    print('Epoch:', epoch, '  RMSE:', rmse)\n",
    "            \n",
    "            if self.stop and val_rmses[-2:][0] < val_rmse:\n",
    "                print('BREAK: Validation set not improving anymore')\n",
    "                break\n",
    "                \n",
    "        if plot:\n",
    "            self.plot_rmse(rmses, val_rmses, plot_name)\n",
    "\n",
    "        self.train_time = time.time() - start\n",
    "        self.model = {'p': p, 'q': q, 'bu':b_user, 'bi':b_item, 'cbu': self.c, 'x':x, 'rmse':rmses, 'val_rmse':val_rmses}\n",
    "#################################################################################################\n",
    "\n",
    "    def init_color(self, data_set):\n",
    "        self.t = data_set.groupby(['new_user_id', 'par_col2'])['new_item_id'].apply(list)\n",
    "        F = data_set.groupby(['new_user_id', 'new_item_id'])['par_col2'].apply(self.sim_items)\n",
    "        c = data_set.groupby(['new_user_id', 'new_item_id'])['par_col2'].apply(self.sim_items, random=True)\n",
    "        return F, c\n",
    "\n",
    "    def sim_items(self, x, random=False):\n",
    "        u_id = x.name[0]\n",
    "        col = x.iloc[0]\n",
    "        if random:\n",
    "            return np.random.normal(0,.1,len(self.t[u_id, col]))\n",
    "        return self.t[u_id, col]\n",
    "    \n",
    "    def create_matrix(self, X_train, n_users, n_items):\n",
    "        r = X_train['new_user_id']\n",
    "        c = X_train['new_item_id']\n",
    "        d = X_train['rating']\n",
    "        train_matrix = sparse.coo_matrix((d, (r, c)), shape=(n_users, n_items))\n",
    "    \n",
    "        return train_matrix.tocsr()\n",
    "    \n",
    "    def create_mu(self, train_set):\n",
    "        # Better mean calculation according to https://sifter.org/~simon/journal/20061211.html\n",
    "        va = train_set.groupby('new_user_id')['rating'].mean().var() #variance mean ratings users\n",
    "        vb = train_set.groupby('new_item_id')['rating'].mean().var() #variance mean ratings items\n",
    "        k = va/vb #variance proportion\n",
    "        better_mu = (train_set['rating'].mean() + train_set['rating'].sum()) / (k+len(train_set))\n",
    "        return better_mu\n",
    "    \n",
    "    def plot_rmse(self, rmse, val_rmses=[], plot_name=''):\n",
    "        plt.plot(np.arange(len(rmse)), rmse)\n",
    "        if len(val_rmses) > 0:\n",
    "            plt.plot(np.arange(len(val_rmses)), val_rmses, color='red')\n",
    "        plt.title('RMSE')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('RMSE')\n",
    "        plt.legend(['Train', 'Validation'])\n",
    "        if len(plot_name) > 0:\n",
    "            plt.savefig('Plots/' + plot_name + '.png')\n",
    "        plt.show()\n",
    "\n",
    "    def test(self, test_data, val=False):\n",
    "        if not val:\n",
    "            self.test_data = test_data\n",
    "        tuples_test = [tuple(x) for x in test_data[['new_user_id', 'new_item_id', 'rating']].to_numpy()]\n",
    "        test_matrix = self.create_matrix(test_data, self.total_users, self.total_items)\n",
    "        \n",
    "        if self.use_impl_fb and val:\n",
    "            self.N_test = self.val_data.groupby('new_user_id')['new_item_id'].apply(list)\n",
    "        elif self.use_impl_fb:\n",
    "            self.N_test = self.test_data.groupby('new_user_id')['new_item_id'].apply(list)\n",
    "            \n",
    "        total_error = 0\n",
    "        estimates = []\n",
    "        for u, i, r_ui in tuples_test:\n",
    "            u = int(u)\n",
    "            i = int(i)\n",
    "            est = self.estimate(u, i, test_matrix, test_data)\n",
    "            estimates.append(est)\n",
    "            total_error += np.square(r_ui - est)\n",
    "        \n",
    "        rmse = np.sqrt(total_error / len(tuples_test))\n",
    "        \n",
    "        if not val:\n",
    "            self.test_results = {'rmse': rmse, 'estimates':estimates}\n",
    "            print('RMSE on test set:', self.test_results['rmse'])\n",
    "        else:\n",
    "            return rmse\n",
    "\n",
    "    def estimate(self, u, i, test_matrix, test_data):\n",
    "        est = self.mu + self.model['bu'][u] + self.model['bi'][i]\n",
    "        impl_fb_u = np.zeros(self.nolf)\n",
    "        cb = 0\n",
    "        if u in self.train_data['new_user_id'] and i in self.train_data['new_item_id']:\n",
    "            \n",
    "            if self.use_impl_fb and u in self.N.index:\n",
    "                sqrt_Nu = np.sqrt(len(self.N[u]))\n",
    "                for j in self.N[u]:   \n",
    "                    impl_fb_u += self.model['x'][j] / sqrt_Nu\n",
    "            \n",
    "            if self.use_color and (u,i) in self.model['cbu']:\n",
    "                F_ui =  self.F[u,i] #Set of items associated with i and rated by u\n",
    "                u_mu = self.mu + self.model['bu'][u]\n",
    "                sqrt_F_ui = np.sqrt(len(F_ui))\n",
    "                for index, f in enumerate(F_ui):\n",
    "                    r_uf = self.train_data[(self.train_data['new_user_id']==u) & (self.train_data['new_item_id']==f)]['rating'].iloc[0]\n",
    "                    cb += (r_uf - u_mu) * self.model['cbu'][u,i][index]\n",
    "                cb /=  sqrt_F_ui\n",
    "                \n",
    "            est += cb + np.dot(self.model['p'][u] + impl_fb_u, self.model['q'][i])\n",
    "\n",
    "        return est\n",
    "    \n",
    "    def store_results(self, log_path, res_name, user_thres, item_thres):\n",
    "        train_size = round((len(self.train_data) / (len(self.train_data) + len(self.test_data) + len(self.val_data))),1)\n",
    "        test_size = round((len(self.test_data) / (len(self.train_data) + len(self.test_data) + len(self.val_data))),1)\n",
    "        val_size = round((len(self.val_data) / (len(self.train_data) + len(self.test_data) + len(self.val_data))),1)\n",
    "        \n",
    "        result_info = {'RMSE_test': self.test_results['rmse'], 'train_speed': round(self.train_time,2)}\n",
    "        other_info = {'u_thres': user_thres,'i_thres': item_thres, 'train_size':train_size, 'test_size':test_size, 'val_size':val_size, 'train_rmse':self.model['rmse'], 'val_rmse':self.model['val_rmse']}\n",
    "        final_log = dict(result_info, **self.params, **other_info)\n",
    "\n",
    "        if not os.path.exists(log_path + res_name):\n",
    "            df_results = pd.DataFrame(columns=final_log.keys())\n",
    "            print('new results created')\n",
    "\n",
    "        else:\n",
    "            df_results = pd.read_pickle(log_path + res_name)\n",
    "            print('results added')\n",
    "\n",
    "        df_results = df_results.append(final_log, ignore_index=True)\n",
    "        pd.to_pickle(df_results, log_path + res_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPR():\n",
    "    def __init__(self, total_users, total_items, params):\n",
    "        self.total_users = total_users\n",
    "        self.total_items = total_items\n",
    "        self.nolf = params['nolf']\n",
    "        self.n_iterations = params['n_iterations']\n",
    "        self.alpha = params['alpha']\n",
    "        self.reg_user = params['reg_user']\n",
    "        self.reg_item = params['reg_item']\n",
    "        self.reg_bias = params['reg_bias']\n",
    "        self.alpha_decay = self.alpha / self.n_iterations\n",
    "        self.model = {'loss_list':[], 'learning_rate':[]}\n",
    "        \n",
    "    def fit(self, train_set, val_set, val_rank, batch_size=1000):\n",
    "        #Init\n",
    "        s = time.time()\n",
    "        self.model['p'] = np.random.normal(0, .1, (self.total_users, self.nolf))  # users\n",
    "        self.model['q'] = np.random.normal(0, .1, (self.total_items, self.nolf))  # items\n",
    "        self.model['b'] = np.zeros(self.total_items)\n",
    "        \n",
    "#         val_prec_at = []\n",
    "#         val_rec_at = []\n",
    "#         val_hitcount = []\n",
    "        \n",
    "        # Create samples \n",
    "        n_sgd_samples = len(train_set) * self.n_iterations\n",
    "        \n",
    "        z = 0\n",
    "        self.model['train_time'] = 0\n",
    "        print('init and sampling done:', time.time() - s, 'seconds')\n",
    "        for i in range(self.n_iterations):\n",
    "            sgd_users, sgd_pos_items, sgd_neg_items = self.user_sampling(train_set, n_sgd_samples)\n",
    "        \n",
    "        while (z+1)*batch_size < n_sgd_samples:\n",
    "            s_it = time.time()\n",
    "            it_loss = self.train(sgd_users[z*batch_size:(z+1)*batch_size], sgd_pos_items[z*batch_size:(z+1)*batch_size], sgd_neg_items[z*batch_size:(z+1)*batch_size])\n",
    "            \n",
    "            if z > 0:\n",
    "                self.update_alpha(it_loss)\n",
    "            \n",
    "            z += 1\n",
    "            self.model['loss_list'].append(it_loss) \n",
    "\n",
    "#             rec_at, prec_at, hitcount = self.eval(val_set, val_rank)\n",
    "            t_it = time.time()- s_it\n",
    "            self.model['train_time'] += t_it\n",
    "            print('batch:', z, ' loss:', round(it_loss,4), 'iteration time:', round(t_it/2,2))#, ' val prec@' + str(val_rank), ':', round(prec_at,5), ' val rec@' + str(val_rank), ':', round(rec_at,5), '  Hits:', hitcount)#'  alpha:', self.alpha)\n",
    "    \n",
    "#             val_prec_at.append(prec_at)\n",
    "#             val_rec_at.append(rec_at)\n",
    "#             val_hitcount.append(hitcount)\n",
    "            \n",
    "#         self.model['val_prec_at'] = val_prec_at\n",
    "#         self.model['val_rec_at'] = val_rec_at\n",
    "#         self.model['val_hitcount'] = val_hitcount\n",
    "        \n",
    "        \n",
    "    def create_matrices(self, data):\n",
    "        r = data['new_user_id']\n",
    "        c = data['new_item_id']\n",
    "        d = data['rating']\n",
    "        m = sparse.csr_matrix((d, (r, c)), shape=(self.total_users, self.total_items))\n",
    "        m_ones = m.copy()\n",
    "        m_ones[m_ones > 0] = 1                 \n",
    "        return m, m_ones\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + math.exp(-x))\n",
    "    \n",
    "    def user_sampling(self, data, n_samples):\n",
    "        train_ratings, train_ones = self.create_matrices(train_set)\n",
    "        user_items = train_set.groupby('new_user_id')['new_item_id'].apply(list)\n",
    "        train_users  = train_set.new_user_id.unique()\n",
    "        train_items = train_set.new_item_id.unique()\n",
    "        \n",
    "        sgd_users, sgd_pos_items, sgd_neg_items = [], [], []\n",
    "        for sample in range(n_samples):\n",
    "            u = np.random.choice(train_users)\n",
    "            i = random.choice(user_items[u])\n",
    "\n",
    "            j = int(np.random.choice(train_items)) # neg item\n",
    "#             j_v = int(train_ones[u,j]) # Value, NEEDED?\n",
    "\n",
    "            while j in user_items[u]: # j cannot be the same item or an item with a 1\n",
    "                j = int(np.random.choice(train_items))\n",
    "#                 j_v = int(train_ones[u,j])\n",
    "            \n",
    "            sgd_users.append(u)\n",
    "            sgd_pos_items.append(i)\n",
    "            sgd_neg_items.append(j)\n",
    "            \n",
    "        return sgd_users, sgd_pos_items, sgd_neg_items\n",
    "        \n",
    "    def train(self, users, pos_items, neg_items):\n",
    "        for u, i, j in zip(users, pos_items, neg_items):\n",
    "            pos_item_pred = self.model['b'][i] + np.dot(self.model['p'][u], self.model['q'][i].T)\n",
    "            neg_item_pred = self.model['b'][j] + np.dot(self.model['p'][u], self.model['q'][j].T)\n",
    "            diff = pos_item_pred - neg_item_pred\n",
    "\n",
    "            loss_value = - np.log(self.sigmoid(diff)) #NEGATIVE?\n",
    "            regulariser = self.reg_user * np.dot(self.model['p'][u], self.model['p'][u]) + self.reg_item * np.dot(self.model['q'][i],self.model['q'][i]) + self.reg_item/10 * np.dot(self.model['q'][j], self.model['q'][j]) + self.reg_bias * (self.model['b'][i]**2 + self.model['b'][j]**2) \n",
    "            it_loss = loss_value + regulariser\n",
    "\n",
    "            diff_deriv = self.sigmoid(- diff)\n",
    "            \n",
    "            #SGD update\n",
    "            for f in range(self.nolf): # update each factor (see notes for derivatives)\n",
    "                self.model['p'][u,f] += self.alpha * (diff_deriv * (self.model['q'][i,f] - self.model['q'][j,f]) - self.reg_user * self.model['p'][u,f])\n",
    "                self.model['q'][i,f] += self.alpha * (diff_deriv * self.model['p'][u,f] - self.reg_item * self.model['q'][i,f])\n",
    "                self.model['q'][j,f] += self.alpha * (diff_deriv * (-self.model['p'][u,f]) - self.reg_item / 10 * self.model['q'][j,f])\n",
    "                self.model['b'][i] += self.alpha * (diff_deriv * self.reg_bias * self.model['b'][i])\n",
    "                self.model['b'][j] += self.alpha * (- diff_deriv * (- self.reg_bias) * self.model['b'][j])\n",
    "\n",
    "#                 it_loss += self.reg_user * self.model['p'][u,f] * self.model['p'][u,f] + self.reg_item * self.model['q'][i,f] * self.model['q'][i,f] + self.reg_item * self.model['q'][j,f] * self.model['q'][j,f]\n",
    "        return it_loss\n",
    "        \n",
    "    def update_alpha(self, it_loss):\n",
    "        last_loss = self.model['loss_list'][-1]\n",
    "        if(last_loss < it_loss): #bold driver\n",
    "            self.alpha = 0.5 * self.alpha\n",
    "            return\n",
    "        \n",
    "        self.alpha = (1 - self.alpha_decay) * self.alpha\n",
    "        self.model['learning_rate'].append(self.alpha)\n",
    "        \n",
    "    def eval(self, val_set, max_rank):\n",
    "        import eval_rank\n",
    "        val_ratings, val_ones = create_matrices(val_set, self.total_users, self.total_items)\n",
    "        result = self.model\n",
    "        users = val_set.new_user_id.unique()\n",
    "        items = val_set.new_item_id.unique()\n",
    "\n",
    "        s = time.time()\n",
    "        rank_at = max_rank\n",
    "        mp_splits = 4\n",
    "        users_split = np.array_split(users, mp_splits)\n",
    "\n",
    "        if __name__ == '__main__':\n",
    "            pool = mp.Pool(processes = mp_splits)\n",
    "            ranked = pool.map(eval_rank.eval_rank, [[result, users_split[0], items, val_ones, rank_at], \n",
    "                                                    [result, users_split[1], items, val_ones, rank_at], \n",
    "                                                    [result, users_split[2], items, val_ones, rank_at], \n",
    "                                                    [result, users_split[3], items, val_ones, rank_at]])\n",
    "            pool.close()\n",
    "\n",
    "            ranked_df = pd.DataFrame()\n",
    "\n",
    "            for i in range(mp_splits):\n",
    "                ranked_df = pd.concat([ranked_df, ranked[i]])\n",
    "\n",
    "            t = time.time() - s\n",
    "            hitcount = 0\n",
    "            for u in ranked_df.index:\n",
    "                hitcount += len(set(ranked_df.loc[u]['true_id']) & set(ranked_df.loc[u]['pred_items_ranked']))\n",
    "\n",
    "            prec_at =  hitcount / (len(ranked_df) * rank_at)\n",
    "            rec_at = hitcount / (len(ranked_df) * len(ranked_df.loc[0]['true_id']))\n",
    "            \n",
    "            return prec_at, rec_at, hitcount\n",
    "#             print(t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "375.742px",
    "left": "1048.75px",
    "top": "110.57px",
    "width": "200.295px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
