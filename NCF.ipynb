{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version: 2.1.0\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU'), PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(f'TF version: {tf.__version__}')\n",
    "print(tf.config.list_physical_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = 'C:/Users/robin.opdam/Google Drive/Thesis (Msc)/Thesis_shared_files/'\n",
    "path = '/Users/Robin/Google Drive/Thesis (Msc)/Thesis_shared_files/'\n",
    "path = '../datasets/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon Fashion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = 'data/Amazon/'\n",
    "# file_name = 'Amazon_full' # file_name = 'Amazon_05_users' \n",
    "# file_name = 'Amazon_01_users'\n",
    "file_name = 'am_80k_users'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MovieLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = 'data/ML/'\n",
    "# file_name = 'ml_1m'\n",
    "# file_name = 'ML_full' # file_name = 'ML_05_users'\n",
    "# file_name = 'ML_01_users'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "      <th>datetime</th>\n",
       "      <th>rating</th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6904244</th>\n",
       "      <td>A2EQZT4NOBKME3</td>\n",
       "      <td>B00SFLJZ52</td>\n",
       "      <td>2015-06-11</td>\n",
       "      <td>3.0</td>\n",
       "      <td>29949</td>\n",
       "      <td>137482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4472551</th>\n",
       "      <td>A2EQZT4NOBKME3</td>\n",
       "      <td>B00D4TJRWG</td>\n",
       "      <td>2015-06-11</td>\n",
       "      <td>5.0</td>\n",
       "      <td>29949</td>\n",
       "      <td>73746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10630561</th>\n",
       "      <td>A2EQZT4NOBKME3</td>\n",
       "      <td>B00OBT081W</td>\n",
       "      <td>2016-01-15</td>\n",
       "      <td>5.0</td>\n",
       "      <td>29949</td>\n",
       "      <td>126216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5801430</th>\n",
       "      <td>A2EQZT4NOBKME3</td>\n",
       "      <td>B00KD9AGAC</td>\n",
       "      <td>2016-08-24</td>\n",
       "      <td>5.0</td>\n",
       "      <td>29949</td>\n",
       "      <td>108075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5505899</th>\n",
       "      <td>A1QKA075BTCNIH</td>\n",
       "      <td>B00ISY7VNO</td>\n",
       "      <td>2015-01-13</td>\n",
       "      <td>5.0</td>\n",
       "      <td>15581</td>\n",
       "      <td>100488</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    user        item   datetime  rating  user_id  item_id\n",
       "6904244   A2EQZT4NOBKME3  B00SFLJZ52 2015-06-11     3.0    29949   137482\n",
       "4472551   A2EQZT4NOBKME3  B00D4TJRWG 2015-06-11     5.0    29949    73746\n",
       "10630561  A2EQZT4NOBKME3  B00OBT081W 2016-01-15     5.0    29949   126216\n",
       "5801430   A2EQZT4NOBKME3  B00KD9AGAC 2016-08-24     5.0    29949   108075\n",
       "5505899   A1QKA075BTCNIH  B00ISY7VNO 2015-01-13     5.0    15581   100488"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle(path + file_name)\n",
    "df.user_id = df.user_id.astype('category').cat.codes\n",
    "df.item_id = df.item_id.astype('category').cat.codes\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Data_prep import leave_users_out\n",
    "remaining, subset = leave_users_out(df, 40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "351771"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = subset\n",
    "df.user_id = df.user_id.astype('category').cat.codes\n",
    "df.item_id = df.item_id.astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_perc = test_perc = 0.1\n",
    "n_last_items_val = n_last_items_test = 1\n",
    "\n",
    "total_items = len(df.item_id.unique())\n",
    "total_users = len(df.user_id.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Data_prep import train_val_test_split\n",
    "datasets = train_val_test_split(df, val_perc, test_perc, n_last_items_val, n_last_items_test)\n",
    "train_set, val_set, test_set = datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Neural Collaborative Filtering (NCF)\n",
    "Using the NCF framework we build Generalized Matrix Factorisation (GMF), Multiplayer Perceptron Matrix Factorisation (MLP) and combine the two in Neural Matrix Factorisation (NeuMF)\n",
    "- paper: http://papers.www2017.com.au.s3-website-ap-southeast-2.amazonaws.com/proceedings/p173.pdf\n",
    "- blog: https://medium.com/@victorkohler/collaborative-filtering-using-deep-neural-networks-in-tensorflow-96e5d41a39a1\n",
    "- code: https://github.com/Leavingseason/NeuralCF/blob/master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CKPTS Store Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_num = 'am_40k_nolf_20'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "GMF_params = {\n",
    "    'learning_rate': 0.001,\n",
    "    'batch_size': 256,\n",
    "    'nolf': 20,\n",
    "    'regs': [0.00001,0.00001],\n",
    "    'epochs': 20,\n",
    "    'sample_size': len(train_set),#int(0.5*len(train_set.user_id.unique())),\n",
    "    'num_neg': 4,\n",
    "    'ckpt_dir': f'../NeuMF_storage/GMF_ckpts_{run_num}/ckpts',\n",
    "    'optimizer':'Adam'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_params = {\n",
    "    'learning_rate': 0.001,\n",
    "    'batch_size': 256,\n",
    "    'layers': [64,32,16,8],\n",
    "    'reg_layers': [0,0,0,0],\n",
    "    'epochs': 20,\n",
    "    'sample_size': len(train_set),#int(0.5*len(train_set.user_id.unique())),\n",
    "    'num_neg': 4,\n",
    "    'ckpt_dir': f'../NeuMF_storage/MLP_ckpts_{run_num}/ckpts',\n",
    "    'optimizer':'Adam'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "NeuMF_params = {\n",
    "    'learning_rate': 0.001,\n",
    "    'batch_size': 256,\n",
    "    'layers': [64,32,16,8],\n",
    "    'reg_layers': [0,0,0,0],\n",
    "    'reg_mf': [0.00001, 0.00001],\n",
    "    'nolf': 20,\n",
    "    'epochs': 20,\n",
    "    'sample_size': len(train_set),#int(0.5*len(train_set.user_id.unique())),\n",
    "    'num_neg': 4,\n",
    "    'ckpt_dir': f'../NeuMF_storage/NeuMF_ckpts_{run_num}/ckpts',\n",
    "    'optimizer':'Adam'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from NCF import NCF\n",
    "NCF = NCF(total_users, total_items, GMF_params, MLP_params, NeuMF_params)\n",
    "\n",
    "NCF.build_GMF_model()\n",
    "NCF.build_MLP_model()\n",
    "NCF.build_NeuMF_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MP sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsampler(worker_id, epochs, user_items, train_users, train_items, params):\n",
    "    print(f'worker {workes_id} started')\n",
    "    all_user_inputs, all_item_inputs, all_labels = [], [], []\n",
    "    for epoch in range(epochs): \n",
    "        user_inputs, item_inputs, labels = [], [], []\n",
    "        for s in range(params['sample_size']):\n",
    "            # Add positive item\n",
    "            u = np.random.choice(train_users)\n",
    "            u_items = user_items[u]\n",
    "            i = np.random.choice(u_items)\n",
    "\n",
    "            user_inputs.append(u)\n",
    "            item_inputs.append(i)\n",
    "            labels.append(1)\n",
    "\n",
    "            # Add negative item\n",
    "            for i in range(params['num_neg']):\n",
    "                j = np.random.choice(train_items)\n",
    "                while j in u_items:  # neg item j cannot be in the set of pos items of user u\n",
    "                    j = np.random.choice(train_items)\n",
    "\n",
    "                user_inputs.append(u)\n",
    "                item_inputs.append(j)\n",
    "                labels.append(0)\n",
    "                \n",
    "        all_user_inputs.append(user_inputs)\n",
    "        all_item_inputs.append(item_inputs)\n",
    "        all_labels.append(labels)\n",
    "        \n",
    "    return {'u':all_user_inputs, 'i':all_item_inputs, 'l':all_labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 48.5 s, sys: 13.1 s, total: 1min 1s\n",
      "Wall time: 1h 18min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import multiprocessing as mp\n",
    "from progressbar import progressbar\n",
    "if __name__ == '__main__':\n",
    "    processors = mp.cpu_count()\n",
    "    params = NCF.GMF_params\n",
    "#     params = NCF.MLP_params\n",
    "#     params = NCF.NeuMF_params\n",
    "    data = train_set\n",
    "\n",
    "    all_user_inputs, all_item_inputs, all_labels = [], [], []\n",
    "    user_items = data.groupby('user_id')['item_id'].apply(list)\n",
    "    train_users = data.user_id.unique()\n",
    "    train_items = data.item_id.unique()\n",
    "    \n",
    "    epoch_splits = np.array_split(np.array(range(params['epochs'])), processors)\n",
    "    args = []\n",
    "    for worker_id, epoch_split in enumerate(epoch_splits):\n",
    "        args.append((worker_id, len(epoch_split), user_items, train_users, train_items, params))\n",
    "        \n",
    "    with mp.Pool(processes=processors) as pool:\n",
    "        results = pool.starmap(subsampler, args)\n",
    "    \n",
    "    all_user_inputs, all_item_inputs, all_labels = [], [], []\n",
    "    samples = [[], [], []]\n",
    "    for res_epochs in results:\n",
    "        all_user_inputs.extend(res_epochs['u'])\n",
    "        all_item_inputs.extend(res_epochs['i'])\n",
    "        all_labels.extend(res_epochs['l'])\n",
    "    \n",
    "    GMF_samples_mp = [all_user_inputs, all_item_inputs, all_labels]\n",
    "#     MLP_samples_mp = [all_user_inputs, all_item_inputs, all_labels]\n",
    "#     NeuMF_samples_mp = [all_user_inputs, all_item_inputs, all_labels]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GMF_sample_dict = {'u':GMF_samples_mp[0], 'i':GMF_samples_mp[1], 'l':GMF_samples_mp[2]}\n",
    "GMF_samples_df = pd.DataFrame(GMF_sample_dict)\n",
    "GMF_samples_df.to_pickle('../NeuMF_storage/GMF_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GMF_samples = NCF.create_samples(name='GMF', data=train_set)\n",
    "# MLP_samples = NCF.create_samples(name='MLP', data=train_set)\n",
    "# NeuMF_samples = NCF.create_samples(name='NeuMF', data=train_set)all_user_inputs, all_item_inputs, all_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "def train_model(name='', train_set=[], verbose=1, store_path=''):\n",
    "    model, params = NCF.get_model(name)\n",
    "\n",
    "    ckpts_prefix = os.path.join(params['ckpt_dir'], \"ckpt\")\n",
    "    ckpts_callback = tf.keras.callbacks.ModelCheckpoint(filepath=ckpts_prefix,    \n",
    "                                                     monitor='loss',    \n",
    "                                                     mode='min',    \n",
    "                                                     save_best_only=True,\n",
    "                                                     save_weights_only=True)\n",
    "\n",
    "    if len(train_set) == 0:\n",
    "        raise Exception('No samples available, create samples first using: create_samples')\n",
    "        \n",
    "    fit(model, params, train_set, [ckpts_callback], verbose)\n",
    "\n",
    "    if len(store_path) > 0:\n",
    "        model.save_weights(store_path)\n",
    "\n",
    "\n",
    "def fit(model, params, train_set, callbacks, verbose):\n",
    "    print(f'\\nFitting {model._name} with parameters:')\n",
    "    print(pd.DataFrame.from_dict(params, orient='index'))\n",
    "    user_items = train_set.groupby('user_id')['item_id'].apply(list)\n",
    "    train_users = train_set.user_id.unique()\n",
    "    train_items = train_set.item_id.unique()\n",
    "    num_processes = mp.cpu_count()\n",
    "\n",
    "    for epoch in range(params['epochs']):\n",
    "        print(f'Epoch: {epoch}')\n",
    "        user_inputs, item_inputs, labels = create_sample(user_items, train_users, train_items, params, num_processes)\n",
    "        hist = model.fit([np.array(user_inputs), np.array(item_inputs)], \n",
    "                  np.array(labels), \n",
    "                  batch_size=params['batch_size'], \n",
    "                  verbose=verbose, \n",
    "                  epochs=1, \n",
    "                  shuffle=True,\n",
    "                  callbacks=callbacks)\n",
    "\n",
    "        NCF.history[model._name]['loss'].append(round(hist.history['loss'][0],5))\n",
    "\n",
    "\n",
    "def create_sample(user_items, train_users, train_items, params, num_processes):\n",
    "    samples_sizes_split = np.array_split(np.array(range(params['sample_size'])),8)\n",
    "    args = []\n",
    "    for samples_size in samples_sizes_split:\n",
    "        args.append((user_items, train_users, train_items, len(samples_size), params['num_neg']))\n",
    "    with mp.Pool(processes=num_processes) as pool:\n",
    "        results = pool.starmap(create_sample_worker, args)\n",
    "\n",
    "    user_inputs, item_inputs, labels = [], [], []\n",
    "    for res_epochs in results: \n",
    "        user_inputs.extend(res_epochs['u'])\n",
    "        item_inputs.extend(res_epochs['i'])\n",
    "        labels.extend(res_epochs['l'])\n",
    "\n",
    "    return user_inputs, item_inputs, labels\n",
    "\n",
    "\n",
    "def create_sample_worker(user_items, train_users, train_items, sample_size, num_neg):\n",
    "    user_inputs, item_inputs, labels = [], [], []\n",
    "#     print('worker started')\n",
    "    for s in range(sample_size):\n",
    "        # Add positive item\n",
    "        u = np.random.choice(train_users)\n",
    "        u_items = user_items[u]\n",
    "        i = np.random.choice(u_items)\n",
    "\n",
    "        user_inputs.append(u)\n",
    "        item_inputs.append(i)\n",
    "        labels.append(1)\n",
    "\n",
    "        # Add negative item\n",
    "        for i in range(num_neg):\n",
    "            j = np.random.choice(train_items)\n",
    "            while j in u_items:  # neg item j cannot be in the set of pos items of user u\n",
    "                j = np.random.choice(train_items)\n",
    "\n",
    "            user_inputs.append(u)\n",
    "            item_inputs.append(j)\n",
    "            labels.append(0)\n",
    "\n",
    "    return {'u':user_inputs, 'i':item_inputs, 'l':labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting GMF with parameters:\n",
      "                                                             0\n",
      "learning_rate                                            0.001\n",
      "batch_size                                                 256\n",
      "nolf                                                        20\n",
      "regs                                            [1e-05, 1e-05]\n",
      "epochs                                                      20\n",
      "sample_size                                              86388\n",
      "num_neg                                                      4\n",
      "ckpt_dir       ../NeuMF_storage/GMF_ckpts_am_10k_nolf_20/ckpts\n",
      "optimizer                                                 Adam\n",
      "Epoch: 0\n",
      "Train on 431940 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "431940/431940 [==============================] - 14s 32us/sample - loss: 0.5550\n",
      "Epoch: 1\n",
      "Train on 431940 samples\n",
      "431940/431940 [==============================] - 13s 31us/sample - loss: 0.3242\n",
      "Epoch: 2\n",
      "Train on 431940 samples\n",
      "431940/431940 [==============================] - 13s 31us/sample - loss: 0.1732\n",
      "Epoch: 3\n",
      "Train on 431940 samples\n",
      "431940/431940 [==============================] - 13s 31us/sample - loss: 0.1397\n",
      "Epoch: 4\n",
      "Train on 431940 samples\n",
      " 16128/431940 [>.............................] - ETA: 20s - loss: 0.1282"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-299c64a3384e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GMF'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstore_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'../weights/GMF_weights_{run_num}/GMF_weights'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-47-2cef00820ae2>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(name, train_set, verbose, store_path)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No samples available, create samples first using: create_samples'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mckpts_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstore_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-2cef00820ae2>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(model, params, train_set, callbacks, verbose)\u001b[0m\n\u001b[1;32m     36\u001b[0m                   \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                   \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m                   callbacks=callbacks)\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mNCF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    597\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    train_model('GMF', train_set, store_path=f'../weights/GMF_weights_{run_num}/GMF_weights') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting MLP with parameters:\n",
      "                                                     0\n",
      "learning_rate                                    0.001\n",
      "batch_size                                         256\n",
      "layers                                 [64, 32, 16, 8]\n",
      "reg_layers                                [0, 0, 0, 0]\n",
      "epochs                                              20\n",
      "sample_size                                      86388\n",
      "num_neg                                              4\n",
      "ckpt_dir       ../NeuMF_storage/MLP_ckpts_am_10k/ckpts\n",
      "optimizer                                         Adam\n",
      "Epoch: 0\n",
      "Train on 431940 samples\n",
      "431940/431940 [==============================] - 22s 51us/sample - loss: 0.2170\n",
      "Epoch: 1\n",
      "Train on 431940 samples\n",
      "431940/431940 [==============================] - 21s 49us/sample - loss: 0.0185\n",
      "Epoch: 2\n",
      "Train on 431940 samples\n",
      "431940/431940 [==============================] - 21s 49us/sample - loss: 8.2873e-04\n",
      "Epoch: 3\n",
      "Train on 431940 samples\n",
      "431940/431940 [==============================] - 21s 49us/sample - loss: 2.0146e-04\n",
      "Epoch: 4\n",
      "Train on 431940 samples\n",
      "431940/431940 [==============================] - 21s 50us/sample - loss: 1.5548e-05\n",
      "Epoch: 5\n",
      "Train on 431940 samples\n",
      "431940/431940 [==============================] - 21s 49us/sample - loss: 4.8118e-06\n",
      "Epoch: 6\n",
      "Train on 431940 samples\n",
      "431940/431940 [==============================] - 21s 49us/sample - loss: 1.7010e-06\n",
      "Epoch: 7\n",
      "Train on 431940 samples\n",
      "431940/431940 [==============================] - 21s 49us/sample - loss: 6.2312e-07\n",
      "Epoch: 8\n",
      "Train on 431940 samples\n",
      "431940/431940 [==============================] - 21s 49us/sample - loss: 2.3338e-07\n",
      "Epoch: 9\n",
      "Train on 431940 samples\n",
      "431940/431940 [==============================] - 21s 49us/sample - loss: 8.9819e-08\n",
      "Epoch: 10\n",
      "Train on 431940 samples\n",
      "431940/431940 [==============================] - 21s 49us/sample - loss: 3.6023e-08\n",
      "Epoch: 11\n",
      "Train on 431940 samples\n",
      "431940/431940 [==============================] - 21s 49us/sample - loss: 1.5518e-08\n",
      "Epoch: 12\n",
      "Train on 431940 samples\n",
      "431940/431940 [==============================] - 21s 50us/sample - loss: 7.4144e-09\n",
      "Epoch: 13\n",
      "Train on 431940 samples\n",
      "431940/431940 [==============================] - 22s 50us/sample - loss: 4.1701e-09\n",
      "Epoch: 14\n",
      "Train on 431940 samples\n",
      "431940/431940 [==============================] - 21s 49us/sample - loss: 2.8421e-09\n",
      "Epoch: 15\n",
      "Train on 431940 samples\n",
      "431940/431940 [==============================] - 22s 50us/sample - loss: 2.2756e-09\n",
      "Epoch: 16\n",
      "Train on 431940 samples\n",
      "431940/431940 [==============================] - 21s 49us/sample - loss: 2.0092e-09\n",
      "Epoch: 17\n",
      "Train on 431940 samples\n",
      "431940/431940 [==============================] - 22s 51us/sample - loss: 1.8701e-09\n",
      "Epoch: 18\n",
      "Train on 431940 samples\n",
      "431940/431940 [==============================] - 21s 49us/sample - loss: 1.8054e-09\n",
      "Epoch: 19\n",
      "Train on 431940 samples\n",
      "431940/431940 [==============================] - 21s 50us/sample - loss: 1.7619e-09\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    train_model('MLP', train_set, store_path=f'../weights/MLP_weights_{run_num}/MLP_weights') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NCF.train_model('NeuMF', NeuMF_samples) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load weights for NeuMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "NCF.use_pretrain_model(GMF_weights_path=f'../weights/GMF_weights_{run_num}/GMF_weights',\n",
    "                       MLP_weights_path=f'../weights/MLP_weights_{run_num}/MLP_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting NeuMF with parameters:\n",
      "                                                               0\n",
      "learning_rate                                              0.001\n",
      "batch_size                                                   256\n",
      "layers                                           [64, 32, 16, 8]\n",
      "reg_layers                                          [0, 0, 0, 0]\n",
      "reg_mf                                            [1e-05, 1e-05]\n",
      "nolf                                                          20\n",
      "epochs                                                        20\n",
      "sample_size                                               343771\n",
      "num_neg                                                        4\n",
      "ckpt_dir       ../NeuMF_storage/NeuMF_ckpts_am_40k_nolf_20/ckpts\n",
      "optimizer                                                   Adam\n",
      "Epoch: 0\n",
      "Train on 1718855 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1718855/1718855 [==============================] - 268s 156us/sample - loss: 0.1529\n",
      "Epoch: 1\n",
      "Train on 1718855 samples\n",
      "1718855/1718855 [==============================] - 250s 145us/sample - loss: 9.7807e-05\n",
      "Epoch: 2\n",
      "Train on 1718855 samples\n",
      "1718855/1718855 [==============================] - 259s 151us/sample - loss: 1.5613e-04\n",
      "Epoch: 3\n",
      "Train on 1718855 samples\n",
      "1718855/1718855 [==============================] - 252s 146us/sample - loss: 3.3659e-05\n",
      "Epoch: 4\n",
      "Train on 1718855 samples\n",
      "1718855/1718855 [==============================] - 252s 147us/sample - loss: 4.9021e-08\n",
      "Epoch: 5\n",
      "Train on 1718855 samples\n",
      "1718855/1718855 [==============================] - 252s 147us/sample - loss: 4.7087e-09\n",
      "Epoch: 6\n",
      "Train on 1718855 samples\n",
      "1718855/1718855 [==============================] - 250s 145us/sample - loss: 1.2280e-09\n",
      "Epoch: 7\n",
      "Train on 1718855 samples\n",
      "1718855/1718855 [==============================] - 269s 156us/sample - loss: 8.5819e-10\n",
      "Epoch: 8\n",
      "Train on 1718855 samples\n",
      "1718855/1718855 [==============================] - 250s 145us/sample - loss: 7.7300e-10\n",
      "Epoch: 9\n",
      "Train on 1718855 samples\n",
      "1718855/1718855 [==============================] - 251s 146us/sample - loss: 7.3811e-10\n",
      "Epoch: 10\n",
      "Train on 1718855 samples\n",
      "1718855/1718855 [==============================] - 250s 146us/sample - loss: 7.2082e-10\n",
      "Epoch: 11\n",
      "Train on 1718855 samples\n",
      "1718855/1718855 [==============================] - 249s 145us/sample - loss: 7.1164e-10\n",
      "Epoch: 12\n",
      "Train on 1718855 samples\n",
      "1718855/1718855 [==============================] - 250s 146us/sample - loss: 7.0506e-10\n",
      "Epoch: 13\n",
      "Train on 1718855 samples\n",
      "1718855/1718855 [==============================] - 261s 152us/sample - loss: 7.0191e-10\n",
      "Epoch: 14\n",
      "Train on 1718855 samples\n",
      "1718855/1718855 [==============================] - 260s 151us/sample - loss: 6.9780e-10\n",
      "Epoch: 15\n",
      "Train on 1718855 samples\n",
      " 304640/1718855 [====>.........................] - ETA: 3:24 - loss: 7.0040e-10"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    train_model('NeuMF', train_set, store_path=f'../weights/NeuMF_weights_{run_num}/NeuMF_weights') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 5\n",
    "rank_at = 20\n",
    "sample_len = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full set scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4451\n"
     ]
    }
   ],
   "source": [
    "u = test_set.user_id.unique()[1]\n",
    "print(u)\n",
    "user_array = np.full(total_items, u, dtype='int32')\n",
    "preds = np.hstack(NCF.MLP.predict([user_array, np.arange(total_items)], batch_size=total_items, verbose=0))\n",
    "\n",
    "ids = np.argpartition(preds, -rank_at)[-rank_at:]\n",
    "best_ids = np.argsort(preds[ids])[::-1]\n",
    "best = np.arange(total_items)[ids[best_ids]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3188,  3256,  3191, 10610, 46194, 33450, 49521, 19985,  3169,\n",
       "       33415, 49512,  3281, 20056, 33351, 20085,  3244, 20097, 10549,\n",
       "       20105, 33444])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([27709, 38199,  2727, 20903,  8565, 46599, 27759, 27768, 38133,\n",
       "       27793,  8625, 25749, 38100,  8650, 20832, 38034, 43046, 20801,\n",
       "       10840, 17531])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.hstack(NCF.NeuMF.predict([user_array, np.arange(total_items)], batch_size=total_items, verbose=0)).argsort()[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([53539, 25795, 25817, 25844, 25837, 25824, 25851, 25821, 25793,\n",
       "       25813, 25812, 25811, 25809, 25808, 25804, 25803, 25801, 25799,\n",
       "       25796, 25842])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fc9abe40198>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NCF.NeuMF.load_weights('../weights/NeuMF_weights_ml_1m/NeuMF_weights').expect_partial()\n",
    "NCF.GMF.load_weights('../weights/GMF_weights_ml_1m/GMF_weights').expect_partial()\n",
    "NCF.MLP.load_weights('../weights/MLP_weights_ml_1m/MLP_weights').expect_partial()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52597"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set.item_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 33s, sys: 347 ms, total: 1min 34s\n",
      "Wall time: 1min 30s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ranked_df_full = NCF.get_predictions('GMF', train_set, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank_at</th>\n",
       "      <th>hitcounts</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.000467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.000350</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rank_at  hitcounts  recall  precision\n",
       "0        1          0   0.000   0.000000\n",
       "1        5          5   0.005   0.001000\n",
       "2       10          7   0.007   0.000700\n",
       "3       15          7   0.007   0.000467\n",
       "4       20          7   0.007   0.000350"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Evaluation import get_metrics\n",
    "get_metrics(ranked_df_full, steps, rank_at, stats=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 41s, sys: 577 ms, total: 1min 41s\n",
      "Wall time: 1min 33s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ranked_df_full = NCF.get_predictions('MLP', train_set, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank_at</th>\n",
       "      <th>hitcounts</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rank_at  hitcounts  recall  precision\n",
       "0        1          0     0.0        0.0\n",
       "1        5          0     0.0        0.0\n",
       "2       10          0     0.0        0.0\n",
       "3       15          0     0.0        0.0\n",
       "4       20          0     0.0        0.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Evaluation import get_metrics\n",
    "get_metrics(ranked_df_full, steps, rank_at, stats=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ranked_df_full = NCF.get_predictions('NeuMF', train_set, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Evaluation import get_metrics\n",
    "get_metrics(ranked_df_full, steps, rank_at, stats=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 52.4 s, sys: 648 ms, total: 53 s\n",
      "Wall time: 51.8 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ranked_df = NCF.sample_prediction('GMF', train_set, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank_at</th>\n",
       "      <th>hitcounts</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>351</td>\n",
       "      <td>0.351</td>\n",
       "      <td>0.351000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>377</td>\n",
       "      <td>0.377</td>\n",
       "      <td>0.075400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>403</td>\n",
       "      <td>0.403</td>\n",
       "      <td>0.040300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>413</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.027533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>429</td>\n",
       "      <td>0.429</td>\n",
       "      <td>0.021450</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rank_at  hitcounts  recall  precision\n",
       "0        1        351   0.351   0.351000\n",
       "1        5        377   0.377   0.075400\n",
       "2       10        403   0.403   0.040300\n",
       "3       15        413   0.413   0.027533\n",
       "4       20        429   0.429   0.021450"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Evaluation import get_metrics\n",
    "get_metrics(ranked_df, steps, rank_at, stats=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 52.7 s, sys: 575 ms, total: 53.2 s\n",
      "Wall time: 51.9 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ranked_df = NCF.sample_prediction('MLP', train_set, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank_at</th>\n",
       "      <th>hitcounts</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.128000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>149</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.029800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>187</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.018700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>250</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.016667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>301</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.015050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rank_at  hitcounts  recall  precision\n",
       "0        1        128   0.128   0.128000\n",
       "1        5        149   0.149   0.029800\n",
       "2       10        187   0.187   0.018700\n",
       "3       15        250   0.250   0.016667\n",
       "4       20        301   0.301   0.015050"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Evaluation import get_metrics\n",
    "get_metrics(ranked_df, steps, rank_at, stats=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ranked_df = NCF.sample_prediction('NeuMF', train_set, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Evaluation import get_metrics\n",
    "get_metrics(ranked_df, steps, rank_at, stats=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Multiprocessing for multiple samples creatings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from progressbar import progressbar\n",
    "# def create_samples(params, data, name=''):\n",
    "#         print(f'Creating Samples for {name}')\n",
    "# #         _, params = self.get_model(name)\n",
    "#         all_user_inputs, all_item_inputs, all_labels = [], [], []\n",
    "#         user_items = data.groupby('user_id')['item_id'].apply(list)\n",
    "#         train_users = data.user_id.unique()\n",
    "#         train_items = data.item_id.unique()\n",
    "\n",
    "#         pbar = progressbar.ProgressBar()\n",
    "#         for n in pbar(range(params['epochs'])):\n",
    "#             user_inputs, item_inputs, labels = [], [], []\n",
    "#             for s in range(int(params['sample_size'])):\n",
    "#                 # Add positive item\n",
    "#                 u = np.random.choice(train_users)\n",
    "#                 u_items = user_items[u]\n",
    "#                 i = np.random.choice(u_items)\n",
    "\n",
    "#                 user_inputs.append(u)\n",
    "#                 item_inputs.append(i)\n",
    "#                 labels.append(1)\n",
    "\n",
    "#                 # Add negative item\n",
    "#                 for i in range(params['num_neg']):\n",
    "#                     j = np.random.choice(train_items)\n",
    "#                     while j in u_items:  # neg item j cannot be in the set of pos items of user u\n",
    "#                         j = np.random.choice(train_items)\n",
    "\n",
    "#                     user_inputs.append(u)\n",
    "#                     item_inputs.append(j)\n",
    "#                     labels.append(0)\n",
    "\n",
    "#             all_user_inputs.append(user_inputs)\n",
    "#             all_item_inputs.append(item_inputs)\n",
    "#             all_labels.append(labels)\n",
    "\n",
    "#         return [all_user_inputs, all_item_inputs, all_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import multiprocessing as mp\n",
    "# if __name__ == '__main__':\n",
    "    \n",
    "#     with mp.Pool(processes=2) as pool:\n",
    "#         results = pool.starmap(create_samples, [(GMF_params, train_set, 'GMF'), (MLP_params, train_set, 'MLP')])\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
