{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version: 2.1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU'),\n",
       " PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "print('TF version:', tf.__version__)\n",
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MovieLens Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../'\n",
    "data_path = 'datasets/' # Paperspace\n",
    "file_name = 'am_like_ml_01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "      <th>datetime</th>\n",
       "      <th>rating</th>\n",
       "      <th>item_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A0039616ADOZ0KMWQRNX</td>\n",
       "      <td>B00QFJG1U8</td>\n",
       "      <td>2016-10-04</td>\n",
       "      <td>5.0</td>\n",
       "      <td>87754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>A0039616ADOZ0KMWQRNX</td>\n",
       "      <td>B010ACF2PK</td>\n",
       "      <td>2016-10-04</td>\n",
       "      <td>5.0</td>\n",
       "      <td>104761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>A0039616ADOZ0KMWQRNX</td>\n",
       "      <td>B00BFE0IZ2</td>\n",
       "      <td>2016-10-04</td>\n",
       "      <td>5.0</td>\n",
       "      <td>42989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>A0039616ADOZ0KMWQRNX</td>\n",
       "      <td>B01CZMQCPC</td>\n",
       "      <td>2016-10-04</td>\n",
       "      <td>5.0</td>\n",
       "      <td>127248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>A0039616ADOZ0KMWQRNX</td>\n",
       "      <td>B01B5DLG7G</td>\n",
       "      <td>2016-10-26</td>\n",
       "      <td>4.0</td>\n",
       "      <td>123866</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                  user        item   datetime  rating  item_id\n",
       "0        0  A0039616ADOZ0KMWQRNX  B00QFJG1U8 2016-10-04     5.0    87754\n",
       "1        0  A0039616ADOZ0KMWQRNX  B010ACF2PK 2016-10-04     5.0   104761\n",
       "2        0  A0039616ADOZ0KMWQRNX  B00BFE0IZ2 2016-10-04     5.0    42989\n",
       "3        0  A0039616ADOZ0KMWQRNX  B01CZMQCPC 2016-10-04     5.0   127248\n",
       "4        0  A0039616ADOZ0KMWQRNX  B01B5DLG7G 2016-10-26     4.0   123866"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle(path + data_path + file_name)\n",
    "df['item_id'] = df.item.astype('category').cat.codes\n",
    "df['user_id'] = df.user.astype('category').cat.codes\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_ext = file_name[:2]\n",
    "total_items = len(df.item_id.unique())\n",
    "first_model_id = str(0) + '_' + res_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = pd.read_pickle(path + 'results/' + res_ext + '/all_models_2')\n",
    "# new_model_id = str(int(all_models.model_id.max()[0]) + 1) + '_' + res_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for value in all_models['loss'][:3]:\n",
    "    plt.plot(value)\n",
    "    plt.legend(all_models['model_id'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "'model_id':first_model_id,\n",
    "'train_time':0,\n",
    "'epochs':0,\n",
    "\n",
    "# Grid Search params\n",
    "'BATCH_SIZE':32,\n",
    "'learning_rate':0.1,\n",
    "'delta':0.2,             # Diversity Bias\n",
    "'max_seq_len':30,        # Max length of sequence71=median\n",
    "'embedding_dim':100,\n",
    "'rnn_units':20,\n",
    "    \n",
    "'val_perc':0.1,          # Percentage of users from df in val and test set\n",
    "'test_perc':0.1, \n",
    "'n_items_val':0,        # Number of last (chronologically) items in val and test set\n",
    "'n_items_test':1,\n",
    "\n",
    "'pad_value':total_items, # Pad with total_items+1 => masked => still use item 0\n",
    "'shift_targets_by':1  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = params['BATCH_SIZE']\n",
    "learning_rate = params['learning_rate']\n",
    "delta = params['delta']\n",
    "max_seq_len = params['max_seq_len']\n",
    "\n",
    "val_perc = params['val_perc']\n",
    "test_perc = params['test_perc']\n",
    "n_items_val = params['n_items_val']\n",
    "n_items_test = params['n_items_test']\n",
    "\n",
    "pad_value = params['pad_value']\n",
    "shift_targets_by = params['shift_targets_by'] \n",
    "\n",
    "embedding_dim = params['embedding_dim']\n",
    "rnn_units = params['rnn_units']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Data_prep import train_val_test_split, create_seq_batch_dataset\n",
    "from Models import build_LSTM_model, store_LSTM_model\n",
    "from Evaluation import recall_metric, diversity_bias_loss, create_diversity_bias, get_predictions, get_metrics\n",
    "from Helpers import TimingCallback\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Val Split\n",
    "data_split = train_val_test_split(df, val_perc, test_perc, n_items_val, n_items_test, seqs=True)\n",
    "train_set, val_set, val_left_out_items, test_set, test_left_out_items = data_split "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FetP-6nDc-Vd"
   },
   "source": [
    "# Configure Checkpoint directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext = '_' + file_name[:2] #ML or Am\n",
    "# directory = './ckpts/ckpts' \n",
    "directory = '../ckpts/ckpts'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total runs: 32\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [0.1, 0.2]\n",
    "epochs = [200]\n",
    "deltas = [0.2, 0.5]\n",
    "batch_sizes = [16, 32]\n",
    "max_seq_lens = [18, 30] #Median=18\n",
    "rnn_units = [20, 50]\n",
    "\n",
    "rank_at = 20\n",
    "test_batch_size = 1024\n",
    "\n",
    "total_runs = len(learning_rates) * len(epochs) * len(deltas) * len(batch_sizes) * len(max_seq_lens) * len(rnn_units)\n",
    "runs = 0\n",
    "print(f'Total runs: {total_runs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================== \n",
      "Run: 1/32\n",
      "Number of epochs:\t 200\n",
      "Number of (h) units:\t 20\n",
      "Batch size:\t\t 16\n",
      "Max seq len:\t\t 18\n",
      "Learning rate:\t\t 0.1\n",
      "Run Time: 93.95833333333333 mins\n",
      "================================================== \n",
      "Run: 2/32\n",
      "Number of epochs:\t 200\n",
      "Number of (h) units:\t 20\n",
      "Batch size:\t\t 16\n",
      "Max seq len:\t\t 18\n",
      "Learning rate:\t\t 0.2\n",
      "Run Time: 72.28483333333334 mins\n",
      "================================================== \n",
      "Run: 3/32\n",
      "Number of epochs:\t 200\n",
      "Number of (h) units:\t 20\n",
      "Batch size:\t\t 16\n",
      "Max seq len:\t\t 30\n",
      "Learning rate:\t\t 0.1\n",
      "Run Time: 136.13066666666666 mins\n",
      "================================================== \n",
      "Run: 4/32\n",
      "Number of epochs:\t 200\n",
      "Number of (h) units:\t 20\n",
      "Batch size:\t\t 16\n",
      "Max seq len:\t\t 30\n",
      "Learning rate:\t\t 0.2\n",
      "Run Time: 137.772 mins\n",
      "================================================== \n",
      "Run: 5/32\n",
      "Number of epochs:\t 200\n",
      "Number of (h) units:\t 20\n",
      "Batch size:\t\t 32\n",
      "Max seq len:\t\t 18\n",
      "Learning rate:\t\t 0.1\n",
      "Run Time: 79.4605 mins\n",
      "================================================== \n",
      "Run: 6/32\n",
      "Number of epochs:\t 200\n",
      "Number of (h) units:\t 20\n",
      "Batch size:\t\t 32\n",
      "Max seq len:\t\t 18\n",
      "Learning rate:\t\t 0.2\n",
      "Run Time: 79.74783333333333 mins\n",
      "================================================== \n",
      "Run: 7/32\n",
      "Number of epochs:\t 200\n",
      "Number of (h) units:\t 20\n",
      "Batch size:\t\t 32\n",
      "Max seq len:\t\t 30\n",
      "Learning rate:\t\t 0.1\n",
      "Run Time: 121.17833333333333 mins\n",
      "================================================== \n",
      "Run: 8/32\n",
      "Number of epochs:\t 200\n",
      "Number of (h) units:\t 20\n",
      "Batch size:\t\t 32\n",
      "Max seq len:\t\t 30\n",
      "Learning rate:\t\t 0.2\n",
      "Run Time: 121.25366666666667 mins\n",
      "================================================== \n",
      "Run: 9/32\n",
      "Number of epochs:\t 200\n",
      "Number of (h) units:\t 50\n",
      "Batch size:\t\t 16\n",
      "Max seq len:\t\t 18\n",
      "Learning rate:\t\t 0.1\n",
      "Run Time: 84.812 mins\n",
      "================================================== \n",
      "Run: 10/32\n",
      "Number of epochs:\t 200\n",
      "Number of (h) units:\t 50\n",
      "Batch size:\t\t 16\n",
      "Max seq len:\t\t 18\n",
      "Learning rate:\t\t 0.2\n",
      "Run Time: 76.02816666666666 mins\n",
      "================================================== \n",
      "Run: 11/32\n",
      "Number of epochs:\t 200\n",
      "Number of (h) units:\t 50\n",
      "Batch size:\t\t 16\n",
      "Max seq len:\t\t 30\n",
      "Learning rate:\t\t 0.1\n",
      "Run Time: 145.15050000000002 mins\n",
      "================================================== \n",
      "Run: 12/32\n",
      "Number of epochs:\t 200\n",
      "Number of (h) units:\t 50\n",
      "Batch size:\t\t 16\n",
      "Max seq len:\t\t 30\n",
      "Learning rate:\t\t 0.2\n",
      "Run Time: 122.71466666666667 mins\n",
      "================================================== \n",
      "Run: 13/32\n",
      "Number of epochs:\t 200\n",
      "Number of (h) units:\t 50\n",
      "Batch size:\t\t 32\n",
      "Max seq len:\t\t 18\n",
      "Learning rate:\t\t 0.1\n",
      "Run Time: 84.64899999999999 mins\n",
      "================================================== \n",
      "Run: 14/32\n",
      "Number of epochs:\t 200\n",
      "Number of (h) units:\t 50\n",
      "Batch size:\t\t 32\n",
      "Max seq len:\t\t 18\n",
      "Learning rate:\t\t 0.2\n",
      "Run Time: 69.35616666666667 mins\n",
      "================================================== \n",
      "Run: 15/32\n",
      "Number of epochs:\t 200\n",
      "Number of (h) units:\t 50\n",
      "Batch size:\t\t 32\n",
      "Max seq len:\t\t 30\n",
      "Learning rate:\t\t 0.1\n",
      "Run Time: 125.31816666666667 mins\n",
      "================================================== \n",
      "Run: 16/32\n",
      "Number of epochs:\t 200\n",
      "Number of (h) units:\t 50\n",
      "Batch size:\t\t 32\n",
      "Max seq len:\t\t 30\n",
      "Learning rate:\t\t 0.2\n",
      "Run Time: 104.92733333333334 mins\n",
      "================================================== \n",
      "Run: 17/32\n",
      "Number of epochs:\t 200\n",
      "Number of (h) units:\t 20\n",
      "Batch size:\t\t 16\n",
      "Max seq len:\t\t 18\n",
      "Learning rate:\t\t 0.1\n",
      "Run Time: 51.041333333333334 mins\n",
      "================================================== \n",
      "Run: 18/32\n",
      "Number of epochs:\t 200\n",
      "Number of (h) units:\t 20\n",
      "Batch size:\t\t 16\n",
      "Max seq len:\t\t 18\n",
      "Learning rate:\t\t 0.2\n",
      "Run Time: 94.96266666666666 mins\n",
      "================================================== \n",
      "Run: 19/32\n",
      "Number of epochs:\t 200\n",
      "Number of (h) units:\t 20\n",
      "Batch size:\t\t 16\n",
      "Max seq len:\t\t 30\n",
      "Learning rate:\t\t 0.1\n",
      "Run Time: 135.8235 mins\n",
      "================================================== \n",
      "Run: 20/32\n",
      "Number of epochs:\t 200\n",
      "Number of (h) units:\t 20\n",
      "Batch size:\t\t 16\n",
      "Max seq len:\t\t 30\n",
      "Learning rate:\t\t 0.2\n",
      "Run Time: 135.90416666666667 mins\n",
      "================================================== \n",
      "Run: 21/32\n",
      "Number of epochs:\t 200\n",
      "Number of (h) units:\t 20\n",
      "Batch size:\t\t 32\n",
      "Max seq len:\t\t 18\n",
      "Learning rate:\t\t 0.1\n",
      "Run Time: 79.028 mins\n",
      "================================================== \n",
      "Run: 22/32\n",
      "Number of epochs:\t 200\n",
      "Number of (h) units:\t 20\n",
      "Batch size:\t\t 32\n",
      "Max seq len:\t\t 18\n",
      "Learning rate:\t\t 0.2\n",
      "Run Time: 78.57533333333335 mins\n",
      "================================================== \n",
      "Run: 23/32\n",
      "Number of epochs:\t 200\n",
      "Number of (h) units:\t 20\n",
      "Batch size:\t\t 32\n",
      "Max seq len:\t\t 30\n",
      "Learning rate:\t\t 0.1\n",
      "Run Time: 106.2375 mins\n",
      "================================================== \n",
      "Run: 24/32\n",
      "Number of epochs:\t 200\n",
      "Number of (h) units:\t 20\n",
      "Batch size:\t\t 32\n",
      "Max seq len:\t\t 30\n",
      "Learning rate:\t\t 0.2\n",
      "Run Time: 119.1795 mins\n",
      "================================================== \n",
      "Run: 25/32\n",
      "Number of epochs:\t 200\n",
      "Number of (h) units:\t 50\n",
      "Batch size:\t\t 16\n",
      "Max seq len:\t\t 18\n",
      "Learning rate:\t\t 0.1\n",
      "Run Time: 100.81183333333334 mins\n",
      "================================================== \n",
      "Run: 26/32\n",
      "Number of epochs:\t 200\n",
      "Number of (h) units:\t 50\n",
      "Batch size:\t\t 16\n",
      "Max seq len:\t\t 18\n",
      "Learning rate:\t\t 0.2\n",
      "Run Time: 100.38283333333334 mins\n",
      "================================================== \n",
      "Run: 27/32\n",
      "Number of epochs:\t 200\n",
      "Number of (h) units:\t 50\n",
      "Batch size:\t\t 16\n",
      "Max seq len:\t\t 30\n",
      "Learning rate:\t\t 0.1\n",
      "Run Time: 143.26566666666668 mins\n",
      "================================================== \n",
      "Run: 28/32\n",
      "Number of epochs:\t 200\n",
      "Number of (h) units:\t 50\n",
      "Batch size:\t\t 16\n",
      "Max seq len:\t\t 30\n",
      "Learning rate:\t\t 0.2\n",
      "Run Time: 143.29799999999997 mins\n",
      "================================================== \n",
      "Run: 29/32\n",
      "Number of epochs:\t 200\n",
      "Number of (h) units:\t 50\n",
      "Batch size:\t\t 32\n",
      "Max seq len:\t\t 18\n",
      "Learning rate:\t\t 0.1\n",
      "Run Time: 83.57366666666667 mins\n",
      "================================================== \n",
      "Run: 30/32\n",
      "Number of epochs:\t 200\n",
      "Number of (h) units:\t 50\n",
      "Batch size:\t\t 32\n",
      "Max seq len:\t\t 18\n",
      "Learning rate:\t\t 0.2\n",
      "Run Time: 83.56883333333333 mins\n",
      "================================================== \n",
      "Run: 31/32\n",
      "Number of epochs:\t 200\n",
      "Number of (h) units:\t 50\n",
      "Batch size:\t\t 32\n",
      "Max seq len:\t\t 30\n",
      "Learning rate:\t\t 0.1\n",
      "Run Time: 124.339 mins\n",
      "================================================== \n",
      "Run: 32/32\n",
      "Number of epochs:\t 200\n",
      "Number of (h) units:\t 50\n",
      "Batch size:\t\t 32\n",
      "Max seq len:\t\t 30\n",
      "Learning rate:\t\t 0.2\n",
      "Run Time: 124.72316666666667 mins\n"
     ]
    }
   ],
   "source": [
    "metrics=[recall_metric(total_items=total_items)]\n",
    "for epoch in epochs:\n",
    "    params['epochs'] = epoch\n",
    "    for delta in deltas:\n",
    "        params['delta'] = delta\n",
    "        \n",
    "        # Pre-Calculate diversity_bias\n",
    "        diversity_bias = create_diversity_bias(train_set, total_items, delta)\n",
    "        loss=diversity_bias_loss(db=diversity_bias, total_items=total_items)\n",
    "        \n",
    "        for rnn_unit in rnn_units:\n",
    "            params['rnn_units'] = rnn_unit\n",
    "        \n",
    "            for batch_size in batch_sizes:\n",
    "                params['BATCH_SIZE'] = batch_size\n",
    "                \n",
    "                # Rebuild model \n",
    "                model = build_LSTM_model(total_items = total_items, embedding_dim = embedding_dim, mask_value = pad_value, rnn_units = rnn_unit, batch_size = batch_size, return_sequences=True)\n",
    "\n",
    "                for max_seq_len in max_seq_lens:\n",
    "                    params['max_seq_len'] = max_seq_len\n",
    "\n",
    "                    # Create new datasets\n",
    "                    train_dataset = create_seq_batch_dataset(df=train_set, shift=shift_targets_by, max_seq_len=max_seq_len, pad_value=pad_value, batch_size=batch_size, stats=False, drop_remainder=True)\n",
    "                    val_dataset = create_seq_batch_dataset(df=val_set, shift=shift_targets_by, max_seq_len=max_seq_len, pad_value=pad_value, batch_size=batch_size, stats=False, drop_remainder=True) \n",
    "\n",
    "                    for learning_rate in learning_rates:\n",
    "                        runs += 1\n",
    "                        s = time.time()\n",
    "\n",
    "                        # Print current run\n",
    "                        print('='*50, '\\nRun:', str(runs) + '/' + str(total_runs))\n",
    "                        print('Number of epochs:\\t', epoch)\n",
    "                        print('Number of (h) units:\\t', rnn_unit)\n",
    "                        print('Batch size:\\t\\t', batch_size)\n",
    "                        print('Max seq len:\\t\\t', max_seq_len)\n",
    "                        print('Learning rate:\\t\\t', learning_rate)\n",
    "\n",
    "                        params['learning_rate'] =  learning_rate\n",
    "\n",
    "                        # Compile Model\n",
    "                        model = build_LSTM_model(total_items = total_items, embedding_dim = embedding_dim, mask_value = pad_value, rnn_units = rnn_unit, batch_size = batch_size, return_sequences=True)\n",
    "                        optimizer=tf.keras.optimizers.Adagrad(lr=learning_rate)\n",
    "\n",
    "                        model.compile(optimizer=optimizer,\n",
    "                                      loss=loss, \n",
    "                                      metrics=metrics)\n",
    "\n",
    "                        # Create Callbacks\n",
    "                        checkpoint_dir = directory + '_' + str(params['model_id'])\n",
    "                        checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "                        \n",
    "                        checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath = checkpoint_prefix, monitor = 'val_recall', mode = 'max', save_best_only = True, save_weights_only = True)\n",
    "                        early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor = 'val_recall', min_delta = 0.0001, mode = 'max', patience = 15)\n",
    "                        timing_callback = TimingCallback()\n",
    "                        callbacks = [checkpoint_callback, early_stopping_callback, timing_callback]\n",
    "                        \n",
    "                        # Run \n",
    "                        history = model.fit(x=train_dataset, \n",
    "                                            validation_data=val_dataset, \n",
    "                                            epochs=epoch,\n",
    "                                            callbacks=callbacks,\n",
    "                                            verbose=0)\n",
    "                        \n",
    "                        \n",
    "                        # Restore lates checkpoint and predict \n",
    "                        model = build_LSTM_model(total_items = total_items, embedding_dim = embedding_dim, mask_value = pad_value, rnn_units = rnn_unit, batch_size = None, return_sequences=False)\n",
    "                        model.load_weights(tf.train.latest_checkpoint(checkpoint_dir)).expect_partial()\n",
    "                        model.build(tf.TensorShape([1, None]))\n",
    "                        preds_df = get_predictions(model, test_set, test_left_out_items, test_batch_size, max_seq_len, pad_value, rank_at)\n",
    "                        preds_df.to_pickle(path + 'results/am/predictions/' + 'predictions_' + params['model_id'])\n",
    "                        metrics_test = get_metrics(preds_df, 5, rank_at, stats=False)\n",
    "#                        print(metrics_test)\n",
    "                        \n",
    "                        # Store model\n",
    "                        store_path = path + 'results/' + res_ext + '/all_models_2'\n",
    "                        train_time = np.sum(timing_callback.logs)\n",
    "                        all_models = store_LSTM_model(store_path, params.copy(), history.history.copy(), train_time, metrics_test, store=True)\n",
    "                        \n",
    "                        # Change Model Id for next model\n",
    "                        params['model_id'] = str(int(params['model_id'][0]) + 1) + '_am'\n",
    "                        print(f'Run Time: {round(time.time() - s,2)/60} mins')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prediction speed x10 with 1024 prediction batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
